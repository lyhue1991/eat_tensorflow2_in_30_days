# ã€Š30å¤©åƒæ‰é‚£åª TensorFlow2.0 ã€‹å¼€ç¯‡è¾ ğŸ”¥ğŸ”¥


ğŸ“š gitbookç”µå­ä¹¦åœ°å€ï¼š https://lyhue1991.github.io/eat_tensorflow2_in_30_days

ğŸš€ githubé¡¹ç›®åœ°å€ï¼šhttps://github.com/lyhue1991/eat_tensorflow2_in_30_d


### ä¸€ï¼ŒTensorFlow2 ğŸ è¿˜æ˜¯ PytorchğŸ”¥

å…ˆè¯´ç»“è®º:

**å¦‚æœæ˜¯å·¥ç¨‹å¸ˆï¼Œåº”è¯¥ä¼˜å…ˆé€‰TensorFlow2.**

**å¦‚æœæ˜¯å­¦ç”Ÿæˆ–è€…ç ”ç©¶äººå‘˜ï¼Œåº”è¯¥ä¼˜å…ˆé€‰æ‹©Pytorch.**

**å¦‚æœæ—¶é—´è¶³å¤Ÿï¼Œæœ€å¥½TensorFlow2å’ŒPytorchéƒ½è¦å­¦ä¹ æŒæ¡ã€‚**


ç†ç”±å¦‚ä¸‹ï¼š

* 1ï¼Œ**åœ¨å·¥ä¸šç•Œæœ€é‡è¦çš„æ˜¯æ¨¡å‹è½åœ°ï¼Œç›®å‰å›½å†…çš„å¤§éƒ¨åˆ†äº’è”ç½‘ä¼ä¸šåªæ”¯æŒTensorFlowæ¨¡å‹çš„åœ¨çº¿éƒ¨ç½²ï¼Œä¸æ”¯æŒPytorchã€‚** å¹¶ä¸”å·¥ä¸šç•Œæ›´åŠ æ³¨é‡çš„æ˜¯æ¨¡å‹çš„é«˜å¯ç”¨æ€§ï¼Œè®¸å¤šæ—¶å€™ä½¿ç”¨çš„éƒ½æ˜¯æˆç†Ÿçš„æ¨¡å‹æ¶æ„ï¼Œè°ƒè¯•éœ€æ±‚å¹¶ä¸å¤§ã€‚


* 2ï¼Œ**ç ”ç©¶äººå‘˜æœ€é‡è¦çš„æ˜¯å¿«é€Ÿè¿­ä»£å‘è¡¨æ–‡ç« ï¼Œéœ€è¦å°è¯•ä¸€äº›è¾ƒæ–°çš„æ¨¡å‹æ¶æ„ã€‚è€ŒPytorchåœ¨æ˜“ç”¨æ€§ä¸Šç›¸æ¯”TensorFlow2æœ‰ä¸€äº›ä¼˜åŠ¿ï¼Œæ›´åŠ æ–¹ä¾¿è°ƒè¯•ã€‚** å¹¶ä¸”åœ¨2019å¹´ä»¥æ¥åœ¨å­¦æœ¯ç•Œå é¢†äº†å¤§åŠå£æ±Ÿå±±ï¼Œèƒ½å¤Ÿæ‰¾åˆ°çš„ç›¸åº”æœ€æ–°ç ”ç©¶æˆæœæ›´å¤šã€‚


* 3ï¼ŒTensorFlow2å’ŒPytorchå®é™…ä¸Šæ•´ä½“é£æ ¼å·²ç»éå¸¸ç›¸ä¼¼äº†ï¼Œå­¦ä¼šäº†å…¶ä¸­ä¸€ä¸ªï¼Œå­¦ä¹ å¦å¤–ä¸€ä¸ªå°†æ¯”è¾ƒå®¹æ˜“ã€‚ä¸¤ç§æ¡†æ¶éƒ½æŒæ¡çš„è¯ï¼Œèƒ½å¤Ÿå‚è€ƒçš„å¼€æºæ¨¡å‹æ¡ˆä¾‹æ›´å¤šï¼Œå¹¶ä¸”å¯ä»¥æ–¹ä¾¿åœ°åœ¨ä¸¤ç§æ¡†æ¶ä¹‹é—´åˆ‡æ¢ã€‚

```python

```

### äºŒï¼ŒKerasğŸ å’Œ tf.keras ğŸ

å…ˆè¯´ç»“è®ºï¼š

**Kerasåº“åœ¨2.3.0ç‰ˆæœ¬åå°†ä¸å†æ›´æ–°ï¼Œç”¨æˆ·åº”è¯¥ä½¿ç”¨tf.kerasã€‚**


Keraså¯ä»¥çœ‹æˆæ˜¯ä¸€ç§æ·±åº¦å­¦ä¹ æ¡†æ¶çš„é«˜é˜¶æ¥å£è§„èŒƒï¼Œå®ƒå¸®åŠ©ç”¨æˆ·ä»¥æ›´ç®€æ´çš„å½¢å¼å®šä¹‰å’Œè®­ç»ƒæ·±åº¦å­¦ä¹ ç½‘ç»œã€‚

ä½¿ç”¨pipå®‰è£…çš„Kerasåº“åŒæ—¶åœ¨tensorflow,theano,CNTKç­‰åç«¯åŸºç¡€ä¸Šè¿›è¡Œäº†è¿™ç§é«˜é˜¶æ¥å£è§„èŒƒçš„å®ç°ã€‚

è€Œtf.kerasæ˜¯åœ¨TensorFlowä¸­ä»¥TensorFlowä½é˜¶APIä¸ºåŸºç¡€å®ç°çš„è¿™ç§é«˜é˜¶æ¥å£ï¼Œå®ƒæ˜¯Tensorflowçš„ä¸€ä¸ªå­æ¨¡å—ã€‚

tf.kerasç»å¤§éƒ¨åˆ†åŠŸèƒ½å’Œå…¼å®¹å¤šç§åç«¯çš„Kerasåº“ç”¨æ³•å®Œå…¨ä¸€æ ·ï¼Œä½†å¹¶éå…¨éƒ¨ï¼Œå®ƒå’ŒTensorFlowä¹‹é—´çš„ç»“åˆæ›´ä¸ºç´§å¯†ã€‚

éšç€è°·æ­Œå¯¹Kerasçš„æ”¶è´­ï¼ŒKerasåº“2.3.0ç‰ˆæœ¬åä¹Ÿå°†ä¸å†è¿›è¡Œæ›´æ–°ï¼Œç”¨æˆ·åº”å½“ä½¿ç”¨tf.kerasè€Œä¸æ˜¯ä½¿ç”¨pipå®‰è£…çš„Keras.

### ä¸‰ï¼Œæœ¬ä¹¦ğŸ“–é¢å‘è¯»è€… ğŸ‘¼


**æœ¬ä¹¦å‡å®šè¯»è€…æœ‰ä¸€å®šçš„æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ åŸºç¡€ï¼Œä½¿ç”¨è¿‡Kerasæˆ–è€…Tensorflow1.0æˆ–è€…Pytorchæ­å»ºè®­ç»ƒè¿‡æ¨¡å‹ã€‚**

**å¯¹äºæ²¡æœ‰ä»»ä½•æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ åŸºç¡€çš„åŒå­¦ï¼Œå»ºè®®åœ¨å­¦ä¹ æœ¬ä¹¦æ—¶åŒæ­¥å‚è€ƒå­¦ä¹ ã€ŠPythonæ·±åº¦å­¦ä¹ ã€‹ä¸€ä¹¦ã€‚**

ã€ŠPythonæ·±åº¦å­¦ä¹ ã€‹è¿™æœ¬ä¹¦æ˜¯Kerasä¹‹çˆ¶Francois Cholletæ‰€è‘—ï¼Œè¯¥ä¹¦å‡å®šè¯»è€…æ— ä»»ä½•æœºå™¨å­¦ä¹ çŸ¥è¯†ï¼Œä»¥Kerasä¸ºå·¥å…·ï¼Œ

ä½¿ç”¨ä¸°å¯Œçš„èŒƒä¾‹ç¤ºèŒƒæ·±åº¦å­¦ä¹ çš„æœ€ä½³å®è·µï¼Œè¯¥ä¹¦é€šä¿—æ˜“æ‡‚ï¼Œ**å…¨ä¹¦æ²¡æœ‰ä¸€ä¸ªæ•°å­¦å…¬å¼ï¼Œæ³¨é‡åŸ¹å…»è¯»è€…çš„æ·±åº¦å­¦ä¹ ç›´è§‰ã€‚**ã€‚

è¯¥ä¹¦ç”µå­ç‰ˆä¸‹è½½é“¾æ¥ï¼šhttps://pan.baidu.com/s/1-4q6VjLTb3ZxcefyNCbjSA æå–ç ï¼šwtzo 



### å››ï¼Œæœ¬ä¹¦å†™ä½œé£æ ¼ ğŸ‰


**æœ¬ä¹¦æ˜¯ä¸€æœ¬å¯¹äººç±»ç”¨æˆ·æå…¶å‹å–„çš„TensorFlow2.0å…¥é—¨å·¥å…·ä¹¦ï¼Œä¸åˆ»æ„æ¶å¿ƒè¯»è€…æ˜¯æœ¬ä¹¦çš„åº•é™è¦æ±‚ï¼ŒDon't let me thinkæ˜¯æœ¬ä¹¦çš„æœ€é«˜è¿½æ±‚ã€‚**

æœ¬ä¹¦ä¸»è¦æ˜¯åœ¨å‚è€ƒTensorFlowå®˜æ–¹æ–‡æ¡£å’Œå‡½æ•°docæ–‡æ¡£åŸºç¡€ä¸Šæ•´ç†å†™æˆçš„ã€‚

ä½†æœ¬ä¹¦åœ¨ç¯‡ç« ç»“æ„å’ŒèŒƒä¾‹é€‰å–ä¸Šåšäº†å¤§é‡çš„ä¼˜åŒ–ã€‚

ä¸åŒäºå®˜æ–¹æ–‡æ¡£æ··ä¹±çš„ç¯‡ç« ç»“æ„ï¼Œæ—¢æœ‰æ•™ç¨‹åˆæœ‰æŒ‡å—ï¼Œç¼ºå°‘æ•´ä½“çš„ç¼–æ’é€»è¾‘ã€‚

æœ¬ä¹¦æŒ‰ç…§å†…å®¹éš¾æ˜“ç¨‹åº¦ã€è¯»è€…æ£€ç´¢ä¹ æƒ¯å’ŒTensorFlowè‡ªèº«çš„å±‚æ¬¡ç»“æ„è®¾è®¡å†…å®¹ï¼Œå¾ªåºæ¸è¿›ï¼Œå±‚æ¬¡æ¸…æ™°ï¼Œæ–¹ä¾¿æŒ‰ç…§åŠŸèƒ½æŸ¥æ‰¾ç›¸åº”èŒƒä¾‹ã€‚

ä¸åŒäºå®˜æ–¹æ–‡æ¡£å†—é•¿çš„èŒƒä¾‹ä»£ç ï¼Œæœ¬ä¹¦åœ¨èŒƒä¾‹è®¾è®¡ä¸Šå°½å¯èƒ½ç®€çº¦åŒ–å’Œç»“æ„åŒ–ï¼Œå¢å¼ºèŒƒä¾‹æ˜“è¯»æ€§å’Œé€šç”¨æ€§ï¼Œå¤§éƒ¨åˆ†ä»£ç ç‰‡æ®µåœ¨å®è·µä¸­å¯å³å–å³ç”¨ã€‚

**å¦‚æœè¯´é€šè¿‡å­¦ä¹ TensorFlowå®˜æ–¹æ–‡æ¡£æŒæ¡TensorFlow2.0çš„éš¾åº¦å¤§æ¦‚æ˜¯9çš„è¯ï¼Œé‚£ä¹ˆé€šè¿‡å­¦ä¹ æœ¬ä¹¦æŒæ¡TensorFlow2.0çš„éš¾åº¦åº”è¯¥å¤§æ¦‚æ˜¯3.**

è°¨ä»¥ä¸‹å›¾å¯¹æ¯”ä¸€ä¸‹TensorFlowå®˜æ–¹æ•™ç¨‹ä¸æœ¬æ•™ç¨‹çš„å·®å¼‚ã€‚

![](./data/30å¤©åƒæ‰é‚£ä¸ªTF2.0.jpg)



### äº”ï¼Œæœ¬ä¹¦å­¦ä¹ æ–¹æ¡ˆ â°

**1ï¼Œå­¦ä¹ è®¡åˆ’**

æœ¬ä¹¦æ˜¯ä½œè€…åˆ©ç”¨å·¥ä½œä¹‹ä½™å’Œç–«æƒ…æ”¾å‡æœŸé—´å¤§æ¦‚2ä¸ªæœˆå†™æˆçš„ï¼Œå¤§éƒ¨åˆ†è¯»è€…åº”è¯¥åœ¨30å¤©å¯ä»¥å®Œå…¨å­¦ä¼šã€‚

é¢„è®¡æ¯å¤©èŠ±è´¹çš„å­¦ä¹ æ—¶é—´åœ¨30åˆ†é’Ÿåˆ°2ä¸ªå°æ—¶ä¹‹é—´ã€‚

å½“ç„¶ï¼Œæœ¬ä¹¦ä¹Ÿéå¸¸é€‚åˆä½œä¸ºTensorFlowçš„å·¥å…·æ‰‹å†Œåœ¨å·¥ç¨‹è½åœ°æ—¶ä½œä¸ºèŒƒä¾‹åº“å‚è€ƒã€‚

**ç‚¹å‡»å­¦ä¹ å†…å®¹è“è‰²æ ‡é¢˜å³å¯è¿›å…¥è¯¥ç« èŠ‚ã€‚**


|æ—¥æœŸ | å­¦ä¹ å†…å®¹                                                       | å†…å®¹éš¾åº¦   | é¢„è®¡å­¦ä¹ æ—¶é—´ | æ›´æ–°çŠ¶æ€|
|----:|:--------------------------------------------------------------|-----------:|----------:|-----:|
|&nbsp;|[**ä¸€ã€TensorFlowçš„å»ºæ¨¡æµç¨‹**](./ä¸€ã€TensorFlowçš„å»ºæ¨¡æµç¨‹.md)    |â­ï¸   |   0hour   |âœ…    |
|day1 |  [1-1,ç»“æ„åŒ–æ•°æ®å»ºæ¨¡æµç¨‹èŒƒä¾‹](./1-1,ç»“æ„åŒ–æ•°æ®å»ºæ¨¡æµç¨‹èŒƒä¾‹.md)    | â­ï¸â­ï¸â­ï¸ |   1hour    |âœ…    |
|day2 |[1-2,å›¾ç‰‡æ•°æ®å»ºæ¨¡æµç¨‹èŒƒä¾‹](./1-2,å›¾ç‰‡æ•°æ®å»ºæ¨¡æµç¨‹èŒƒä¾‹.md)    | â­ï¸â­ï¸â­ï¸â­ï¸  |   2hour    |âœ…    |
|day3 |  [1-3,æ–‡æœ¬æ•°æ®å»ºæ¨¡æµç¨‹èŒƒä¾‹](./1-3,æ–‡æœ¬æ•°æ®å»ºæ¨¡æµç¨‹èŒƒä¾‹.md)   | â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸  |   2hour    |âœ…    |
|day4 |  [1-4,æ—¶é—´åºåˆ—æ•°æ®å»ºæ¨¡æµç¨‹èŒƒä¾‹](./1-4,æ—¶é—´åºåˆ—æ•°æ®å»ºæ¨¡æµç¨‹èŒƒä¾‹.md)   | â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸  |   2hour    |âœ…    |
|&nbsp;    |[**äºŒã€TensorFlowçš„æ ¸å¿ƒæ¦‚å¿µ**](./äºŒã€TensorFlowçš„æ ¸å¿ƒæ¦‚å¿µ.md)  | â­ï¸  |  0hour |âœ…  |
|day5 |  [2-1,å¼ é‡æ•°æ®ç»“æ„](./2-1,å¼ é‡æ•°æ®ç»“æ„.md)  | â­ï¸â­ï¸â­ï¸â­ï¸   |   1hour    |âœ…    |
|day6 |  [2-2,ä¸‰ç§è®¡ç®—å›¾](./2-2,ä¸‰ç§è®¡ç®—å›¾.md)  | â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸   |   2hour    |âœ…    |
|day7 |  [2-3,è‡ªåŠ¨å¾®åˆ†æœºåˆ¶](./2-3,è‡ªåŠ¨å¾®åˆ†æœºåˆ¶.md)  | â­ï¸â­ï¸â­ï¸   |   1hour    |âœ…    |
|&nbsp; |[**ä¸‰ã€TensorFlowçš„å±‚æ¬¡ç»“æ„**](./ä¸‰ã€TensorFlowçš„å±‚æ¬¡ç»“æ„.md) |   â­ï¸  |  0hour   |âœ…  |
|day8 |  [3-1,ä½é˜¶APIç¤ºèŒƒ](./3-1,ä½é˜¶APIç¤ºèŒƒ.md)   | â­ï¸â­ï¸   |   0.5hour    |âœ…   |
|day9 |  [3-2,ä¸­é˜¶APIç¤ºèŒƒ](./3-2,ä¸­é˜¶APIç¤ºèŒƒ.md)   | â­ï¸â­ï¸â­ï¸   |   0.5hour    |âœ…  |
|day10 |  [3-3,é«˜é˜¶APIç¤ºèŒƒ](./3-3,é«˜é˜¶APIç¤ºèŒƒ.md)  | â­ï¸â­ï¸â­ï¸   |   0.5hour    |âœ…  |
|&nbsp; |[**å››ã€TensorFlowçš„ä½é˜¶API**](./å››ã€TensorFlowçš„ä½é˜¶API.md) |â­ï¸    | 0hour|âœ…  |
|day11|  [4-1,å¼ é‡çš„ç»“æ„æ“ä½œ](./4-1,å¼ é‡çš„ç»“æ„æ“ä½œ.md)  | â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸   |   2hour    |âœ…   |
|day12|  [4-2,å¼ é‡çš„æ•°å­¦è¿ç®—](./4-2,å¼ é‡çš„æ•°å­¦è¿ç®—.md)   | â­ï¸â­ï¸â­ï¸â­ï¸   |   1hour    |âœ…  |
|day13|  [4-3,AutoGraphçš„ä½¿ç”¨è§„èŒƒ](./4-3,AutoGraphçš„ä½¿ç”¨è§„èŒƒ.md)| â­ï¸â­ï¸â­ï¸   |   0.5hour    |âœ…  |
|day14|  [4-4,AutoGraphçš„æœºåˆ¶åŸç†](./4-4,AutoGraphçš„æœºåˆ¶åŸç†.md)    | â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸   |   2hour    |âœ…  |
|day15|  [4-5,AutoGraphå’Œtf.Module](./4-5,AutoGraphå’Œtf.Module.md)  | â­ï¸â­ï¸â­ï¸â­ï¸   |   1hour    |âœ…  |
|&nbsp; |[**äº”ã€TensorFlowçš„ä¸­é˜¶API**](./äº”ã€TensorFlowçš„ä¸­é˜¶API.md) |  â­ï¸  | 0hour|âœ… |
|day16|  [5-1,æ•°æ®ç®¡é“Dataset](./5-1,æ•°æ®ç®¡é“Dataset.md)   | â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸   |   2hour    |âœ…  |
|day17|  [5-2,ç‰¹å¾åˆ—feature_column](./5-2,ç‰¹å¾åˆ—feature_column.md)   | â­ï¸â­ï¸â­ï¸â­ï¸   |   1hour    |âœ…  |
|day18|  [5-3,æ¿€æ´»å‡½æ•°activation](./5-3,æ¿€æ´»å‡½æ•°activation.md)    | â­ï¸â­ï¸â­ï¸   |   0.5hour    |âœ…   |
|day19|  [5-4,æ¨¡å‹å±‚layers](./5-4,æ¨¡å‹å±‚layers.md)  | â­ï¸â­ï¸â­ï¸   |   1hour    |âœ…  |
|day20|  [5-5,æŸå¤±å‡½æ•°losses](./5-5,æŸå¤±å‡½æ•°losses.md)    | â­ï¸â­ï¸â­ï¸   |   1hour    |âœ…  |
|day21|  [5-6,è¯„ä¼°æŒ‡æ ‡metrics](./5-6,è¯„ä¼°æŒ‡æ ‡metrics.md)    | â­ï¸â­ï¸â­ï¸   |   1hour    |âœ…   |
|day22|  [5-7,ä¼˜åŒ–å™¨optimizers](./5-7,ä¼˜åŒ–å™¨optimizers.md)    | â­ï¸â­ï¸â­ï¸   |   0.5hour    |âœ…   |
|day23|  [5-8,å›è°ƒå‡½æ•°callbacks](./5-8,å›è°ƒå‡½æ•°callbacks.md)   | â­ï¸â­ï¸â­ï¸â­ï¸   |   1hour    |âœ…   |
|&nbsp; |[**å…­ã€TensorFlowçš„é«˜é˜¶API**](./å…­ã€TensorFlowçš„é«˜é˜¶API.md)|    â­ï¸ | 0hour|âœ…  |
|day24|  [6-1,æ„å»ºæ¨¡å‹çš„3ç§æ–¹æ³•](./6-1,æ„å»ºæ¨¡å‹çš„3ç§æ–¹æ³•.md)   | â­ï¸â­ï¸â­ï¸   |   1hour    |âœ… |
|day25|  [6-2,è®­ç»ƒæ¨¡å‹çš„3ç§æ–¹æ³•](./6-2,è®­ç»ƒæ¨¡å‹çš„3ç§æ–¹æ³•.md)  | â­ï¸â­ï¸â­ï¸â­ï¸   |   1hour    |âœ…   |
|day26|  [6-3,ä½¿ç”¨å•GPUè®­ç»ƒæ¨¡å‹](./6-3,ä½¿ç”¨å•GPUè®­ç»ƒæ¨¡å‹.md)    | â­ï¸â­ï¸   |   0.5hour    |âœ…   |
|day27|  [6-4,ä½¿ç”¨å¤šGPUè®­ç»ƒæ¨¡å‹](./6-4,ä½¿ç”¨å¤šGPUè®­ç»ƒæ¨¡å‹.md)    | â­ï¸â­ï¸   |   0.5hour    |âœ…  |
|day28|  [6-5,ä½¿ç”¨TPUè®­ç»ƒæ¨¡å‹](./6-5,ä½¿ç”¨TPUè®­ç»ƒæ¨¡å‹.md)   | â­ï¸â­ï¸   |   0.5hour    |âœ…  |
|day29| [6-6,ä½¿ç”¨tensorflow-servingéƒ¨ç½²æ¨¡å‹](./6-6,ä½¿ç”¨tensorflow-servingéƒ¨ç½²æ¨¡å‹.md) | â­ï¸â­ï¸â­ï¸â­ï¸| 1hour |âœ…   |
|day30| [6-7,ä½¿ç”¨spark-scalaè°ƒç”¨tensorflowæ¨¡å‹](./6-7,ä½¿ç”¨spark-scalaè°ƒç”¨tensorflowæ¨¡å‹.md) | â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸|2hour|âœ…  |


```python

```

**2ï¼Œå­¦ä¹ ç¯å¢ƒ**


æœ¬ä¹¦å…¨éƒ¨æºç åœ¨jupyterä¸­ç¼–å†™æµ‹è¯•é€šè¿‡ï¼Œå»ºè®®é€šè¿‡gitå…‹éš†åˆ°æœ¬åœ°ï¼Œå¹¶åœ¨jupyterä¸­äº¤äº’å¼è¿è¡Œå­¦ä¹ ã€‚

ä¸ºäº†ç›´æ¥èƒ½å¤Ÿåœ¨jupyterä¸­æ‰“å¼€markdownæ–‡ä»¶ï¼Œå»ºè®®å®‰è£…jupytextï¼Œå°†markdownè½¬æ¢æˆipnbã€‚

æ­¤å¤–ï¼Œä¹Ÿå¯ä»¥å…³æ³¨å…¬ä¼—å·â€**Pythonä¸ç®—æ³•ä¹‹ç¾**â€œ ï¼Œåå°å›å¤å…³é”®å­—ï¼š**tf**ï¼Œè·å–æœ¬ä¹¦ipynbæºç çš„ä¸‹è½½é“¾æ¥ã€‚

```python
#å…‹éš†æœ¬ä¹¦æºç åˆ°æœ¬åœ°
#!git clone https://github.com/lyhue1991/eat_tensorflow2_in_30_days

#å»ºè®®åœ¨jupyter notebook ä¸Šå®‰è£…jupytextï¼Œä»¥ä¾¿èƒ½å¤Ÿå°†æœ¬ä¹¦å„ç« èŠ‚markdownæ–‡ä»¶è§†ä½œipynbæ–‡ä»¶è¿è¡Œ
#!pip install -i https://pypi.tuna.tsinghua.edu.cn/simple -U jupytext
    
#å»ºè®®åœ¨jupyter notebook ä¸Šå®‰è£…æœ€æ–°ç‰ˆæœ¬tensorflow æµ‹è¯•æœ¬ä¹¦ä¸­çš„ä»£ç 
#!pip install -i https://pypi.tuna.tsinghua.edu.cn/simple  -U tensorflow
```

```python
import tensorflow as tf

#æ³¨ï¼šæœ¬ä¹¦å…¨éƒ¨ä»£ç åœ¨tensorflow 2.1ç‰ˆæœ¬æµ‹è¯•é€šè¿‡
tf.print("tensorflow version:",tf.__version__)

a = tf.constant("hello")
b = tf.constant("tensorflow2")
c = tf.strings.join([a,b]," ")
tf.print(c)
```

```
tensorflow version: 2.1.0
hello tensorflow2
```


### å…­ï¼Œé¼“åŠ±å’Œè”ç³»ä½œè€… ğŸˆğŸˆ

**å¦‚æœæœ¬ä¹¦å¯¹ä½ æœ‰æ‰€å¸®åŠ©ï¼Œæƒ³é¼“åŠ±ä¸€ä¸‹ä½œè€…ï¼Œè®°å¾—ç»™æœ¬é¡¹ç›®åŠ ä¸€é¢—æ˜Ÿæ˜Ÿstarâ­ï¸ï¼Œå¹¶åˆ†äº«ç»™ä½ çš„æœ‹å‹ä»¬å–”ğŸ˜Š!** 

å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·"**Pythonä¸ç®—æ³•ä¹‹ç¾**"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚

å¦‚æœæœ‰æƒ³è¦è·å–æœ¬ä¹¦çš„jupyter notebookæºä»£ç çš„å°ä¼™ä¼´ï¼Œä¹Ÿå¯ä»¥å…³æ³¨å…¬ä¼—å·ï¼Œåœ¨åå°å›å¤å…³é”®å­—ï¼š**tf**ï¼Œè·å–æœ¬ä¹¦å…¨éƒ¨ä»£ç å’Œæ•°æ®é›†ä¸‹è½½é“¾æ¥ã€‚

![image.png](./data/Pythonä¸ç®—æ³•ä¹‹ç¾logo.jpg)

```python

```
# ä¸€ã€TensorFlowçš„å»ºæ¨¡æµç¨‹


å°½ç®¡TensorFlowè®¾è®¡ä¸Šè¶³å¤Ÿçµæ´»ï¼Œå¯ä»¥ç”¨äºè¿›è¡Œå„ç§å¤æ‚çš„æ•°å€¼è®¡ç®—ã€‚

ä½†é€šå¸¸äººä»¬ä½¿ç”¨TensorFlowæ¥å®ç°æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œå°¤å…¶å¸¸ç”¨äºå®ç°ç¥ç»ç½‘ç»œæ¨¡å‹ã€‚

ä»åŸç†ä¸Šè¯´å¯ä»¥ä½¿ç”¨å¼ é‡æ„å»ºè®¡ç®—å›¾æ¥å®šä¹‰ç¥ç»ç½‘ç»œï¼Œå¹¶é€šè¿‡è‡ªåŠ¨å¾®åˆ†æœºåˆ¶è®­ç»ƒæ¨¡å‹ã€‚

ä½†ä¸ºç®€æ´èµ·è§ï¼Œä¸€èˆ¬æ¨èä½¿ç”¨TensorFlowçš„é«˜å±‚æ¬¡kerasæ¥å£æ¥å®ç°ç¥ç»ç½‘ç»œç½‘æ¨¡å‹ã€‚


ä½¿ç”¨TensorFlowå®ç°ç¥ç»ç½‘ç»œæ¨¡å‹çš„ä¸€èˆ¬æµç¨‹åŒ…æ‹¬ï¼š

1ï¼Œå‡†å¤‡æ•°æ®

2ï¼Œå®šä¹‰æ¨¡å‹

3ï¼Œè®­ç»ƒæ¨¡å‹

4ï¼Œè¯„ä¼°æ¨¡å‹

5ï¼Œä½¿ç”¨æ¨¡å‹

6ï¼Œä¿å­˜æ¨¡å‹ã€‚


**å¯¹æ–°æ‰‹æ¥è¯´ï¼Œå…¶ä¸­æœ€å›°éš¾çš„éƒ¨åˆ†å®é™…ä¸Šæ˜¯å‡†å¤‡æ•°æ®è¿‡ç¨‹ã€‚** 

æˆ‘ä»¬åœ¨å®è·µä¸­é€šå¸¸ä¼šé‡åˆ°çš„æ•°æ®ç±»å‹åŒ…æ‹¬ç»“æ„åŒ–æ•°æ®ï¼Œå›¾ç‰‡æ•°æ®ï¼Œæ–‡æœ¬æ•°æ®ï¼Œæ—¶é—´åºåˆ—æ•°æ®ã€‚

æˆ‘ä»¬å°†åˆ†åˆ«ä»¥titanicç”Ÿå­˜é¢„æµ‹é—®é¢˜ï¼Œcifar2å›¾ç‰‡åˆ†ç±»é—®é¢˜ï¼Œimdbç”µå½±è¯„è®ºåˆ†ç±»é—®é¢˜ï¼Œå›½å†…æ–°å† ç–«æƒ…ç»“æŸæ—¶é—´é¢„æµ‹é—®é¢˜ä¸ºä¾‹ï¼Œæ¼”ç¤ºåº”ç”¨tensorflowå¯¹è¿™å››ç±»æ•°æ®çš„å»ºæ¨¡æ–¹æ³•ã€‚




å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·"Pythonä¸ç®—æ³•ä¹‹ç¾"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚

![](./data/Pythonä¸ç®—æ³•ä¹‹ç¾logo.jpg)
# 1-1,ç»“æ„åŒ–æ•°æ®å»ºæ¨¡æµç¨‹èŒƒä¾‹


### ä¸€ï¼Œå‡†å¤‡æ•°æ®


titanicæ•°æ®é›†çš„ç›®æ ‡æ˜¯æ ¹æ®ä¹˜å®¢ä¿¡æ¯é¢„æµ‹ä»–ä»¬åœ¨Titanicå·æ’å‡»å†°å±±æ²‰æ²¡åèƒ½å¦ç”Ÿå­˜ã€‚

ç»“æ„åŒ–æ•°æ®ä¸€èˆ¬ä¼šä½¿ç”¨Pandasä¸­çš„DataFrameè¿›è¡Œé¢„å¤„ç†ã€‚


```python
import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt
import tensorflow as tf 
from tensorflow.keras import models,layers

dftrain_raw = pd.read_csv('./data/titanic/train.csv')
dftest_raw = pd.read_csv('./data/titanic/test.csv')
dftrain_raw.head(10)
```

![](./data/1-1-æ•°æ®é›†å±•ç¤º.jpg)


å­—æ®µè¯´æ˜ï¼š

* Survived:0ä»£è¡¨æ­»äº¡ï¼Œ1ä»£è¡¨å­˜æ´»ã€yæ ‡ç­¾ã€‘
* Pclass:ä¹˜å®¢æ‰€æŒç¥¨ç±»ï¼Œæœ‰ä¸‰ç§å€¼(1,2,3) ã€è½¬æ¢æˆonehotç¼–ç ã€‘
* Name:ä¹˜å®¢å§“å ã€èˆå»ã€‘
* Sex:ä¹˜å®¢æ€§åˆ« ã€è½¬æ¢æˆboolç‰¹å¾ã€‘
* Age:ä¹˜å®¢å¹´é¾„(æœ‰ç¼ºå¤±) ã€æ•°å€¼ç‰¹å¾ï¼Œæ·»åŠ â€œå¹´é¾„æ˜¯å¦ç¼ºå¤±â€ä½œä¸ºè¾…åŠ©ç‰¹å¾ã€‘
* SibSp:ä¹˜å®¢å…„å¼Ÿå§å¦¹/é…å¶çš„ä¸ªæ•°(æ•´æ•°å€¼) ã€æ•°å€¼ç‰¹å¾ã€‘
* Parch:ä¹˜å®¢çˆ¶æ¯/å­©å­çš„ä¸ªæ•°(æ•´æ•°å€¼)ã€æ•°å€¼ç‰¹å¾ã€‘
* Ticket:ç¥¨å·(å­—ç¬¦ä¸²)ã€èˆå»ã€‘
* Fare:ä¹˜å®¢æ‰€æŒç¥¨çš„ä»·æ ¼(æµ®ç‚¹æ•°ï¼Œ0-500ä¸ç­‰) ã€æ•°å€¼ç‰¹å¾ã€‘
* Cabin:ä¹˜å®¢æ‰€åœ¨èˆ¹èˆ±(æœ‰ç¼ºå¤±) ã€æ·»åŠ â€œæ‰€åœ¨èˆ¹èˆ±æ˜¯å¦ç¼ºå¤±â€ä½œä¸ºè¾…åŠ©ç‰¹å¾ã€‘
* Embarked:ä¹˜å®¢ç™»èˆ¹æ¸¯å£:Sã€Cã€Q(æœ‰ç¼ºå¤±)ã€è½¬æ¢æˆonehotç¼–ç ï¼Œå››ç»´åº¦ S,C,Q,nanã€‘



åˆ©ç”¨Pandasçš„æ•°æ®å¯è§†åŒ–åŠŸèƒ½æˆ‘ä»¬å¯ä»¥ç®€å•åœ°è¿›è¡Œæ¢ç´¢æ€§æ•°æ®åˆ†æEDAï¼ˆExploratory Data Analysisï¼‰ã€‚

labelåˆ†å¸ƒæƒ…å†µ

```python
%matplotlib inline
%config InlineBackend.figure_format = 'png'
ax = dftrain_raw['Survived'].value_counts().plot(kind = 'bar',
     figsize = (12,8),fontsize=15,rot = 0)
ax.set_ylabel('Counts',fontsize = 15)
ax.set_xlabel('Survived',fontsize = 15)
plt.show()
```

![](./data/1-1-Labelåˆ†å¸ƒ.jpg)


å¹´é¾„åˆ†å¸ƒæƒ…å†µ

```python
%matplotlib inline
%config InlineBackend.figure_format = 'png'
ax = dftrain_raw['Age'].plot(kind = 'hist',bins = 20,color= 'purple',
                    figsize = (12,8),fontsize=15)

ax.set_ylabel('Frequency',fontsize = 15)
ax.set_xlabel('Age',fontsize = 15)
plt.show()
```

![](./data/1-1-å¹´é¾„åˆ†å¸ƒ.jpg)


å¹´é¾„å’Œlabelçš„ç›¸å…³æ€§

```python
%matplotlib inline
%config InlineBackend.figure_format = 'png'
ax = dftrain_raw.query('Survived == 0')['Age'].plot(kind = 'density',
                      figsize = (12,8),fontsize=15)
dftrain_raw.query('Survived == 1')['Age'].plot(kind = 'density',
                      figsize = (12,8),fontsize=15)
ax.legend(['Survived==0','Survived==1'],fontsize = 12)
ax.set_ylabel('Density',fontsize = 15)
ax.set_xlabel('Age',fontsize = 15)
plt.show()
```

![](./data/1-1-å¹´é¾„ç›¸å…³æ€§.jpg)


ä¸‹é¢ä¸ºæ­£å¼çš„æ•°æ®é¢„å¤„ç†

```python
def preprocessing(dfdata):

    dfresult= pd.DataFrame()

    #Pclass
    dfPclass = pd.get_dummies(dfdata['Pclass'])
    dfPclass.columns = ['Pclass_' +str(x) for x in dfPclass.columns ]
    dfresult = pd.concat([dfresult,dfPclass],axis = 1)

    #Sex
    dfSex = pd.get_dummies(dfdata['Sex'])
    dfresult = pd.concat([dfresult,dfSex],axis = 1)

    #Age
    dfresult['Age'] = dfdata['Age'].fillna(0)
    dfresult['Age_null'] = pd.isna(dfdata['Age']).astype('int32')

    #SibSp,Parch,Fare
    dfresult['SibSp'] = dfdata['SibSp']
    dfresult['Parch'] = dfdata['Parch']
    dfresult['Fare'] = dfdata['Fare']

    #Carbin
    dfresult['Cabin_null'] =  pd.isna(dfdata['Cabin']).astype('int32')

    #Embarked
    dfEmbarked = pd.get_dummies(dfdata['Embarked'],dummy_na=True)
    dfEmbarked.columns = ['Embarked_' + str(x) for x in dfEmbarked.columns]
    dfresult = pd.concat([dfresult,dfEmbarked],axis = 1)

    return(dfresult)

x_train = preprocessing(dftrain_raw)
y_train = dftrain_raw['Survived'].values

x_test = preprocessing(dftest_raw)
y_test = dftest_raw['Survived'].values

print("x_train.shape =", x_train.shape )
print("x_test.shape =", x_test.shape )

```

```
x_train.shape = (712, 15)
x_test.shape = (179, 15)
```

```python

```

### äºŒï¼Œå®šä¹‰æ¨¡å‹


ä½¿ç”¨Kerasæ¥å£æœ‰ä»¥ä¸‹3ç§æ–¹å¼æ„å»ºæ¨¡å‹ï¼šä½¿ç”¨SequentialæŒ‰å±‚é¡ºåºæ„å»ºæ¨¡å‹ï¼Œä½¿ç”¨å‡½æ•°å¼APIæ„å»ºä»»æ„ç»“æ„æ¨¡å‹ï¼Œç»§æ‰¿ModelåŸºç±»æ„å»ºè‡ªå®šä¹‰æ¨¡å‹ã€‚

æ­¤å¤„é€‰æ‹©ä½¿ç”¨æœ€ç®€å•çš„Sequentialï¼ŒæŒ‰å±‚é¡ºåºæ¨¡å‹ã€‚

```python
tf.keras.backend.clear_session()

model = models.Sequential()
model.add(layers.Dense(20,activation = 'relu',input_shape=(15,)))
model.add(layers.Dense(10,activation = 'relu' ))
model.add(layers.Dense(1,activation = 'sigmoid' ))

model.summary()
```

```
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense (Dense)                (None, 20)                320       
_________________________________________________________________
dense_1 (Dense)              (None, 10)                210       
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 11        
=================================================================
Total params: 541
Trainable params: 541
Non-trainable params: 0
_________________________________________________________________
```


### ä¸‰ï¼Œè®­ç»ƒæ¨¡å‹


è®­ç»ƒæ¨¡å‹é€šå¸¸æœ‰3ç§æ–¹æ³•ï¼Œå†…ç½®fitæ–¹æ³•ï¼Œå†…ç½®train_on_batchæ–¹æ³•ï¼Œä»¥åŠè‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ã€‚æ­¤å¤„æˆ‘ä»¬é€‰æ‹©æœ€å¸¸ç”¨ä¹Ÿæœ€ç®€å•çš„å†…ç½®fitæ–¹æ³•ã€‚

```python
# äºŒåˆ†ç±»é—®é¢˜é€‰æ‹©äºŒå…ƒäº¤å‰ç†µæŸå¤±å‡½æ•°
model.compile(optimizer='adam',
            loss='binary_crossentropy',
            metrics=['AUC'])

history = model.fit(x_train,y_train,
                    batch_size= 64,
                    epochs= 30,
                    validation_split=0.2 #åˆ†å‰²ä¸€éƒ¨åˆ†è®­ç»ƒæ•°æ®ç”¨äºéªŒè¯
                   )
```

```
Train on 569 samples, validate on 143 samples
Epoch 1/30
569/569 [==============================] - 1s 2ms/sample - loss: 3.5841 - AUC: 0.4079 - val_loss: 3.4429 - val_AUC: 0.4129
Epoch 2/30
569/569 [==============================] - 0s 102us/sample - loss: 2.6093 - AUC: 0.3967 - val_loss: 2.4886 - val_AUC: 0.4139
Epoch 3/30
569/569 [==============================] - 0s 68us/sample - loss: 1.8375 - AUC: 0.4003 - val_loss: 1.7383 - val_AUC: 0.4223
Epoch 4/30
569/569 [==============================] - 0s 83us/sample - loss: 1.2545 - AUC: 0.4390 - val_loss: 1.1936 - val_AUC: 0.4765
Epoch 5/30
569/569 [==============================] - ETA: 0s - loss: 1.4435 - AUC: 0.375 - 0s 90us/sample - loss: 0.9141 - AUC: 0.5192 - val_loss: 0.8274 - val_AUC: 0.5584
Epoch 6/30
569/569 [==============================] - 0s 110us/sample - loss: 0.7052 - AUC: 0.6290 - val_loss: 0.6596 - val_AUC: 0.6880
Epoch 7/30
569/569 [==============================] - 0s 90us/sample - loss: 0.6410 - AUC: 0.7086 - val_loss: 0.6519 - val_AUC: 0.6845
Epoch 8/30
569/569 [==============================] - 0s 93us/sample - loss: 0.6246 - AUC: 0.7080 - val_loss: 0.6480 - val_AUC: 0.6846
Epoch 9/30
569/569 [==============================] - 0s 73us/sample - loss: 0.6088 - AUC: 0.7113 - val_loss: 0.6497 - val_AUC: 0.6838
Epoch 10/30
569/569 [==============================] - 0s 79us/sample - loss: 0.6051 - AUC: 0.7117 - val_loss: 0.6454 - val_AUC: 0.6873
Epoch 11/30
569/569 [==============================] - 0s 96us/sample - loss: 0.5972 - AUC: 0.7218 - val_loss: 0.6369 - val_AUC: 0.6888
Epoch 12/30
569/569 [==============================] - 0s 92us/sample - loss: 0.5918 - AUC: 0.7294 - val_loss: 0.6330 - val_AUC: 0.6908
Epoch 13/30
569/569 [==============================] - 0s 75us/sample - loss: 0.5864 - AUC: 0.7363 - val_loss: 0.6281 - val_AUC: 0.6948
Epoch 14/30
569/569 [==============================] - 0s 104us/sample - loss: 0.5832 - AUC: 0.7426 - val_loss: 0.6240 - val_AUC: 0.7030
Epoch 15/30
569/569 [==============================] - 0s 74us/sample - loss: 0.5777 - AUC: 0.7507 - val_loss: 0.6200 - val_AUC: 0.7066
Epoch 16/30
569/569 [==============================] - 0s 79us/sample - loss: 0.5726 - AUC: 0.7569 - val_loss: 0.6155 - val_AUC: 0.7132
Epoch 17/30
569/569 [==============================] - 0s 99us/sample - loss: 0.5674 - AUC: 0.7643 - val_loss: 0.6070 - val_AUC: 0.7255
Epoch 18/30
569/569 [==============================] - 0s 97us/sample - loss: 0.5631 - AUC: 0.7721 - val_loss: 0.6061 - val_AUC: 0.7305
Epoch 19/30
569/569 [==============================] - 0s 73us/sample - loss: 0.5580 - AUC: 0.7792 - val_loss: 0.6027 - val_AUC: 0.7332
Epoch 20/30
569/569 [==============================] - 0s 85us/sample - loss: 0.5533 - AUC: 0.7861 - val_loss: 0.5997 - val_AUC: 0.7366
Epoch 21/30
569/569 [==============================] - 0s 87us/sample - loss: 0.5497 - AUC: 0.7926 - val_loss: 0.5961 - val_AUC: 0.7433
Epoch 22/30
569/569 [==============================] - 0s 101us/sample - loss: 0.5454 - AUC: 0.7987 - val_loss: 0.5943 - val_AUC: 0.7438
Epoch 23/30
569/569 [==============================] - 0s 100us/sample - loss: 0.5398 - AUC: 0.8057 - val_loss: 0.5926 - val_AUC: 0.7492
Epoch 24/30
569/569 [==============================] - 0s 79us/sample - loss: 0.5328 - AUC: 0.8122 - val_loss: 0.5912 - val_AUC: 0.7493
Epoch 25/30
569/569 [==============================] - 0s 86us/sample - loss: 0.5283 - AUC: 0.8147 - val_loss: 0.5902 - val_AUC: 0.7509
Epoch 26/30
569/569 [==============================] - 0s 67us/sample - loss: 0.5246 - AUC: 0.8196 - val_loss: 0.5845 - val_AUC: 0.7552
Epoch 27/30
569/569 [==============================] - 0s 72us/sample - loss: 0.5205 - AUC: 0.8271 - val_loss: 0.5837 - val_AUC: 0.7584
Epoch 28/30
569/569 [==============================] - 0s 74us/sample - loss: 0.5144 - AUC: 0.8302 - val_loss: 0.5848 - val_AUC: 0.7561
Epoch 29/30
569/569 [==============================] - 0s 77us/sample - loss: 0.5099 - AUC: 0.8326 - val_loss: 0.5809 - val_AUC: 0.7583
Epoch 30/30
569/569 [==============================] - 0s 80us/sample - loss: 0.5071 - AUC: 0.8349 - val_loss: 0.5816 - val_AUC: 0.7605

```


### å››ï¼Œè¯„ä¼°æ¨¡å‹


æˆ‘ä»¬é¦–å…ˆè¯„ä¼°ä¸€ä¸‹æ¨¡å‹åœ¨è®­ç»ƒé›†å’ŒéªŒè¯é›†ä¸Šçš„æ•ˆæœã€‚

```python
%matplotlib inline
%config InlineBackend.figure_format = 'svg'

import matplotlib.pyplot as plt

def plot_metric(history, metric):
    train_metrics = history.history[metric]
    val_metrics = history.history['val_'+metric]
    epochs = range(1, len(train_metrics) + 1)
    plt.plot(epochs, train_metrics, 'bo--')
    plt.plot(epochs, val_metrics, 'ro-')
    plt.title('Training and validation '+ metric)
    plt.xlabel("Epochs")
    plt.ylabel(metric)
    plt.legend(["train_"+metric, 'val_'+metric])
    plt.show()
```

```python
plot_metric(history,"loss")
```

![](./data/1-1-Lossæ›²çº¿.jpg)

```python
plot_metric(history,"AUC")
```

![](./data/1-1-AUCæ›²çº¿.jpg)


æˆ‘ä»¬å†çœ‹ä¸€ä¸‹æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„æ•ˆæœ.

```python
model.evaluate(x = x_test,y = y_test)
```

```
[0.5191367897907448, 0.8122605]
```

```python

```

### äº”ï¼Œä½¿ç”¨æ¨¡å‹

```python
#é¢„æµ‹æ¦‚ç‡
model.predict(x_test[0:10])
#model(tf.constant(x_test[0:10].values,dtype = tf.float32)) #ç­‰ä»·å†™æ³•
```

```
array([[0.26501188],
       [0.40970832],
       [0.44285864],
       [0.78408605],
       [0.47650957],
       [0.43849158],
       [0.27426785],
       [0.5962582 ],
       [0.59476686],
       [0.17882936]], dtype=float32)
```

```python
#é¢„æµ‹ç±»åˆ«
model.predict_classes(x_test[0:10])
```

```
array([[0],
       [0],
       [0],
       [1],
       [0],
       [0],
       [0],
       [1],
       [1],
       [0]], dtype=int32)
```

```python

```

### å…­ï¼Œä¿å­˜æ¨¡å‹


å¯ä»¥ä½¿ç”¨Kerasæ–¹å¼ä¿å­˜æ¨¡å‹ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨TensorFlowåŸç”Ÿæ–¹å¼ä¿å­˜ã€‚å‰è€…ä»…ä»…é€‚åˆä½¿ç”¨Pythonç¯å¢ƒæ¢å¤æ¨¡å‹ï¼Œåè€…åˆ™å¯ä»¥è·¨å¹³å°è¿›è¡Œæ¨¡å‹éƒ¨ç½²ã€‚

æ¨èä½¿ç”¨åä¸€ç§æ–¹å¼è¿›è¡Œä¿å­˜ã€‚


**1ï¼ŒKerasæ–¹å¼ä¿å­˜**

```python
# ä¿å­˜æ¨¡å‹ç»“æ„åŠæƒé‡

model.save('./data/keras_model.h5')  

del model  #åˆ é™¤ç°æœ‰æ¨¡å‹

# identical to the previous one
model = models.load_model('./data/keras_model.h5')
model.evaluate(x_test,y_test)
```

```
[0.5191367897907448, 0.8122605]
```

```python
# ä¿å­˜æ¨¡å‹ç»“æ„
json_str = model.to_json()

# æ¢å¤æ¨¡å‹ç»“æ„
model_json = models.model_from_json(json_str)
```

```python
#ä¿å­˜æ¨¡å‹æƒé‡
model.save_weights('./data/keras_model_weight.h5')

# æ¢å¤æ¨¡å‹ç»“æ„
model_json = models.model_from_json(json_str)
model_json.compile(
        optimizer='adam',
        loss='binary_crossentropy',
        metrics=['AUC']
    )

# åŠ è½½æƒé‡
model_json.load_weights('./data/keras_model_weight.h5')
model_json.evaluate(x_test,y_test)
```

```
[0.5191367897907448, 0.8122605]
```


**2ï¼ŒTensorFlowåŸç”Ÿæ–¹å¼ä¿å­˜**

```python
# ä¿å­˜æƒé‡ï¼Œè¯¥æ–¹å¼ä»…ä»…ä¿å­˜æƒé‡å¼ é‡
model.save_weights('./data/tf_model_weights.ckpt',save_format = "tf")
```

```python
# ä¿å­˜æ¨¡å‹ç»“æ„ä¸æ¨¡å‹å‚æ•°åˆ°æ–‡ä»¶,è¯¥æ–¹å¼ä¿å­˜çš„æ¨¡å‹å…·æœ‰è·¨å¹³å°æ€§ä¾¿äºéƒ¨ç½²

model.save('./data/tf_model_savedmodel', save_format="tf")
print('export saved model.')

model_loaded = tf.keras.models.load_model('./data/tf_model_savedmodel')
model_loaded.evaluate(x_test,y_test)
```

```
[0.5191365896656527, 0.8122605]
```


å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·"Pythonä¸ç®—æ³•ä¹‹ç¾"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚

![image.png](./data/Pythonä¸ç®—æ³•ä¹‹ç¾logo.jpg)

```python

```
# 1-2,å›¾ç‰‡æ•°æ®å»ºæ¨¡æµç¨‹èŒƒä¾‹


### ä¸€ï¼Œå‡†å¤‡æ•°æ®


cifar2æ•°æ®é›†ä¸ºcifar10æ•°æ®é›†çš„å­é›†ï¼ŒåªåŒ…æ‹¬å‰ä¸¤ç§ç±»åˆ«airplaneå’Œautomobileã€‚

è®­ç»ƒé›†æœ‰airplaneå’Œautomobileå›¾ç‰‡å„5000å¼ ï¼Œæµ‹è¯•é›†æœ‰airplaneå’Œautomobileå›¾ç‰‡å„1000å¼ ã€‚

cifar2ä»»åŠ¡çš„ç›®æ ‡æ˜¯è®­ç»ƒä¸€ä¸ªæ¨¡å‹æ¥å¯¹é£æœºairplaneå’ŒæœºåŠ¨è½¦automobileä¸¤ç§å›¾ç‰‡è¿›è¡Œåˆ†ç±»ã€‚

æˆ‘ä»¬å‡†å¤‡çš„Cifar2æ•°æ®é›†çš„æ–‡ä»¶ç»“æ„å¦‚ä¸‹æ‰€ç¤ºã€‚

![](./data/cifar2.jpg)

```python

```

åœ¨tensorflowä¸­å‡†å¤‡å›¾ç‰‡æ•°æ®çš„å¸¸ç”¨æ–¹æ¡ˆæœ‰ä¸¤ç§ï¼Œç¬¬ä¸€ç§æ˜¯ä½¿ç”¨tf.kerasä¸­çš„ImageDataGeneratorå·¥å…·æ„å»ºå›¾ç‰‡æ•°æ®ç”Ÿæˆå™¨ã€‚

ç¬¬äºŒç§æ˜¯ä½¿ç”¨tf.data.Datasetæ­é…tf.imageä¸­çš„ä¸€äº›å›¾ç‰‡å¤„ç†æ–¹æ³•æ„å»ºæ•°æ®ç®¡é“ã€‚

ç¬¬ä¸€ç§æ–¹æ³•æ›´ä¸ºç®€å•ï¼Œå…¶ä½¿ç”¨èŒƒä¾‹å¯ä»¥å‚è€ƒä»¥ä¸‹æ–‡ç« ã€‚

https://zhuanlan.zhihu.com/p/67466552

ç¬¬äºŒç§æ–¹æ³•æ˜¯TensorFlowçš„åŸç”Ÿæ–¹æ³•ï¼Œæ›´åŠ çµæ´»ï¼Œä½¿ç”¨å¾—å½“çš„è¯ä¹Ÿå¯ä»¥è·å¾—æ›´å¥½çš„æ€§èƒ½ã€‚

æˆ‘ä»¬æ­¤å¤„ä»‹ç»ç¬¬äºŒç§æ–¹æ³•ã€‚


```python
import tensorflow as tf 
from tensorflow.keras import datasets,layers,models

BATCH_SIZE = 100

def load_image(img_path,size = (32,32)):
    label = tf.constant(1,tf.int8) if tf.strings.regex_full_match(img_path,".*/automobile/.*") \
            else tf.constant(0,tf.int8)
    img = tf.io.read_file(img_path)
    img = tf.image.decode_jpeg(img) #æ³¨æ„æ­¤å¤„ä¸ºjpegæ ¼å¼
    img = tf.image.resize(img,size)/255.0
    return(img,label)

```

```python
#ä½¿ç”¨å¹¶è¡ŒåŒ–é¢„å¤„ç†num_parallel_calls å’Œé¢„å­˜æ•°æ®prefetchæ¥æå‡æ€§èƒ½
ds_train = tf.data.Dataset.list_files("./data/cifar2/train/*/*.jpg") \
           .map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE) \
           .shuffle(buffer_size = 1000).batch(BATCH_SIZE) \
           .prefetch(tf.data.experimental.AUTOTUNE)  

ds_test = tf.data.Dataset.list_files("./data/cifar2/test/*/*.jpg") \
           .map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE) \
           .batch(BATCH_SIZE) \
           .prefetch(tf.data.experimental.AUTOTUNE)  

```

```python
%matplotlib inline
%config InlineBackend.figure_format = 'svg'

#æŸ¥çœ‹éƒ¨åˆ†æ ·æœ¬
from matplotlib import pyplot as plt 

plt.figure(figsize=(8,8)) 
for i,(img,label) in enumerate(ds_train.unbatch().take(9)):
    ax=plt.subplot(3,3,i+1)
    ax.imshow(img.numpy())
    ax.set_title("label = %d"%label)
    ax.set_xticks([])
    ax.set_yticks([]) 
plt.show()

```

![](./data/1-2-å›¾ç‰‡é¢„è§ˆ.jpg)

```python
for x,y in ds_train.take(1):
    print(x.shape,y.shape)
```

```
(100, 32, 32, 3) (100,)
```

```python

```

### äºŒï¼Œå®šä¹‰æ¨¡å‹


ä½¿ç”¨Kerasæ¥å£æœ‰ä»¥ä¸‹3ç§æ–¹å¼æ„å»ºæ¨¡å‹ï¼šä½¿ç”¨SequentialæŒ‰å±‚é¡ºåºæ„å»ºæ¨¡å‹ï¼Œä½¿ç”¨å‡½æ•°å¼APIæ„å»ºä»»æ„ç»“æ„æ¨¡å‹ï¼Œç»§æ‰¿ModelåŸºç±»æ„å»ºè‡ªå®šä¹‰æ¨¡å‹ã€‚

æ­¤å¤„é€‰æ‹©ä½¿ç”¨å‡½æ•°å¼APIæ„å»ºæ¨¡å‹ã€‚

```python
tf.keras.backend.clear_session() #æ¸…ç©ºä¼šè¯

inputs = layers.Input(shape=(32,32,3))
x = layers.Conv2D(32,kernel_size=(3,3))(inputs)
x = layers.MaxPool2D()(x)
x = layers.Conv2D(64,kernel_size=(5,5))(x)
x = layers.MaxPool2D()(x)
x = layers.Dropout(rate=0.1)(x)
x = layers.Flatten()(x)
x = layers.Dense(32,activation='relu')(x)
outputs = layers.Dense(1,activation = 'sigmoid')(x)

model = models.Model(inputs = inputs,outputs = outputs)

model.summary()
```

```
Model: "model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 32, 32, 3)]       0         
_________________________________________________________________
conv2d (Conv2D)              (None, 30, 30, 32)        896       
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 15, 15, 32)        0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 11, 11, 64)        51264     
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         
_________________________________________________________________
dropout (Dropout)            (None, 5, 5, 64)          0         
_________________________________________________________________
flatten (Flatten)            (None, 1600)              0         
_________________________________________________________________
dense (Dense)                (None, 32)                51232     
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 33        
=================================================================
Total params: 103,425
Trainable params: 103,425
Non-trainable params: 0
_________________________________________________________________
```

```python

```

### ä¸‰ï¼Œè®­ç»ƒæ¨¡å‹


è®­ç»ƒæ¨¡å‹é€šå¸¸æœ‰3ç§æ–¹æ³•ï¼Œå†…ç½®fitæ–¹æ³•ï¼Œå†…ç½®train_on_batchæ–¹æ³•ï¼Œä»¥åŠè‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ã€‚æ­¤å¤„æˆ‘ä»¬é€‰æ‹©æœ€å¸¸ç”¨ä¹Ÿæœ€ç®€å•çš„å†…ç½®fitæ–¹æ³•ã€‚

```python
import datetime

logdir = "./data/keras_model/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)

model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
        loss=tf.keras.losses.binary_crossentropy,
        metrics=["accuracy"]
    )

history = model.fit(ds_train,epochs= 10,validation_data=ds_test,
                    callbacks = [tensorboard_callback],workers = 4)

```

```
Train for 100 steps, validate for 20 steps
Epoch 1/10
100/100 [==============================] - 16s 156ms/step - loss: 0.4830 - accuracy: 0.7697 - val_loss: 0.3396 - val_accuracy: 0.8475
Epoch 2/10
100/100 [==============================] - 14s 142ms/step - loss: 0.3437 - accuracy: 0.8469 - val_loss: 0.2997 - val_accuracy: 0.8680
Epoch 3/10
100/100 [==============================] - 13s 131ms/step - loss: 0.2871 - accuracy: 0.8777 - val_loss: 0.2390 - val_accuracy: 0.9015
Epoch 4/10
100/100 [==============================] - 12s 117ms/step - loss: 0.2410 - accuracy: 0.9040 - val_loss: 0.2005 - val_accuracy: 0.9195
Epoch 5/10
100/100 [==============================] - 13s 130ms/step - loss: 0.1992 - accuracy: 0.9213 - val_loss: 0.1949 - val_accuracy: 0.9180
Epoch 6/10
100/100 [==============================] - 14s 136ms/step - loss: 0.1737 - accuracy: 0.9323 - val_loss: 0.1723 - val_accuracy: 0.9275
Epoch 7/10
100/100 [==============================] - 14s 139ms/step - loss: 0.1531 - accuracy: 0.9412 - val_loss: 0.1670 - val_accuracy: 0.9310
Epoch 8/10
100/100 [==============================] - 13s 134ms/step - loss: 0.1299 - accuracy: 0.9525 - val_loss: 0.1553 - val_accuracy: 0.9340
Epoch 9/10
100/100 [==============================] - 14s 137ms/step - loss: 0.1158 - accuracy: 0.9556 - val_loss: 0.1581 - val_accuracy: 0.9340
Epoch 10/10
100/100 [==============================] - 14s 142ms/step - loss: 0.1006 - accuracy: 0.9617 - val_loss: 0.1614 - val_accuracy: 0.9345
```

```python

```

### å››ï¼Œè¯„ä¼°æ¨¡å‹

```python
#%load_ext tensorboard
#%tensorboard --logdir ./data/keras_model
```

```python
from tensorboard import notebook
notebook.list() 
```

```python
#åœ¨tensorboardä¸­æŸ¥çœ‹æ¨¡å‹
notebook.start("--logdir ./data/keras_model")
```

```python

```

![](./data/1-2-tensorboard.jpg)

```python
import pandas as pd 
dfhistory = pd.DataFrame(history.history)
dfhistory.index = range(1,len(dfhistory) + 1)
dfhistory.index.name = 'epoch'

dfhistory
```

![](./data/1-2-dfhistory.jpg)

```python
%matplotlib inline
%config InlineBackend.figure_format = 'svg'

import matplotlib.pyplot as plt

def plot_metric(history, metric):
    train_metrics = history.history[metric]
    val_metrics = history.history['val_'+metric]
    epochs = range(1, len(train_metrics) + 1)
    plt.plot(epochs, train_metrics, 'bo--')
    plt.plot(epochs, val_metrics, 'ro-')
    plt.title('Training and validation '+ metric)
    plt.xlabel("Epochs")
    plt.ylabel(metric)
    plt.legend(["train_"+metric, 'val_'+metric])
    plt.show()
```

```python
plot_metric(history,"loss")
```

![](./data/1-2-Lossæ›²çº¿.jpg)

```python
plot_metric(history,"accuracy")
```

![](./data/1-2-Accuracyæ›²çº¿.jpg)

```python
#å¯ä»¥ä½¿ç”¨evaluateå¯¹æ•°æ®è¿›è¡Œè¯„ä¼°
val_loss,val_accuracy = model.evaluate(ds_test,workers=4)
print(val_loss,val_accuracy)

```

```
0.16139143370091916 0.9345
```


### äº”ï¼Œä½¿ç”¨æ¨¡å‹


å¯ä»¥ä½¿ç”¨model.predict(ds_test)è¿›è¡Œé¢„æµ‹ã€‚

ä¹Ÿå¯ä»¥ä½¿ç”¨model.predict_on_batch(x_test)å¯¹ä¸€ä¸ªæ‰¹é‡è¿›è¡Œé¢„æµ‹ã€‚

```python
model.predict(ds_test)
```

```
array([[9.9996173e-01],
       [9.5104784e-01],
       [2.8648047e-04],
       ...,
       [1.1484033e-03],
       [3.5589080e-02],
       [9.8537153e-01]], dtype=float32)
```

```python
for x,y in ds_test.take(1):
    print(model.predict_on_batch(x[0:20]))
```

```
tf.Tensor(
[[3.8065155e-05]
 [8.8236779e-01]
 [9.1433197e-01]
 [9.9921846e-01]
 [6.4052093e-01]
 [4.9970779e-03]
 [2.6735585e-04]
 [9.9842811e-01]
 [7.9198682e-01]
 [7.4823302e-01]
 [8.7208226e-03]
 [9.3951421e-03]
 [9.9790359e-01]
 [9.9998581e-01]
 [2.1642199e-05]
 [1.7915063e-02]
 [2.5839690e-02]
 [9.7538447e-01]
 [9.7393811e-01]
 [9.7333014e-01]], shape=(20, 1), dtype=float32)
```




```python

```

### å…­ï¼Œä¿å­˜æ¨¡å‹


æ¨èä½¿ç”¨TensorFlowåŸç”Ÿæ–¹å¼ä¿å­˜æ¨¡å‹ã€‚

```python
# ä¿å­˜æƒé‡ï¼Œè¯¥æ–¹å¼ä»…ä»…ä¿å­˜æƒé‡å¼ é‡
model.save_weights('./data/tf_model_weights.ckpt',save_format = "tf")
```

```python
# ä¿å­˜æ¨¡å‹ç»“æ„ä¸æ¨¡å‹å‚æ•°åˆ°æ–‡ä»¶,è¯¥æ–¹å¼ä¿å­˜çš„æ¨¡å‹å…·æœ‰è·¨å¹³å°æ€§ä¾¿äºéƒ¨ç½²

model.save('./data/tf_model_savedmodel', save_format="tf")
print('export saved model.')

model_loaded = tf.keras.models.load_model('./data/tf_model_savedmodel')
model_loaded.evaluate(ds_test)
```

```
[0.16139124035835267, 0.9345]
```

```python

```

å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·"Pythonä¸ç®—æ³•ä¹‹ç¾"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚

![image.png](./data/Pythonä¸ç®—æ³•ä¹‹ç¾logo.jpg)
# 1-3,æ–‡æœ¬æ•°æ®å»ºæ¨¡æµç¨‹èŒƒä¾‹


### ä¸€ï¼Œå‡†å¤‡æ•°æ®


imdbæ•°æ®é›†çš„ç›®æ ‡æ˜¯æ ¹æ®ç”µå½±è¯„è®ºçš„æ–‡æœ¬å†…å®¹é¢„æµ‹è¯„è®ºçš„æƒ…æ„Ÿæ ‡ç­¾ã€‚

è®­ç»ƒé›†æœ‰20000æ¡ç”µå½±è¯„è®ºæ–‡æœ¬ï¼Œæµ‹è¯•é›†æœ‰5000æ¡ç”µå½±è¯„è®ºæ–‡æœ¬ï¼Œå…¶ä¸­æ­£é¢è¯„è®ºå’Œè´Ÿé¢è¯„è®ºéƒ½å„å ä¸€åŠã€‚

æ–‡æœ¬æ•°æ®é¢„å¤„ç†è¾ƒä¸ºç¹çï¼ŒåŒ…æ‹¬ä¸­æ–‡åˆ‡è¯ï¼ˆæœ¬ç¤ºä¾‹ä¸æ¶‰åŠï¼‰ï¼Œæ„å»ºè¯å…¸ï¼Œç¼–ç è½¬æ¢ï¼Œåºåˆ—å¡«å……ï¼Œæ„å»ºæ•°æ®ç®¡é“ç­‰ç­‰ã€‚



åœ¨tensorflowä¸­å®Œæˆæ–‡æœ¬æ•°æ®é¢„å¤„ç†çš„å¸¸ç”¨æ–¹æ¡ˆæœ‰ä¸¤ç§ï¼Œç¬¬ä¸€ç§æ˜¯åˆ©ç”¨tf.keras.preprocessingä¸­çš„Tokenizerè¯å…¸æ„å»ºå·¥å…·å’Œtf.keras.utils.Sequenceæ„å»ºæ–‡æœ¬æ•°æ®ç”Ÿæˆå™¨ç®¡é“ã€‚

ç¬¬äºŒç§æ˜¯ä½¿ç”¨tf.data.Datasetæ­é….keras.layers.experimental.preprocessing.TextVectorizationé¢„å¤„ç†å±‚ã€‚

ç¬¬ä¸€ç§æ–¹æ³•è¾ƒä¸ºå¤æ‚ï¼Œå…¶ä½¿ç”¨èŒƒä¾‹å¯ä»¥å‚è€ƒä»¥ä¸‹æ–‡ç« ã€‚

https://zhuanlan.zhihu.com/p/67697840

ç¬¬äºŒç§æ–¹æ³•ä¸ºTensorFlowåŸç”Ÿæ–¹å¼ï¼Œç›¸å¯¹ä¹Ÿæ›´åŠ ç®€å•ä¸€äº›ã€‚

æˆ‘ä»¬æ­¤å¤„ä»‹ç»ç¬¬äºŒç§æ–¹æ³•ã€‚


![](./data/ç”µå½±è¯„è®º.jpg)

```python
import numpy as np 
import pandas as pd 
from matplotlib import pyplot as plt
import tensorflow as tf
from tensorflow.keras import models,layers,preprocessing,optimizers,losses,metrics
from tensorflow.keras.layers.experimental.preprocessing import TextVectorization
import re,string

train_data_path = "./data/imdb/train.csv"
test_data_path =  "./data/imdb/test.csv"

MAX_WORDS = 10000  # ä»…è€ƒè™‘æœ€é«˜é¢‘çš„10000ä¸ªè¯
MAX_LEN = 200  # æ¯ä¸ªæ ·æœ¬ä¿ç•™200ä¸ªè¯çš„é•¿åº¦
BATCH_SIZE = 20 


#æ„å»ºç®¡é“
def split_line(line):
    arr = tf.strings.split(line,"\t")
    label = tf.expand_dims(tf.cast(tf.strings.to_number(arr[0]),tf.int32),axis = 0)
    text = tf.expand_dims(arr[1],axis = 0)
    return (text,label)

ds_train_raw =  tf.data.TextLineDataset(filenames = [train_data_path]) \
   .map(split_line,num_parallel_calls = tf.data.experimental.AUTOTUNE) \
   .shuffle(buffer_size = 1000).batch(BATCH_SIZE) \
   .prefetch(tf.data.experimental.AUTOTUNE)

ds_test_raw = tf.data.TextLineDataset(filenames = [test_data_path]) \
   .map(split_line,num_parallel_calls = tf.data.experimental.AUTOTUNE) \
   .batch(BATCH_SIZE) \
   .prefetch(tf.data.experimental.AUTOTUNE)


#æ„å»ºè¯å…¸
def clean_text(text):
    lowercase = tf.strings.lower(text)
    stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')
    cleaned_punctuation = tf.strings.regex_replace(stripped_html,
         '[%s]' % re.escape(string.punctuation),'')
    return cleaned_punctuation

vectorize_layer = TextVectorization(
    standardize=clean_text,
    split = 'whitespace',
    max_tokens=MAX_WORDS-1, #æœ‰ä¸€ä¸ªç•™ç»™å ä½ç¬¦
    output_mode='int',
    output_sequence_length=MAX_LEN)

ds_text = ds_train_raw.map(lambda text,label: text)
vectorize_layer.adapt(ds_text)
print(vectorize_layer.get_vocabulary()[0:100])


#å•è¯ç¼–ç 
ds_train = ds_train_raw.map(lambda text,label:(vectorize_layer(text),label)) \
    .prefetch(tf.data.experimental.AUTOTUNE)
ds_test = ds_test_raw.map(lambda text,label:(vectorize_layer(text),label)) \
    .prefetch(tf.data.experimental.AUTOTUNE)

```

```
[b'the', b'and', b'a', b'of', b'to', b'is', b'in', b'it', b'i', b'this', b'that', b'was', b'as', b'for', b'with', b'movie', b'but', b'film', b'on', b'not', b'you', b'his', b'are', b'have', b'be', b'he', b'one', b'its', b'at', b'all', b'by', b'an', b'they', b'from', b'who', b'so', b'like', b'her', b'just', b'or', b'about', b'has', b'if', b'out', b'some', b'there', b'what', b'good', b'more', b'when', b'very', b'she', b'even', b'my', b'no', b'would', b'up', b'time', b'only', b'which', b'story', b'really', b'their', b'were', b'had', b'see', b'can', b'me', b'than', b'we', b'much', b'well', b'get', b'been', b'will', b'into', b'people', b'also', b'other', b'do', b'bad', b'because', b'great', b'first', b'how', b'him', b'most', b'dont', b'made', b'then', b'them', b'films', b'movies', b'way', b'make', b'could', b'too', b'any', b'after', b'characters']
```

```python

```

### äºŒï¼Œå®šä¹‰æ¨¡å‹


ä½¿ç”¨Kerasæ¥å£æœ‰ä»¥ä¸‹3ç§æ–¹å¼æ„å»ºæ¨¡å‹ï¼šä½¿ç”¨SequentialæŒ‰å±‚é¡ºåºæ„å»ºæ¨¡å‹ï¼Œä½¿ç”¨å‡½æ•°å¼APIæ„å»ºä»»æ„ç»“æ„æ¨¡å‹ï¼Œç»§æ‰¿ModelåŸºç±»æ„å»ºè‡ªå®šä¹‰æ¨¡å‹ã€‚

æ­¤å¤„é€‰æ‹©ä½¿ç”¨ç»§æ‰¿ModelåŸºç±»æ„å»ºè‡ªå®šä¹‰æ¨¡å‹ã€‚

```python
# æ¼”ç¤ºè‡ªå®šä¹‰æ¨¡å‹èŒƒä¾‹ï¼Œå®é™…ä¸Šåº”è¯¥ä¼˜å…ˆä½¿ç”¨Sequentialæˆ–è€…å‡½æ•°å¼API

tf.keras.backend.clear_session()

class CnnModel(models.Model):
    def __init__(self):
        super(CnnModel, self).__init__()
        
    def build(self,input_shape):
        self.embedding = layers.Embedding(MAX_WORDS,7,input_length=MAX_LEN)
        self.conv_1 = layers.Conv1D(16, kernel_size= 5,name = "conv_1",activation = "relu")
        self.pool = layers.MaxPool1D()
        self.conv_2 = layers.Conv1D(128, kernel_size=2,name = "conv_2",activation = "relu")
        self.flatten = layers.Flatten()
        self.dense = layers.Dense(1,activation = "sigmoid")
        super(CnnModel,self).build(input_shape)
    
    def call(self, x):
        x = self.embedding(x)
        x = self.conv_1(x)
        x = self.pool(x)
        x = self.conv_2(x)
        x = self.pool(x)
        x = self.flatten(x)
        x = self.dense(x)
        return(x)
    
model = CnnModel()
model.build(input_shape =(None,MAX_LEN))
model.summary()

```

```python

```

```
Model: "cnn_model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        multiple                  70000     
_________________________________________________________________
conv_1 (Conv1D)              multiple                  576       
_________________________________________________________________
max_pooling1d (MaxPooling1D) multiple                  0         
_________________________________________________________________
conv_2 (Conv1D)              multiple                  4224      
_________________________________________________________________
flatten (Flatten)            multiple                  0         
_________________________________________________________________
dense (Dense)                multiple                  6145      
=================================================================
Total params: 80,945
Trainable params: 80,945
Non-trainable params: 0
_________________________________________________________________
```

```python

```

### ä¸‰ï¼Œè®­ç»ƒæ¨¡å‹


è®­ç»ƒæ¨¡å‹é€šå¸¸æœ‰3ç§æ–¹æ³•ï¼Œå†…ç½®fitæ–¹æ³•ï¼Œå†…ç½®train_on_batchæ–¹æ³•ï¼Œä»¥åŠè‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ã€‚æ­¤å¤„æˆ‘ä»¬é€šè¿‡è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯è®­ç»ƒæ¨¡å‹ã€‚

```python
#æ‰“å°æ—¶é—´åˆ†å‰²çº¿
@tf.function
def printbar():
    ts = tf.timestamp()
    today_ts = ts%(24*60*60)

    hour = tf.cast(today_ts//3600+8,tf.int32)%tf.constant(24)
    minite = tf.cast((today_ts%3600)//60,tf.int32)
    second = tf.cast(tf.floor(today_ts%60),tf.int32)
    
    def timeformat(m):
        if tf.strings.length(tf.strings.format("{}",m))==1:
            return(tf.strings.format("0{}",m))
        else:
            return(tf.strings.format("{}",m))
    
    timestring = tf.strings.join([timeformat(hour),timeformat(minite),
                timeformat(second)],separator = ":")
    tf.print("=========="*8,end = "")
    tf.print(timestring)
```

```python
optimizer = optimizers.Nadam()
loss_func = losses.BinaryCrossentropy()

train_loss = metrics.Mean(name='train_loss')
train_metric = metrics.BinaryAccuracy(name='train_accuracy')

valid_loss = metrics.Mean(name='valid_loss')
valid_metric = metrics.BinaryAccuracy(name='valid_accuracy')


@tf.function
def train_step(model, features, labels):
    with tf.GradientTape() as tape:
        predictions = model(features,training = True)
        loss = loss_func(labels, predictions)
    gradients = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))

    train_loss.update_state(loss)
    train_metric.update_state(labels, predictions)
    

@tf.function
def valid_step(model, features, labels):
    predictions = model(features,training = False)
    batch_loss = loss_func(labels, predictions)
    valid_loss.update_state(batch_loss)
    valid_metric.update_state(labels, predictions)


def train_model(model,ds_train,ds_valid,epochs):
    for epoch in tf.range(1,epochs+1):
        
        for features, labels in ds_train:
            train_step(model,features,labels)

        for features, labels in ds_valid:
            valid_step(model,features,labels)
        
        #æ­¤å¤„logsæ¨¡æ¿éœ€è¦æ ¹æ®metricå…·ä½“æƒ…å†µä¿®æ”¹
        logs = 'Epoch={},Loss:{},Accuracy:{},Valid Loss:{},Valid Accuracy:{}' 
        
        if epoch%1==0:
            printbar()
            tf.print(tf.strings.format(logs,
            (epoch,train_loss.result(),train_metric.result(),valid_loss.result(),valid_metric.result())))
            tf.print("")
        
        train_loss.reset_states()
        valid_loss.reset_states()
        train_metric.reset_states()
        valid_metric.reset_states()

train_model(model,ds_train,ds_test,epochs = 6)

```

```
================================================================================13:54:08
Epoch=1,Loss:0.442317516,Accuracy:0.7695,Valid Loss:0.323672801,Valid Accuracy:0.8614

================================================================================13:54:20
Epoch=2,Loss:0.245737702,Accuracy:0.90215,Valid Loss:0.356488883,Valid Accuracy:0.8554

================================================================================13:54:32
Epoch=3,Loss:0.17360799,Accuracy:0.93455,Valid Loss:0.361132562,Valid Accuracy:0.8674

================================================================================13:54:44
Epoch=4,Loss:0.113476314,Accuracy:0.95975,Valid Loss:0.483677238,Valid Accuracy:0.856

================================================================================13:54:57
Epoch=5,Loss:0.0698405355,Accuracy:0.9768,Valid Loss:0.607856631,Valid Accuracy:0.857

================================================================================13:55:15
Epoch=6,Loss:0.0366807655,Accuracy:0.98825,Valid Loss:0.745884955,Valid Accuracy:0.854
```


### å››ï¼Œè¯„ä¼°æ¨¡å‹


é€šè¿‡è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯è®­ç»ƒçš„æ¨¡å‹æ²¡æœ‰ç»è¿‡ç¼–è¯‘ï¼Œæ— æ³•ç›´æ¥ä½¿ç”¨model.evaluate(ds_valid)æ–¹æ³•

```python

def evaluate_model(model,ds_valid):
    for features, labels in ds_valid:
         valid_step(model,features,labels)
    logs = 'Valid Loss:{},Valid Accuracy:{}' 
    tf.print(tf.strings.format(logs,(valid_loss.result(),valid_metric.result())))
    
    valid_loss.reset_states()
    train_metric.reset_states()
    valid_metric.reset_states()

    
```

```python
evaluate_model(model,ds_test)
```

```
Valid Loss:0.745884418,Valid Accuracy:0.854
```

```python

```

### äº”ï¼Œä½¿ç”¨æ¨¡å‹


å¯ä»¥ä½¿ç”¨ä»¥ä¸‹æ–¹æ³•:

* model.predict(ds_test)
* model(x_test)
* model.call(x_test)
* model.predict_on_batch(x_test)

æ¨èä¼˜å…ˆä½¿ç”¨model.predict(ds_test)æ–¹æ³•ï¼Œæ—¢å¯ä»¥å¯¹Datasetï¼Œä¹Ÿå¯ä»¥å¯¹Tensorä½¿ç”¨ã€‚

```python
model.predict(ds_test)
```

```
array([[0.7864823 ],
       [0.9999901 ],
       [0.99944776],
       ...,
       [0.8498302 ],
       [0.13382755],
       [1.        ]], dtype=float32)
```

```python
for x_test,_ in ds_test.take(1):
    print(model(x_test))
    #ä»¥ä¸‹æ–¹æ³•ç­‰ä»·ï¼š
    #print(model.call(x_test))
    #print(model.predict_on_batch(x_test))
```

```
tf.Tensor(
[[7.8648227e-01]
 [9.9999011e-01]
 [9.9944776e-01]
 [3.7153201e-09]
 [9.4462049e-01]
 [2.3522753e-04]
 [1.2044354e-04]
 [9.3752089e-07]
 [9.9996352e-01]
 [9.3435925e-01]
 [9.8746723e-01]
 [9.9908626e-01]
 [4.1563155e-08]
 [4.1808244e-03]
 [8.0184749e-05]
 [8.3910513e-01]
 [3.5167937e-05]
 [7.2113985e-01]
 [4.5228912e-03]
 [9.9942589e-01]], shape=(20, 1), dtype=float32)
```

```python

```

### å…­ï¼Œä¿å­˜æ¨¡å‹


æ¨èä½¿ç”¨TensorFlowåŸç”Ÿæ–¹å¼ä¿å­˜æ¨¡å‹ã€‚

```python
model.save('./data/tf_model_savedmodel', save_format="tf")
print('export saved model.')

model_loaded = tf.keras.models.load_model('./data/tf_model_savedmodel')
model_loaded.predict(ds_test)
```

```
array([[0.7864823 ],
       [0.9999901 ],
       [0.99944776],
       ...,
       [0.8498302 ],
       [0.13382755],
       [1.        ]], dtype=float32)
```


å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·"Pythonä¸ç®—æ³•ä¹‹ç¾"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚

![image.png](./data/Pythonä¸ç®—æ³•ä¹‹ç¾logo.jpg)





# 1-4,æ—¶é—´åºåˆ—æ•°æ®å»ºæ¨¡æµç¨‹èŒƒä¾‹


å›½å†…çš„æ–°å† è‚ºç‚ç–«æƒ…ä»å‘ç°è‡³ä»Šå·²ç»æŒç»­3ä¸ªå¤šæœˆäº†ï¼Œè¿™åœºèµ·æºäºåƒé‡å‘³çš„ç¾éš¾ç»™å¤§å®¶çš„ç”Ÿæ´»é€ æˆäº†è¯¸å¤šæ–¹é¢çš„å½±å“ã€‚

æœ‰çš„åŒå­¦æ˜¯æ”¶å…¥ä¸Šçš„ï¼Œæœ‰çš„åŒå­¦æ˜¯æ„Ÿæƒ…ä¸Šçš„ï¼Œæœ‰çš„åŒå­¦æ˜¯å¿ƒç†ä¸Šçš„ï¼Œè¿˜æœ‰çš„åŒå­¦æ˜¯ä½“é‡ä¸Šçš„ã€‚

é‚£ä¹ˆå›½å†…çš„æ–°å† è‚ºç‚ç–«æƒ…ä½•æ—¶ç»“æŸå‘¢ï¼Ÿä»€ä¹ˆæ—¶å€™æˆ‘ä»¬æ‰å¯ä»¥é‡è·è‡ªç”±å‘¢ï¼Ÿ

æœ¬ç¯‡æ–‡ç« å°†åˆ©ç”¨TensorFlow2.0å»ºç«‹æ—¶é—´åºåˆ—RNNæ¨¡å‹ï¼Œå¯¹å›½å†…çš„æ–°å† è‚ºç‚ç–«æƒ…ç»“æŸæ—¶é—´è¿›è¡Œé¢„æµ‹ã€‚


![](./data/ç–«æƒ…å‰åå¯¹æ¯”.png)


### ä¸€ï¼Œå‡†å¤‡æ•°æ®


æœ¬æ–‡çš„æ•°æ®é›†å–è‡ªtushareï¼Œè·å–è¯¥æ•°æ®é›†çš„æ–¹æ³•å‚è€ƒäº†ä»¥ä¸‹æ–‡ç« ã€‚

ã€Šhttps://zhuanlan.zhihu.com/p/109556102ã€‹

![](./data/1-4-æ–°å¢äººæ•°.png)



```python
import numpy as np
import pandas as pd 
import matplotlib.pyplot as plt
import tensorflow as tf 
from tensorflow.keras import models,layers,losses,metrics,callbacks 

```

```python
%matplotlib inline
%config InlineBackend.figure_format = 'svg'

df = pd.read_csv("./data/covid-19.csv",sep = "\t")
df.plot(x = "date",y = ["confirmed_num","cured_num","dead_num"],figsize=(10,6))
plt.xticks(rotation=60)

```

![](./data/1-4-ç´¯ç§¯æ›²çº¿.png)

```python
dfdata = df.set_index("date")
dfdiff = dfdata.diff(periods=1).dropna()
dfdiff = dfdiff.reset_index("date")

dfdiff.plot(x = "date",y = ["confirmed_num","cured_num","dead_num"],figsize=(10,6))
plt.xticks(rotation=60)
dfdiff = dfdiff.drop("date",axis = 1).astype("float32")

```

![](./data/1-4-æ–°å¢æ›²çº¿.png)

```python
#ç”¨æŸæ—¥å‰8å¤©çª—å£æ•°æ®ä½œä¸ºè¾“å…¥é¢„æµ‹è¯¥æ—¥æ•°æ®
WINDOW_SIZE = 8

def batch_dataset(dataset):
    dataset_batched = dataset.batch(WINDOW_SIZE,drop_remainder=True)
    return dataset_batched

ds_data = tf.data.Dataset.from_tensor_slices(tf.constant(dfdiff.values,dtype = tf.float32)) \
   .window(WINDOW_SIZE,shift=1).flat_map(batch_dataset)

ds_label = tf.data.Dataset.from_tensor_slices(
    tf.constant(dfdiff.values[WINDOW_SIZE:],dtype = tf.float32))

#æ•°æ®è¾ƒå°ï¼Œå¯ä»¥å°†å…¨éƒ¨è®­ç»ƒæ•°æ®æ”¾å…¥åˆ°ä¸€ä¸ªbatchä¸­ï¼Œæå‡æ€§èƒ½
ds_train = tf.data.Dataset.zip((ds_data,ds_label)).batch(38).cache()


```

### äºŒï¼Œå®šä¹‰æ¨¡å‹


ä½¿ç”¨Kerasæ¥å£æœ‰ä»¥ä¸‹3ç§æ–¹å¼æ„å»ºæ¨¡å‹ï¼šä½¿ç”¨SequentialæŒ‰å±‚é¡ºåºæ„å»ºæ¨¡å‹ï¼Œä½¿ç”¨å‡½æ•°å¼APIæ„å»ºä»»æ„ç»“æ„æ¨¡å‹ï¼Œç»§æ‰¿ModelåŸºç±»æ„å»ºè‡ªå®šä¹‰æ¨¡å‹ã€‚

æ­¤å¤„é€‰æ‹©ä½¿ç”¨å‡½æ•°å¼APIæ„å»ºä»»æ„ç»“æ„æ¨¡å‹ã€‚

```python
#è€ƒè™‘åˆ°æ–°å¢ç¡®è¯Šï¼Œæ–°å¢æ²»æ„ˆï¼Œæ–°å¢æ­»äº¡äººæ•°æ•°æ®ä¸å¯èƒ½å°äº0ï¼Œè®¾è®¡å¦‚ä¸‹ç»“æ„
class Block(layers.Layer):
    def __init__(self, **kwargs):
        super(Block, self).__init__(**kwargs)
    
    def call(self, x_input,x):
        x_out = tf.maximum((1+x)*x_input[:,-1,:],0.0)
        return x_out
    
    def get_config(self):  
        config = super(Block, self).get_config()
        return config

```

```python
tf.keras.backend.clear_session()
x_input = layers.Input(shape = (None,3),dtype = tf.float32)
x = layers.LSTM(3,return_sequences = True,input_shape=(None,3))(x_input)
x = layers.LSTM(3,return_sequences = True,input_shape=(None,3))(x)
x = layers.LSTM(3,return_sequences = True,input_shape=(None,3))(x)
x = layers.LSTM(3,input_shape=(None,3))(x)
x = layers.Dense(3)(x)

#è€ƒè™‘åˆ°æ–°å¢ç¡®è¯Šï¼Œæ–°å¢æ²»æ„ˆï¼Œæ–°å¢æ­»äº¡äººæ•°æ•°æ®ä¸å¯èƒ½å°äº0ï¼Œè®¾è®¡å¦‚ä¸‹ç»“æ„
#x = tf.maximum((1+x)*x_input[:,-1,:],0.0)
x = Block()(x_input,x)
model = models.Model(inputs = [x_input],outputs = [x])
model.summary()

```

```python

```

```
Model: "model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, None, 3)]         0         
_________________________________________________________________
lstm (LSTM)                  (None, None, 3)           84        
_________________________________________________________________
lstm_1 (LSTM)                (None, None, 3)           84        
_________________________________________________________________
lstm_2 (LSTM)                (None, None, 3)           84        
_________________________________________________________________
lstm_3 (LSTM)                (None, 3)                 84        
_________________________________________________________________
dense (Dense)                (None, 3)                 12        
_________________________________________________________________
block (Block)                (None, 3)                 0         
=================================================================
Total params: 348
Trainable params: 348
Non-trainable params: 0
_________________________________________________________________
```


### ä¸‰ï¼Œè®­ç»ƒæ¨¡å‹


è®­ç»ƒæ¨¡å‹é€šå¸¸æœ‰3ç§æ–¹æ³•ï¼Œå†…ç½®fitæ–¹æ³•ï¼Œå†…ç½®train_on_batchæ–¹æ³•ï¼Œä»¥åŠè‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ã€‚æ­¤å¤„æˆ‘ä»¬é€‰æ‹©æœ€å¸¸ç”¨ä¹Ÿæœ€ç®€å•çš„å†…ç½®fitæ–¹æ³•ã€‚

æ³¨ï¼šå¾ªç¯ç¥ç»ç½‘ç»œè°ƒè¯•è¾ƒä¸ºå›°éš¾ï¼Œéœ€è¦è®¾ç½®å¤šä¸ªä¸åŒçš„å­¦ä¹ ç‡å¤šæ¬¡å°è¯•ï¼Œä»¥å–å¾—è¾ƒå¥½çš„æ•ˆæœã€‚

```python
#è‡ªå®šä¹‰æŸå¤±å‡½æ•°ï¼Œè€ƒè™‘å¹³æ–¹å·®å’Œé¢„æµ‹ç›®æ ‡çš„æ¯”å€¼
class MSPE(losses.Loss):
    def call(self,y_true,y_pred):
        err_percent = (y_true - y_pred)**2/(tf.maximum(y_true**2,1e-7))
        mean_err_percent = tf.reduce_mean(err_percent)
        return mean_err_percent
    
    def get_config(self):
        config = super(MSPE, self).get_config()
        return config

```

```python
import datetime

optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)
model.compile(optimizer=optimizer,loss=MSPE(name = "MSPE"))

logdir = "./data/keras_model/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")

tb_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)
#å¦‚æœlossåœ¨100ä¸ªepochåæ²¡æœ‰æå‡ï¼Œå­¦ä¹ ç‡å‡åŠã€‚
lr_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor="loss",factor = 0.5, patience = 100)
#å½“lossåœ¨200ä¸ªepochåæ²¡æœ‰æå‡ï¼Œåˆ™æå‰ç»ˆæ­¢è®­ç»ƒã€‚
stop_callback = tf.keras.callbacks.EarlyStopping(monitor = "loss", patience= 200)
callbacks_list = [tb_callback,lr_callback,stop_callback]

history = model.fit(ds_train,epochs=500,callbacks = callbacks_list)

```

```
Epoch 371/500
1/1 [==============================] - 0s 61ms/step - loss: 0.1184
Epoch 372/500
1/1 [==============================] - 0s 64ms/step - loss: 0.1177
Epoch 373/500
1/1 [==============================] - 0s 56ms/step - loss: 0.1169
Epoch 374/500
1/1 [==============================] - 0s 50ms/step - loss: 0.1161
Epoch 375/500
1/1 [==============================] - 0s 55ms/step - loss: 0.1154
Epoch 376/500
1/1 [==============================] - 0s 55ms/step - loss: 0.1147
Epoch 377/500
1/1 [==============================] - 0s 62ms/step - loss: 0.1140
Epoch 378/500
1/1 [==============================] - 0s 93ms/step - loss: 0.1133
Epoch 379/500
1/1 [==============================] - 0s 85ms/step - loss: 0.1126
Epoch 380/500
1/1 [==============================] - 0s 68ms/step - loss: 0.1119
Epoch 381/500
1/1 [==============================] - 0s 52ms/step - loss: 0.1113
Epoch 382/500
1/1 [==============================] - 0s 54ms/step - loss: 0.1107
Epoch 383/500
1/1 [==============================] - 0s 55ms/step - loss: 0.1100
Epoch 384/500
1/1 [==============================] - 0s 56ms/step - loss: 0.1094
Epoch 385/500
1/1 [==============================] - 0s 54ms/step - loss: 0.1088
Epoch 386/500
1/1 [==============================] - 0s 74ms/step - loss: 0.1082
Epoch 387/500
1/1 [==============================] - 0s 60ms/step - loss: 0.1077
Epoch 388/500
1/1 [==============================] - 0s 52ms/step - loss: 0.1071
Epoch 389/500
1/1 [==============================] - 0s 52ms/step - loss: 0.1066
Epoch 390/500
1/1 [==============================] - 0s 56ms/step - loss: 0.1060
Epoch 391/500
1/1 [==============================] - 0s 61ms/step - loss: 0.1055
Epoch 392/500
1/1 [==============================] - 0s 60ms/step - loss: 0.1050
Epoch 393/500
1/1 [==============================] - 0s 59ms/step - loss: 0.1045
Epoch 394/500
1/1 [==============================] - 0s 65ms/step - loss: 0.1040
Epoch 395/500
1/1 [==============================] - 0s 58ms/step - loss: 0.1035
Epoch 396/500
1/1 [==============================] - 0s 52ms/step - loss: 0.1031
Epoch 397/500
1/1 [==============================] - 0s 58ms/step - loss: 0.1026
Epoch 398/500
1/1 [==============================] - 0s 60ms/step - loss: 0.1022
Epoch 399/500
1/1 [==============================] - 0s 57ms/step - loss: 0.1017
Epoch 400/500
1/1 [==============================] - 0s 63ms/step - loss: 0.1013
Epoch 401/500
1/1 [==============================] - 0s 59ms/step - loss: 0.1009
Epoch 402/500
1/1 [==============================] - 0s 53ms/step - loss: 0.1005
Epoch 403/500
1/1 [==============================] - 0s 56ms/step - loss: 0.1001
Epoch 404/500
1/1 [==============================] - 0s 55ms/step - loss: 0.0997
Epoch 405/500
1/1 [==============================] - 0s 58ms/step - loss: 0.0993
Epoch 406/500
1/1 [==============================] - 0s 53ms/step - loss: 0.0990
Epoch 407/500
1/1 [==============================] - 0s 59ms/step - loss: 0.0986
Epoch 408/500
1/1 [==============================] - 0s 63ms/step - loss: 0.0982
Epoch 409/500
1/1 [==============================] - 0s 67ms/step - loss: 0.0979
Epoch 410/500
1/1 [==============================] - 0s 55ms/step - loss: 0.0976
Epoch 411/500
1/1 [==============================] - 0s 54ms/step - loss: 0.0972
Epoch 412/500
1/1 [==============================] - 0s 55ms/step - loss: 0.0969
Epoch 413/500
1/1 [==============================] - 0s 55ms/step - loss: 0.0966
Epoch 414/500
1/1 [==============================] - 0s 59ms/step - loss: 0.0963
Epoch 415/500
1/1 [==============================] - 0s 60ms/step - loss: 0.0960
Epoch 416/500
1/1 [==============================] - 0s 62ms/step - loss: 0.0957
Epoch 417/500
1/1 [==============================] - 0s 69ms/step - loss: 0.0954
Epoch 418/500
1/1 [==============================] - 0s 60ms/step - loss: 0.0951
Epoch 419/500
1/1 [==============================] - 0s 50ms/step - loss: 0.0948
Epoch 420/500
1/1 [==============================] - 0s 56ms/step - loss: 0.0946
Epoch 421/500
1/1 [==============================] - 0s 57ms/step - loss: 0.0943
Epoch 422/500
1/1 [==============================] - 0s 55ms/step - loss: 0.0941
Epoch 423/500
1/1 [==============================] - 0s 62ms/step - loss: 0.0938
Epoch 424/500
1/1 [==============================] - 0s 60ms/step - loss: 0.0936
Epoch 425/500
1/1 [==============================] - 0s 100ms/step - loss: 0.0933
Epoch 426/500
1/1 [==============================] - 0s 68ms/step - loss: 0.0931
Epoch 427/500
1/1 [==============================] - 0s 60ms/step - loss: 0.0929
Epoch 428/500
1/1 [==============================] - 0s 50ms/step - loss: 0.0926
Epoch 429/500
1/1 [==============================] - 0s 55ms/step - loss: 0.0924
Epoch 430/500
1/1 [==============================] - 0s 57ms/step - loss: 0.0922
Epoch 431/500
1/1 [==============================] - 0s 75ms/step - loss: 0.0920
Epoch 432/500
1/1 [==============================] - 0s 57ms/step - loss: 0.0918
Epoch 433/500
1/1 [==============================] - 0s 77ms/step - loss: 0.0916
Epoch 434/500
1/1 [==============================] - 0s 50ms/step - loss: 0.0914
Epoch 435/500
1/1 [==============================] - 0s 56ms/step - loss: 0.0912
Epoch 436/500
1/1 [==============================] - 0s 60ms/step - loss: 0.0911
Epoch 437/500
1/1 [==============================] - 0s 55ms/step - loss: 0.0909
Epoch 438/500
1/1 [==============================] - 0s 57ms/step - loss: 0.0907
Epoch 439/500
1/1 [==============================] - 0s 59ms/step - loss: 0.0905
Epoch 440/500
1/1 [==============================] - 0s 60ms/step - loss: 0.0904
Epoch 441/500
1/1 [==============================] - 0s 68ms/step - loss: 0.0902
Epoch 442/500
1/1 [==============================] - 0s 73ms/step - loss: 0.0901
Epoch 443/500
1/1 [==============================] - 0s 50ms/step - loss: 0.0899
Epoch 444/500
1/1 [==============================] - 0s 58ms/step - loss: 0.0898
Epoch 445/500
1/1 [==============================] - 0s 56ms/step - loss: 0.0896
Epoch 446/500
1/1 [==============================] - 0s 52ms/step - loss: 0.0895
Epoch 447/500
1/1 [==============================] - 0s 60ms/step - loss: 0.0893
Epoch 448/500
1/1 [==============================] - 0s 64ms/step - loss: 0.0892
Epoch 449/500
1/1 [==============================] - 0s 70ms/step - loss: 0.0891
Epoch 450/500
1/1 [==============================] - 0s 57ms/step - loss: 0.0889
Epoch 451/500
1/1 [==============================] - 0s 53ms/step - loss: 0.0888
Epoch 452/500
1/1 [==============================] - 0s 51ms/step - loss: 0.0887
Epoch 453/500
1/1 [==============================] - 0s 55ms/step - loss: 0.0886
Epoch 454/500
1/1 [==============================] - 0s 58ms/step - loss: 0.0885
Epoch 455/500
1/1 [==============================] - 0s 55ms/step - loss: 0.0883
Epoch 456/500
1/1 [==============================] - 0s 71ms/step - loss: 0.0882
Epoch 457/500
1/1 [==============================] - 0s 50ms/step - loss: 0.0881
Epoch 458/500
1/1 [==============================] - 0s 56ms/step - loss: 0.0880
Epoch 459/500
1/1 [==============================] - 0s 55ms/step - loss: 0.0879
Epoch 460/500
1/1 [==============================] - 0s 57ms/step - loss: 0.0878
Epoch 461/500
1/1 [==============================] - 0s 56ms/step - loss: 0.0878
Epoch 462/500
1/1 [==============================] - 0s 55ms/step - loss: 0.0879
Epoch 463/500
1/1 [==============================] - 0s 60ms/step - loss: 0.0879
Epoch 464/500
1/1 [==============================] - 0s 68ms/step - loss: 0.0888
Epoch 465/500
1/1 [==============================] - 0s 62ms/step - loss: 0.0875
Epoch 466/500
1/1 [==============================] - 0s 55ms/step - loss: 0.0873
Epoch 467/500
1/1 [==============================] - 0s 49ms/step - loss: 0.0872
Epoch 468/500
1/1 [==============================] - 0s 56ms/step - loss: 0.0872
Epoch 469/500
1/1 [==============================] - 0s 55ms/step - loss: 0.0871
Epoch 470/500
1/1 [==============================] - 0s 55ms/step - loss: 0.0871
Epoch 471/500
1/1 [==============================] - 0s 59ms/step - loss: 0.0870
Epoch 472/500
1/1 [==============================] - 0s 68ms/step - loss: 0.0871
Epoch 473/500
1/1 [==============================] - 0s 57ms/step - loss: 0.0869
Epoch 474/500
1/1 [==============================] - 0s 61ms/step - loss: 0.0870
Epoch 475/500
1/1 [==============================] - 0s 47ms/step - loss: 0.0868
Epoch 476/500
1/1 [==============================] - 0s 55ms/step - loss: 0.0868
Epoch 477/500
1/1 [==============================] - 0s 62ms/step - loss: 0.0866
Epoch 478/500
1/1 [==============================] - 0s 58ms/step - loss: 0.0867
Epoch 479/500
1/1 [==============================] - 0s 60ms/step - loss: 0.0865
Epoch 480/500
1/1 [==============================] - 0s 65ms/step - loss: 0.0866
Epoch 481/500
1/1 [==============================] - 0s 58ms/step - loss: 0.0864
Epoch 482/500
1/1 [==============================] - 0s 57ms/step - loss: 0.0865
Epoch 483/500
1/1 [==============================] - 0s 53ms/step - loss: 0.0863
Epoch 484/500
1/1 [==============================] - 0s 56ms/step - loss: 0.0864
Epoch 485/500
1/1 [==============================] - 0s 56ms/step - loss: 0.0862
Epoch 486/500
1/1 [==============================] - 0s 55ms/step - loss: 0.0863
Epoch 487/500
1/1 [==============================] - 0s 52ms/step - loss: 0.0861
Epoch 488/500
1/1 [==============================] - 0s 68ms/step - loss: 0.0862
Epoch 489/500
1/1 [==============================] - 0s 62ms/step - loss: 0.0860
Epoch 490/500
1/1 [==============================] - 0s 57ms/step - loss: 0.0861
Epoch 491/500
1/1 [==============================] - 0s 51ms/step - loss: 0.0859
Epoch 492/500
1/1 [==============================] - 0s 54ms/step - loss: 0.0860
Epoch 493/500
1/1 [==============================] - 0s 51ms/step - loss: 0.0859
Epoch 494/500
1/1 [==============================] - 0s 54ms/step - loss: 0.0860
Epoch 495/500
1/1 [==============================] - 0s 50ms/step - loss: 0.0858
Epoch 496/500
1/1 [==============================] - 0s 69ms/step - loss: 0.0859
Epoch 497/500
1/1 [==============================] - 0s 63ms/step - loss: 0.0857
Epoch 498/500
1/1 [==============================] - 0s 56ms/step - loss: 0.0858
Epoch 499/500
1/1 [==============================] - 0s 54ms/step - loss: 0.0857
Epoch 500/500
1/1 [==============================] - 0s 57ms/step - loss: 0.0858
```

```python

```

### å››ï¼Œè¯„ä¼°æ¨¡å‹


è¯„ä¼°æ¨¡å‹ä¸€èˆ¬è¦è®¾ç½®éªŒè¯é›†æˆ–è€…æµ‹è¯•é›†ï¼Œç”±äºæ­¤ä¾‹æ•°æ®è¾ƒå°‘ï¼Œæˆ‘ä»¬ä»…ä»…å¯è§†åŒ–æŸå¤±å‡½æ•°åœ¨è®­ç»ƒé›†ä¸Šçš„è¿­ä»£æƒ…å†µã€‚

```python
%matplotlib inline
%config InlineBackend.figure_format = 'svg'

import matplotlib.pyplot as plt

def plot_metric(history, metric):
    train_metrics = history.history[metric]
    epochs = range(1, len(train_metrics) + 1)
    plt.plot(epochs, train_metrics, 'bo--')
    plt.title('Training '+ metric)
    plt.xlabel("Epochs")
    plt.ylabel(metric)
    plt.legend(["train_"+metric])
    plt.show()

```

```python
plot_metric(history,"loss")
```

![](./data/1-4-æŸå¤±å‡½æ•°æ›²çº¿.png)


### äº”ï¼Œä½¿ç”¨æ¨¡å‹


æ­¤å¤„æˆ‘ä»¬ä½¿ç”¨æ¨¡å‹é¢„æµ‹ç–«æƒ…ç»“æŸæ—¶é—´ï¼Œå³ æ–°å¢ç¡®è¯Šç—…ä¾‹ä¸º0 çš„æ—¶é—´ã€‚

```python
#ä½¿ç”¨dfresultè®°å½•ç°æœ‰æ•°æ®ä»¥åŠæ­¤åé¢„æµ‹çš„ç–«æƒ…æ•°æ®
dfresult = dfdiff[["confirmed_num","cured_num","dead_num"]].copy()
dfresult.tail()
```

![](./data/1-4-æ—¥æœŸ3æœˆ10.png)

```python
#é¢„æµ‹æ­¤å100å¤©çš„æ–°å¢èµ°åŠ¿,å°†å…¶ç»“æœæ·»åŠ åˆ°dfresultä¸­
for i in range(100):
    arr_predict = model.predict(tf.constant(tf.expand_dims(dfresult.values[-38:,:],axis = 0)))

    dfpredict = pd.DataFrame(tf.cast(tf.floor(arr_predict),tf.float32).numpy(),
                columns = dfresult.columns)
    dfresult = dfresult.append(dfpredict,ignore_index=True)
```

```python
dfresult.query("confirmed_num==0").head()

# ç¬¬55å¤©å¼€å§‹æ–°å¢ç¡®è¯Šé™ä¸º0ï¼Œç¬¬45å¤©å¯¹åº”3æœˆ10æ—¥ï¼Œä¹Ÿå°±æ˜¯10å¤©åï¼Œå³é¢„è®¡3æœˆ20æ—¥æ–°å¢ç¡®è¯Šé™ä¸º0
# æ³¨ï¼šè¯¥é¢„æµ‹åä¹è§‚
```

![](./data/1-4-é¢„æµ‹ç¡®è¯Š.png)

```python

```

```python
dfresult.query("cured_num==0").head()

# ç¬¬164å¤©å¼€å§‹æ–°å¢æ²»æ„ˆé™ä¸º0ï¼Œç¬¬45å¤©å¯¹åº”3æœˆ10æ—¥ï¼Œä¹Ÿå°±æ˜¯å¤§æ¦‚4ä¸ªæœˆåï¼Œå³7æœˆ10æ—¥å·¦å³å…¨éƒ¨æ²»æ„ˆã€‚
# æ³¨: è¯¥é¢„æµ‹åæ‚²è§‚ï¼Œå¹¶ä¸”å­˜åœ¨é—®é¢˜ï¼Œå¦‚æœå°†æ¯å¤©æ–°å¢æ²»æ„ˆäººæ•°åŠ èµ·æ¥ï¼Œå°†è¶…è¿‡ç´¯è®¡ç¡®è¯Šäººæ•°ã€‚
```

![](./data/1-4-é¢„æµ‹æ²»æ„ˆ.png)

```python

```

```python
dfresult.query("dead_num==0").head()

# ç¬¬60å¤©å¼€å§‹ï¼Œæ–°å¢æ­»äº¡é™ä¸º0ï¼Œç¬¬45å¤©å¯¹åº”3æœˆ10æ—¥ï¼Œä¹Ÿå°±æ˜¯å¤§æ¦‚15å¤©åï¼Œå³20200325
# è¯¥é¢„æµ‹è¾ƒä¸ºåˆç†
```

![](./data/1-4-é¢„æµ‹æ­»äº¡.png)

```python

```

### å…­ï¼Œä¿å­˜æ¨¡å‹


æ¨èä½¿ç”¨TensorFlowåŸç”Ÿæ–¹å¼ä¿å­˜æ¨¡å‹ã€‚

```python
model.save('./data/tf_model_savedmodel', save_format="tf")
print('export saved model.')
```

```python
model_loaded = tf.keras.models.load_model('./data/tf_model_savedmodel',compile=False)
optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
model_loaded.compile(optimizer=optimizer,loss=MSPE(name = "MSPE"))
model_loaded.predict(ds_train)
```

```python

```

å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·"Pythonä¸ç®—æ³•ä¹‹ç¾"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚

![image.png](./data/Pythonä¸ç®—æ³•ä¹‹ç¾logo.jpg)



# äºŒã€TensorFlowçš„æ ¸å¿ƒæ¦‚å¿µ

TensorFlowâ„¢ æ˜¯ä¸€ä¸ªé‡‡ç”¨ **æ•°æ®æµå›¾**ï¼ˆdata flow graphsï¼‰ï¼Œç”¨äºæ•°å€¼è®¡ç®—çš„å¼€æºè½¯ä»¶åº“ã€‚èŠ‚ç‚¹ï¼ˆNodesï¼‰åœ¨å›¾ä¸­è¡¨ç¤ºæ•°å­¦æ“ä½œï¼Œå›¾ä¸­çš„çº¿ï¼ˆedgesï¼‰åˆ™è¡¨ç¤ºåœ¨èŠ‚ç‚¹é—´ç›¸äº’è”ç³»çš„å¤šç»´æ•°æ®æ•°ç»„ï¼Œå³å¼ é‡ï¼ˆtensorï¼‰ã€‚å®ƒçµæ´»çš„æ¶æ„è®©ä½ å¯ä»¥**åœ¨å¤šç§å¹³å°ä¸Šå±•å¼€è®¡ç®—**ï¼Œä¾‹å¦‚å°å¼è®¡ç®—æœºä¸­çš„ä¸€ä¸ªæˆ–å¤šä¸ªCPUï¼ˆæˆ–GPUï¼‰ï¼ŒæœåŠ¡å™¨ï¼Œç§»åŠ¨è®¾å¤‡ç­‰ç­‰ã€‚TensorFlow æœ€åˆç”±Googleå¤§è„‘å°ç»„ï¼ˆéš¶å±äºGoogleæœºå™¨æ™ºèƒ½ç ”ç©¶æœºæ„ï¼‰çš„ç ”ç©¶å‘˜å’Œå·¥ç¨‹å¸ˆä»¬å¼€å‘å‡ºæ¥ï¼Œ**ç”¨äºæœºå™¨å­¦ä¹ å’Œæ·±åº¦ç¥ç»ç½‘ç»œ**æ–¹é¢çš„ç ”ç©¶ï¼Œä½†è¿™ä¸ªç³»ç»Ÿçš„é€šç”¨æ€§ä½¿å…¶ä¹Ÿå¯**å¹¿æ³›ç”¨äºå…¶ä»–è®¡ç®—é¢†åŸŸ**ã€‚ 


TensorFlowçš„ä¸»è¦ä¼˜ç‚¹ï¼š

* çµæ´»æ€§ï¼šæ”¯æŒåº•å±‚æ•°å€¼è®¡ç®—ï¼ŒC++è‡ªå®šä¹‰æ“ä½œç¬¦

* å¯ç§»æ¤æ€§ï¼šä»æœåŠ¡å™¨åˆ°PCåˆ°æ‰‹æœºï¼Œä»CPUåˆ°GPUåˆ°TPU

* åˆ†å¸ƒå¼è®¡ç®—ï¼šåˆ†å¸ƒå¼å¹¶è¡Œè®¡ç®—ï¼Œå¯æŒ‡å®šæ“ä½œç¬¦å¯¹åº”è®¡ç®—è®¾å¤‡


ä¿—è¯è¯´ï¼Œä¸‡ä¸ˆé«˜æ¥¼å¹³åœ°èµ·ï¼ŒTensorFlowè¿™åº§å¤§å¦ä¹Ÿæœ‰å®ƒçš„åœ°åŸºã€‚

Tensorflowåº•å±‚æœ€æ ¸å¿ƒçš„æ¦‚å¿µæ˜¯å¼ é‡ï¼Œè®¡ç®—å›¾ä»¥åŠè‡ªåŠ¨å¾®åˆ†ã€‚


å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·"Pythonä¸ç®—æ³•ä¹‹ç¾"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚

![](./data/Pythonä¸ç®—æ³•ä¹‹ç¾logo.jpg)



# 2-1,å¼ é‡æ•°æ®ç»“æ„

ç¨‹åº = æ•°æ®ç»“æ„+ç®—æ³•ã€‚

TensorFlowç¨‹åº = å¼ é‡æ•°æ®ç»“æ„ + è®¡ç®—å›¾ç®—æ³•è¯­è¨€

å¼ é‡å’Œè®¡ç®—å›¾æ˜¯ TensorFlowçš„æ ¸å¿ƒæ¦‚å¿µã€‚

Tensorflowçš„åŸºæœ¬æ•°æ®ç»“æ„æ˜¯å¼ é‡Tensorã€‚å¼ é‡å³å¤šç»´æ•°ç»„ã€‚Tensorflowçš„å¼ é‡å’Œnumpyä¸­çš„arrayå¾ˆç±»ä¼¼ã€‚

ä»è¡Œä¸ºç‰¹æ€§æ¥çœ‹ï¼Œæœ‰ä¸¤ç§ç±»å‹çš„å¼ é‡ï¼Œå¸¸é‡constantå’Œå˜é‡Variable.

å¸¸é‡çš„å€¼åœ¨è®¡ç®—å›¾ä¸­ä¸å¯ä»¥è¢«é‡æ–°èµ‹å€¼ï¼Œå˜é‡å¯ä»¥åœ¨è®¡ç®—å›¾ä¸­ç”¨assignç­‰ç®—å­é‡æ–°èµ‹å€¼ã€‚


### ä¸€ï¼Œå¸¸é‡å¼ é‡


å¼ é‡çš„æ•°æ®ç±»å‹å’Œnumpy.arrayåŸºæœ¬ä¸€ä¸€å¯¹åº”ã€‚

```python
import numpy as np
import tensorflow as tf

i = tf.constant(1) # tf.int32 ç±»å‹å¸¸é‡
l = tf.constant(1,dtype = tf.int64) # tf.int64 ç±»å‹å¸¸é‡
f = tf.constant(1.23) #tf.float32 ç±»å‹å¸¸é‡
d = tf.constant(3.14,dtype = tf.double) # tf.double ç±»å‹å¸¸é‡
s = tf.constant("hello world") # tf.stringç±»å‹å¸¸é‡
b = tf.constant(True) #tf.boolç±»å‹å¸¸é‡


print(tf.int64 == np.int64) 
print(tf.bool == np.bool)
print(tf.double == np.float64)
print(tf.string == np.unicode) # tf.stringç±»å‹å’Œnp.unicodeç±»å‹ä¸ç­‰ä»·

```

```
True
True
True
False
```


ä¸åŒç±»å‹çš„æ•°æ®å¯ä»¥ç”¨ä¸åŒç»´åº¦(rank)çš„å¼ é‡æ¥è¡¨ç¤ºã€‚

æ ‡é‡ä¸º0ç»´å¼ é‡ï¼Œå‘é‡ä¸º1ç»´å¼ é‡ï¼ŒçŸ©é˜µä¸º2ç»´å¼ é‡ã€‚

å½©è‰²å›¾åƒæœ‰rgbä¸‰ä¸ªé€šé“ï¼Œå¯ä»¥è¡¨ç¤ºä¸º3ç»´å¼ é‡ã€‚

è§†é¢‘è¿˜æœ‰æ—¶é—´ç»´ï¼Œå¯ä»¥è¡¨ç¤ºä¸º4ç»´å¼ é‡ã€‚

å¯ä»¥ç®€å•åœ°æ€»ç»“ä¸ºï¼šæœ‰å‡ å±‚ä¸­æ‹¬å·ï¼Œå°±æ˜¯å¤šå°‘ç»´çš„å¼ é‡ã€‚

```python
scalar = tf.constant(True)  #æ ‡é‡ï¼Œ0ç»´å¼ é‡

print(tf.rank(scalar))
print(scalar.numpy().ndim)  # tf.rankçš„ä½œç”¨å’Œnumpyçš„ndimæ–¹æ³•ç›¸åŒ
```

```
tf.Tensor(0, shape=(), dtype=int32)
0
```

```python
vector = tf.constant([1.0,2.0,3.0,4.0]) #å‘é‡ï¼Œ1ç»´å¼ é‡

print(tf.rank(vector))
print(np.ndim(vector.numpy()))
```

```
tf.Tensor(1, shape=(), dtype=int32)
1
```

```python
matrix = tf.constant([[1.0,2.0],[3.0,4.0]]) #çŸ©é˜µ, 2ç»´å¼ é‡

print(tf.rank(matrix).numpy())
print(np.ndim(matrix))
```

```
2
2
```

```python
tensor3 = tf.constant([[[1.0,2.0],[3.0,4.0]],[[5.0,6.0],[7.0,8.0]]])  # 3ç»´å¼ é‡
print(tensor3)
print(tf.rank(tensor3))
```

```
tf.Tensor(
[[[1. 2.]
  [3. 4.]]

 [[5. 6.]
  [7. 8.]]], shape=(2, 2, 2), dtype=float32)
tf.Tensor(3, shape=(), dtype=int32)
```

```python
tensor4 = tf.constant([[[[1.0,1.0],[2.0,2.0]],[[3.0,3.0],[4.0,4.0]]],
                        [[[5.0,5.0],[6.0,6.0]],[[7.0,7.0],[8.0,8.0]]]])  # 4ç»´å¼ é‡
print(tensor4)
print(tf.rank(tensor4))
```

```
tf.Tensor(
[[[[1. 1.]
   [2. 2.]]

  [[3. 3.]
   [4. 4.]]]


 [[[5. 5.]
   [6. 6.]]

  [[7. 7.]
   [8. 8.]]]], shape=(2, 2, 2, 2), dtype=float32)
tf.Tensor(4, shape=(), dtype=int32)
```


å¯ä»¥ç”¨tf.castæ”¹å˜å¼ é‡çš„æ•°æ®ç±»å‹ã€‚

å¯ä»¥ç”¨numpyæ–¹æ³•å°†tensorflowä¸­çš„å¼ é‡è½¬åŒ–æˆnumpyä¸­çš„å¼ é‡ã€‚

å¯ä»¥ç”¨shapeæ–¹æ³•æŸ¥çœ‹å¼ é‡çš„å°ºå¯¸ã€‚

```python
h = tf.constant([123,456],dtype = tf.int32)
f = tf.cast(h,tf.float32)
print(h.dtype, f.dtype)
```

```
<dtype: 'int32'> <dtype: 'float32'>
```

```python
y = tf.constant([[1.0,2.0],[3.0,4.0]])
print(y.numpy()) #è½¬æ¢æˆnp.array
print(y.shape)
```

```
[[1. 2.]
 [3. 4.]]
(2, 2)
```

```python
u = tf.constant(u"ä½ å¥½ ä¸–ç•Œ")
print(u.numpy())  
print(u.numpy().decode("utf-8"))
```

```
b'\xe4\xbd\xa0\xe5\xa5\xbd \xe4\xb8\x96\xe7\x95\x8c'
ä½ å¥½ ä¸–ç•Œ
```

```python

```
### äºŒï¼Œå˜é‡å¼ é‡


æ¨¡å‹ä¸­éœ€è¦è¢«è®­ç»ƒçš„å‚æ•°ä¸€èˆ¬è¢«è®¾ç½®æˆå˜é‡ã€‚

```python
# å¸¸é‡å€¼ä¸å¯ä»¥æ”¹å˜ï¼Œå¸¸é‡çš„é‡æ–°èµ‹å€¼ç›¸å½“äºåˆ›é€ æ–°çš„å†…å­˜ç©ºé—´
c = tf.constant([1.0,2.0])
print(c)
print(id(c))
c = c + tf.constant([1.0,1.0])
print(c)
print(id(c))
```

```
tf.Tensor([1. 2.], shape=(2,), dtype=float32)
5276289568
tf.Tensor([2. 3.], shape=(2,), dtype=float32)
5276290240
```

```python
# å˜é‡çš„å€¼å¯ä»¥æ”¹å˜ï¼Œå¯ä»¥é€šè¿‡assign, assign_addç­‰æ–¹æ³•ç»™å˜é‡é‡æ–°èµ‹å€¼
v = tf.Variable([1.0,2.0],name = "v")
print(v)
print(id(v))
v.assign_add([1.0,1.0])
print(v)
print(id(v))
```
```
<tf.Variable 'v:0' shape=(2,) dtype=float32, numpy=array([1., 2.], dtype=float32)>
5276259888
<tf.Variable 'v:0' shape=(2,) dtype=float32, numpy=array([2., 3.], dtype=float32)>
5276259888

```

```python

```

å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·"Pythonä¸ç®—æ³•ä¹‹ç¾"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚

![image.png](./data/Pythonä¸ç®—æ³•ä¹‹ç¾logo.jpg)



# 2-2,ä¸‰ç§è®¡ç®—å›¾


æœ‰ä¸‰ç§è®¡ç®—å›¾çš„æ„å»ºæ–¹å¼ï¼šé™æ€è®¡ç®—å›¾ï¼ŒåŠ¨æ€è®¡ç®—å›¾ï¼Œä»¥åŠAutograph.

åœ¨TensorFlow1.0æ—¶ä»£ï¼Œé‡‡ç”¨çš„æ˜¯é™æ€è®¡ç®—å›¾ï¼Œéœ€è¦å…ˆä½¿ç”¨TensorFlowçš„å„ç§ç®—å­åˆ›å»ºè®¡ç®—å›¾ï¼Œç„¶åå†å¼€å¯ä¸€ä¸ªä¼šè¯Sessionï¼Œæ˜¾å¼æ‰§è¡Œè®¡ç®—å›¾ã€‚

è€Œåœ¨TensorFlow2.0æ—¶ä»£ï¼Œé‡‡ç”¨çš„æ˜¯åŠ¨æ€è®¡ç®—å›¾ï¼Œå³æ¯ä½¿ç”¨ä¸€ä¸ªç®—å­åï¼Œè¯¥ç®—å­ä¼šè¢«åŠ¨æ€åŠ å…¥åˆ°éšå«çš„é»˜è®¤è®¡ç®—å›¾ä¸­ç«‹å³æ‰§è¡Œå¾—åˆ°ç»“æœï¼Œè€Œæ— éœ€å¼€å¯Sessionã€‚

ä½¿ç”¨åŠ¨æ€è®¡ç®—å›¾å³Eager Excutionçš„å¥½å¤„æ˜¯æ–¹ä¾¿è°ƒè¯•ç¨‹åºï¼Œå®ƒä¼šè®©TensorFlowä»£ç çš„è¡¨ç°å’ŒPythonåŸç”Ÿä»£ç çš„è¡¨ç°ä¸€æ ·ï¼Œå†™èµ·æ¥å°±åƒå†™numpyä¸€æ ·ï¼Œå„ç§æ—¥å¿—æ‰“å°ï¼Œæ§åˆ¶æµå…¨éƒ¨éƒ½æ˜¯å¯ä»¥ä½¿ç”¨çš„ã€‚

ä½¿ç”¨åŠ¨æ€è®¡ç®—å›¾çš„ç¼ºç‚¹æ˜¯è¿è¡Œæ•ˆç‡ç›¸å¯¹ä¼šä½ä¸€äº›ã€‚å› ä¸ºä½¿ç”¨åŠ¨æ€å›¾ä¼šæœ‰è®¸å¤šæ¬¡Pythonè¿›ç¨‹å’ŒTensorFlowçš„C++è¿›ç¨‹ä¹‹é—´çš„é€šä¿¡ã€‚è€Œé™æ€è®¡ç®—å›¾æ„å»ºå®Œæˆä¹‹åå‡ ä¹å…¨éƒ¨åœ¨TensorFlowå†…æ ¸ä¸Šä½¿ç”¨C++ä»£ç æ‰§è¡Œï¼Œæ•ˆç‡æ›´é«˜ã€‚æ­¤å¤–é™æ€å›¾ä¼šå¯¹è®¡ç®—æ­¥éª¤è¿›è¡Œä¸€å®šçš„ä¼˜åŒ–ï¼Œå‰ªå»å’Œç»“æœæ— å…³çš„è®¡ç®—æ­¥éª¤ã€‚

å¦‚æœéœ€è¦åœ¨TensorFlow2.0ä¸­ä½¿ç”¨é™æ€å›¾ï¼Œå¯ä»¥ä½¿ç”¨@tf.functionè£…é¥°å™¨å°†æ™®é€šPythonå‡½æ•°è½¬æ¢æˆå¯¹åº”çš„TensorFlowè®¡ç®—å›¾æ„å»ºä»£ç ã€‚è¿è¡Œè¯¥å‡½æ•°å°±ç›¸å½“äºåœ¨TensorFlow1.0ä¸­ç”¨Sessionæ‰§è¡Œä»£ç ã€‚ä½¿ç”¨tf.functionæ„å»ºé™æ€å›¾çš„æ–¹å¼å«åš Autograph.


### ä¸€ï¼Œè®¡ç®—å›¾ç®€ä»‹


è®¡ç®—å›¾ç”±èŠ‚ç‚¹(nodes)å’Œçº¿(edges)ç»„æˆã€‚

èŠ‚ç‚¹è¡¨ç¤ºæ“ä½œç¬¦Operatorï¼Œæˆ–è€…ç§°ä¹‹ä¸ºç®—å­ï¼Œçº¿è¡¨ç¤ºè®¡ç®—é—´çš„ä¾èµ–ã€‚

å®çº¿è¡¨ç¤ºæœ‰æ•°æ®ä¼ é€’ä¾èµ–ï¼Œä¼ é€’çš„æ•°æ®å³å¼ é‡ã€‚

è™šçº¿é€šå¸¸å¯ä»¥è¡¨ç¤ºæ§åˆ¶ä¾èµ–ï¼Œå³æ‰§è¡Œå…ˆåé¡ºåºã€‚

![](./data/strjoin_graph.png)


### äºŒï¼Œé™æ€è®¡ç®—å›¾


åœ¨TensorFlow1.0ä¸­ï¼Œä½¿ç”¨é™æ€è®¡ç®—å›¾åˆ†ä¸¤æ­¥ï¼Œç¬¬ä¸€æ­¥å®šä¹‰è®¡ç®—å›¾ï¼Œç¬¬äºŒæ­¥åœ¨ä¼šè¯ä¸­æ‰§è¡Œè®¡ç®—å›¾ã€‚


**TensorFlow 1.0é™æ€è®¡ç®—å›¾èŒƒä¾‹**

```python
import tensorflow as tf

#å®šä¹‰è®¡ç®—å›¾
g = tf.Graph()
with g.as_default():
    #placeholderä¸ºå ä½ç¬¦ï¼Œæ‰§è¡Œä¼šè¯æ—¶å€™æŒ‡å®šå¡«å……å¯¹è±¡
    x = tf.placeholder(name='x', shape=[], dtype=tf.string)  
    y = tf.placeholder(name='y', shape=[], dtype=tf.string)
    z = tf.string_join([x,y],name = 'join',separator=' ')

#æ‰§è¡Œè®¡ç®—å›¾
with tf.Session(graph = g) as sess:
    print(sess.run(fetches = z,feed_dict = {x:"hello",y:"world"}))
   
```


**TensorFlow2.0 æ€€æ—§ç‰ˆé™æ€è®¡ç®—å›¾**

TensorFlow2.0ä¸ºäº†ç¡®ä¿å¯¹è€ç‰ˆæœ¬tensorflowé¡¹ç›®çš„å…¼å®¹æ€§ï¼Œåœ¨tf.compat.v1å­æ¨¡å—ä¸­ä¿ç•™äº†å¯¹TensorFlow1.0é‚£ç§é™æ€è®¡ç®—å›¾æ„å»ºé£æ ¼çš„æ”¯æŒã€‚

å¯ç§°ä¹‹ä¸ºæ€€æ—§ç‰ˆé™æ€è®¡ç®—å›¾ï¼Œå·²ç»ä¸æ¨èä½¿ç”¨äº†ã€‚

```python
import tensorflow as tf

g = tf.compat.v1.Graph()
with g.as_default():
    x = tf.compat.v1.placeholder(name='x', shape=[], dtype=tf.string)
    y = tf.compat.v1.placeholder(name='y', shape=[], dtype=tf.string)
    z = tf.strings.join([x,y],name = "join",separator = " ")

with tf.compat.v1.Session(graph = g) as sess:
    # fetchesçš„ç»“æœéå¸¸åƒä¸€ä¸ªå‡½æ•°çš„è¿”å›å€¼ï¼Œè€Œfeed_dictä¸­çš„å ä½ç¬¦ç›¸å½“äºå‡½æ•°çš„å‚æ•°åºåˆ—ã€‚
    result = sess.run(fetches = z,feed_dict = {x:"hello",y:"world"})
    print(result)

```

```
b'hello world'
```


### ä¸‰ï¼ŒåŠ¨æ€è®¡ç®—å›¾


åœ¨TensorFlow2.0ä¸­ï¼Œä½¿ç”¨çš„æ˜¯åŠ¨æ€è®¡ç®—å›¾å’ŒAutograph.

åœ¨TensorFlow1.0ä¸­ï¼Œä½¿ç”¨é™æ€è®¡ç®—å›¾åˆ†ä¸¤æ­¥ï¼Œç¬¬ä¸€æ­¥å®šä¹‰è®¡ç®—å›¾ï¼Œç¬¬äºŒæ­¥åœ¨ä¼šè¯ä¸­æ‰§è¡Œè®¡ç®—å›¾ã€‚

åŠ¨æ€è®¡ç®—å›¾å·²ç»ä¸åŒºåˆ†è®¡ç®—å›¾çš„å®šä¹‰å’Œæ‰§è¡Œäº†ï¼Œè€Œæ˜¯å®šä¹‰åç«‹å³æ‰§è¡Œã€‚å› æ­¤ç§°ä¹‹ä¸º Eager Excution.

Eagerè¿™ä¸ªè‹±æ–‡å•è¯çš„åŸæ„æ˜¯"è¿«ä¸åŠå¾…çš„"ï¼Œä¹Ÿå°±æ˜¯ç«‹å³æ‰§è¡Œçš„æ„æ€ã€‚


```python
# åŠ¨æ€è®¡ç®—å›¾åœ¨æ¯ä¸ªç®—å­å¤„éƒ½è¿›è¡Œæ„å»ºï¼Œæ„å»ºåç«‹å³æ‰§è¡Œ

x = tf.constant("hello")
y = tf.constant("world")
z = tf.strings.join([x,y],separator=" ")

tf.print(z)
```

```
hello world
```

```python
# å¯ä»¥å°†åŠ¨æ€è®¡ç®—å›¾ä»£ç çš„è¾“å…¥å’Œè¾“å‡ºå…³ç³»å°è£…æˆå‡½æ•°

def strjoin(x,y):
    z =  tf.strings.join([x,y],separator = " ")
    tf.print(z)
    return z

result = strjoin(tf.constant("hello"),tf.constant("world"))
print(result)
```

```
hello world
tf.Tensor(b'hello world', shape=(), dtype=string)
```


### å››ï¼ŒTensorFlow2.0çš„Autograph


åŠ¨æ€è®¡ç®—å›¾è¿è¡Œæ•ˆç‡ç›¸å¯¹è¾ƒä½ã€‚

å¯ä»¥ç”¨@tf.functionè£…é¥°å™¨å°†æ™®é€šPythonå‡½æ•°è½¬æ¢æˆå’ŒTensorFlow1.0å¯¹åº”çš„é™æ€è®¡ç®—å›¾æ„å»ºä»£ç ã€‚

åœ¨TensorFlow1.0ä¸­ï¼Œä½¿ç”¨è®¡ç®—å›¾åˆ†ä¸¤æ­¥ï¼Œç¬¬ä¸€æ­¥å®šä¹‰è®¡ç®—å›¾ï¼Œç¬¬äºŒæ­¥åœ¨ä¼šè¯ä¸­æ‰§è¡Œè®¡ç®—å›¾ã€‚

åœ¨TensorFlow2.0ä¸­ï¼Œå¦‚æœé‡‡ç”¨Autographçš„æ–¹å¼ä½¿ç”¨è®¡ç®—å›¾ï¼Œç¬¬ä¸€æ­¥å®šä¹‰è®¡ç®—å›¾å˜æˆäº†å®šä¹‰å‡½æ•°ï¼Œç¬¬äºŒæ­¥æ‰§è¡Œè®¡ç®—å›¾å˜æˆäº†è°ƒç”¨å‡½æ•°ã€‚

ä¸éœ€è¦ä½¿ç”¨ä¼šè¯äº†ï¼Œä¸€äº›éƒ½åƒåŸå§‹çš„Pythonè¯­æ³•ä¸€æ ·è‡ªç„¶ã€‚

å®è·µä¸­ï¼Œæˆ‘ä»¬ä¸€èˆ¬ä¼šå…ˆç”¨åŠ¨æ€è®¡ç®—å›¾è°ƒè¯•ä»£ç ï¼Œç„¶ååœ¨éœ€è¦æé«˜æ€§èƒ½çš„çš„åœ°æ–¹åˆ©ç”¨@tf.functionåˆ‡æ¢æˆAutographè·å¾—æ›´é«˜çš„æ•ˆç‡ã€‚

å½“ç„¶ï¼Œ@tf.functionçš„ä½¿ç”¨éœ€è¦éµå¾ªä¸€å®šçš„è§„èŒƒï¼Œæˆ‘ä»¬åé¢ç« èŠ‚å°†é‡ç‚¹ä»‹ç»ã€‚


```python
import tensorflow as tf

# ä½¿ç”¨autographæ„å»ºé™æ€å›¾

@tf.function
def strjoin(x,y):
    z =  tf.strings.join([x,y],separator = " ")
    tf.print(z)
    return z

result = strjoin(tf.constant("hello"),tf.constant("world"))

print(result)
```

```
hello world
tf.Tensor(b'hello world', shape=(), dtype=string)
```

```python
import datetime

# åˆ›å»ºæ—¥å¿—
stamp = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
logdir = './data/autograph/%s' % stamp
writer = tf.summary.create_file_writer(logdir)

#å¼€å¯autographè·Ÿè¸ª
tf.summary.trace_on(graph=True, profiler=True) 

#æ‰§è¡Œautograph
result = strjoin("hello","world")

#å°†è®¡ç®—å›¾ä¿¡æ¯å†™å…¥æ—¥å¿—
with writer.as_default():
    tf.summary.trace_export(
        name="autograph",
        step=0,
        profiler_outdir=logdir)
```

```python
#å¯åŠ¨ tensorboardåœ¨jupyterä¸­çš„é­”æ³•å‘½ä»¤
%load_ext tensorboard
```

```python
#å¯åŠ¨tensorboard
%tensorboard --logdir ./data/autograph/
```

![](./data/2-2-tensorboardè®¡ç®—å›¾.jpg)

```python

```

å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·"Pythonä¸ç®—æ³•ä¹‹ç¾"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚

![image.png](./data/Pythonä¸ç®—æ³•ä¹‹ç¾logo.jpg)



# 2-3,è‡ªåŠ¨å¾®åˆ†æœºåˆ¶


ç¥ç»ç½‘ç»œé€šå¸¸ä¾èµ–åå‘ä¼ æ’­æ±‚æ¢¯åº¦æ¥æ›´æ–°ç½‘ç»œå‚æ•°ï¼Œæ±‚æ¢¯åº¦è¿‡ç¨‹é€šå¸¸æ˜¯ä¸€ä»¶éå¸¸å¤æ‚è€Œå®¹æ˜“å‡ºé”™çš„äº‹æƒ…ã€‚

è€Œæ·±åº¦å­¦ä¹ æ¡†æ¶å¯ä»¥å¸®åŠ©æˆ‘ä»¬è‡ªåŠ¨åœ°å®Œæˆè¿™ç§æ±‚æ¢¯åº¦è¿ç®—ã€‚

Tensorflowä¸€èˆ¬ä½¿ç”¨æ¢¯åº¦ç£å¸¦tf.GradientTapeæ¥è®°å½•æ­£å‘è¿ç®—è¿‡ç¨‹ï¼Œç„¶ååæ’­ç£å¸¦è‡ªåŠ¨å¾—åˆ°æ¢¯åº¦å€¼ã€‚

è¿™ç§åˆ©ç”¨tf.GradientTapeæ±‚å¾®åˆ†çš„æ–¹æ³•å«åšTensorflowçš„è‡ªåŠ¨å¾®åˆ†æœºåˆ¶ã€‚


### ä¸€ï¼Œåˆ©ç”¨æ¢¯åº¦ç£å¸¦æ±‚å¯¼æ•°

```python
import tensorflow as tf
import numpy as np 

# f(x) = a*x**2 + b*x + cçš„å¯¼æ•°

x = tf.Variable(0.0,name = "x",dtype = tf.float32)
a = tf.constant(1.0)
b = tf.constant(-2.0)
c = tf.constant(1.0)

with tf.GradientTape() as tape:
    y = a*tf.pow(x,2) + b*x + c
    
dy_dx = tape.gradient(y,x)
print(dy_dx)
```

```
tf.Tensor(-2.0, shape=(), dtype=float32)
```

```python

```

```python
# å¯¹å¸¸é‡å¼ é‡ä¹Ÿå¯ä»¥æ±‚å¯¼ï¼Œéœ€è¦å¢åŠ watch

with tf.GradientTape() as tape:
    tape.watch([a,b,c])
    y = a*tf.pow(x,2) + b*x + c
    
dy_dx,dy_da,dy_db,dy_dc = tape.gradient(y,[x,a,b,c])
print(dy_da)
print(dy_dc)

```

```
tf.Tensor(0.0, shape=(), dtype=float32)
tf.Tensor(1.0, shape=(), dtype=float32)
```

```python

```

```python
# å¯ä»¥æ±‚äºŒé˜¶å¯¼æ•°
with tf.GradientTape() as tape2:
    with tf.GradientTape() as tape1:   
        y = a*tf.pow(x,2) + b*x + c
    dy_dx = tape1.gradient(y,x)   
dy2_dx2 = tape2.gradient(dy_dx,x)

print(dy2_dx2)
```

```
tf.Tensor(2.0, shape=(), dtype=float32)
```

```python

```

```python
# å¯ä»¥åœ¨autographä¸­ä½¿ç”¨

@tf.function
def f(x):   
    a = tf.constant(1.0)
    b = tf.constant(-2.0)
    c = tf.constant(1.0)
    
    # è‡ªå˜é‡è½¬æ¢æˆtf.float32
    x = tf.cast(x,tf.float32)
    with tf.GradientTape() as tape:
        tape.watch(x)
        y = a*tf.pow(x,2)+b*x+c
    dy_dx = tape.gradient(y,x) 
    
    return((dy_dx,y))

tf.print(f(tf.constant(0.0)))
tf.print(f(tf.constant(1.0)))
```

```
(-2, 1)
(0, 0)
```

```python

```

### äºŒï¼Œåˆ©ç”¨æ¢¯åº¦ç£å¸¦å’Œä¼˜åŒ–å™¨æ±‚æœ€å°å€¼

```python
# æ±‚f(x) = a*x**2 + b*x + cçš„æœ€å°å€¼
# ä½¿ç”¨optimizer.apply_gradients

x = tf.Variable(0.0,name = "x",dtype = tf.float32)
a = tf.constant(1.0)
b = tf.constant(-2.0)
c = tf.constant(1.0)

optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)
for _ in range(1000):
    with tf.GradientTape() as tape:
        y = a*tf.pow(x,2) + b*x + c
    dy_dx = tape.gradient(y,x)
    optimizer.apply_gradients(grads_and_vars=[(dy_dx,x)])
    
tf.print("y =",y,"; x =",x)
```

```
y = 0 ; x = 0.999998569
```

```python

```

```python
# æ±‚f(x) = a*x**2 + b*x + cçš„æœ€å°å€¼
# ä½¿ç”¨optimizer.minimize
# optimizer.minimizeç›¸å½“äºå…ˆç”¨tapeæ±‚gradient,å†apply_gradient

x = tf.Variable(0.0,name = "x",dtype = tf.float32)

#æ³¨æ„f()æ— å‚æ•°
def f():   
    a = tf.constant(1.0)
    b = tf.constant(-2.0)
    c = tf.constant(1.0)
    y = a*tf.pow(x,2)+b*x+c
    return(y)

optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)   
for _ in range(1000):
    optimizer.minimize(f,[x])   
    
tf.print("y =",f(),"; x =",x)
```

```
y = 0 ; x = 0.999998569
```

```python

```

```python
# åœ¨autographä¸­å®Œæˆæœ€å°å€¼æ±‚è§£
# ä½¿ç”¨optimizer.apply_gradients

x = tf.Variable(0.0,name = "x",dtype = tf.float32)
optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)

@tf.function
def minimizef():
    a = tf.constant(1.0)
    b = tf.constant(-2.0)
    c = tf.constant(1.0)
    
    for _ in tf.range(1000): #æ³¨æ„autographæ—¶ä½¿ç”¨tf.range(1000)è€Œä¸æ˜¯range(1000)
        with tf.GradientTape() as tape:
            y = a*tf.pow(x,2) + b*x + c
        dy_dx = tape.gradient(y,x)
        optimizer.apply_gradients(grads_and_vars=[(dy_dx,x)])
        
    y = a*tf.pow(x,2) + b*x + c
    return y

tf.print(minimizef())
tf.print(x)
```

```
0
0.999998569
```

```python

```

```python
# åœ¨autographä¸­å®Œæˆæœ€å°å€¼æ±‚è§£
# ä½¿ç”¨optimizer.minimize

x = tf.Variable(0.0,name = "x",dtype = tf.float32)
optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)   

@tf.function
def f():   
    a = tf.constant(1.0)
    b = tf.constant(-2.0)
    c = tf.constant(1.0)
    y = a*tf.pow(x,2)+b*x+c
    return(y)

@tf.function
def train(epoch):  
    for _ in tf.range(epoch):  
        optimizer.minimize(f,[x])
    return(f())


tf.print(train(1000))
tf.print(x)

```

```
0
0.999998569
```

```python

```

å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·"Pythonä¸ç®—æ³•ä¹‹ç¾"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚

![image.png](./data/Pythonä¸ç®—æ³•ä¹‹ç¾logo.jpg)



# ä¸‰ã€TensorFlowçš„å±‚æ¬¡ç»“æ„


æœ¬ç« æˆ‘ä»¬ä»‹ç»TensorFlowä¸­5ä¸ªä¸åŒçš„å±‚æ¬¡ç»“æ„ï¼šå³ç¡¬ä»¶å±‚ï¼Œå†…æ ¸å±‚ï¼Œä½é˜¶APIï¼Œä¸­é˜¶APIï¼Œé«˜é˜¶APIã€‚å¹¶ä»¥çº¿æ€§å›å½’ä¸ºä¾‹ï¼Œç›´è§‚å¯¹æ¯”å±•ç¤ºåœ¨ä¸åŒå±‚çº§å®ç°æ¨¡å‹çš„ç‰¹ç‚¹ã€‚

TensorFlowçš„å±‚æ¬¡ç»“æ„ä»ä½åˆ°é«˜å¯ä»¥åˆ†æˆå¦‚ä¸‹äº”å±‚ã€‚

æœ€åº•å±‚ä¸ºç¡¬ä»¶å±‚ï¼ŒTensorFlowæ”¯æŒCPUã€GPUæˆ–TPUåŠ å…¥è®¡ç®—èµ„æºæ± ã€‚

ç¬¬äºŒå±‚ä¸ºC++å®ç°çš„å†…æ ¸ï¼Œkernelå¯ä»¥è·¨å¹³å°åˆ†å¸ƒè¿è¡Œã€‚

ç¬¬ä¸‰å±‚ä¸ºPythonå®ç°çš„æ“ä½œç¬¦ï¼Œæä¾›äº†å°è£…C++å†…æ ¸çš„ä½çº§APIæŒ‡ä»¤ï¼Œä¸»è¦åŒ…æ‹¬å„ç§å¼ é‡æ“ä½œç®—å­ã€è®¡ç®—å›¾ã€è‡ªåŠ¨å¾®åˆ†.
å¦‚tf.Variable,tf.constant,tf.function,tf.GradientTape,tf.nn.softmax...
å¦‚æœæŠŠæ¨¡å‹æ¯”ä½œä¸€ä¸ªæˆ¿å­ï¼Œé‚£ä¹ˆç¬¬ä¸‰å±‚APIå°±æ˜¯ã€æ¨¡å‹ä¹‹ç –ã€‘ã€‚

ç¬¬å››å±‚ä¸ºPythonå®ç°çš„æ¨¡å‹ç»„ä»¶ï¼Œå¯¹ä½çº§APIè¿›è¡Œäº†å‡½æ•°å°è£…ï¼Œä¸»è¦åŒ…æ‹¬å„ç§æ¨¡å‹å±‚ï¼ŒæŸå¤±å‡½æ•°ï¼Œä¼˜åŒ–å™¨ï¼Œæ•°æ®ç®¡é“ï¼Œç‰¹å¾åˆ—ç­‰ç­‰ã€‚
å¦‚tf.keras.layers,tf.keras.losses,tf.keras.metrics,tf.keras.optimizers,tf.data.DataSet,tf.feature_column...
å¦‚æœæŠŠæ¨¡å‹æ¯”ä½œä¸€ä¸ªæˆ¿å­ï¼Œé‚£ä¹ˆç¬¬å››å±‚APIå°±æ˜¯ã€æ¨¡å‹ä¹‹å¢™ã€‘ã€‚

ç¬¬äº”å±‚ä¸ºPythonå®ç°çš„æ¨¡å‹æˆå“ï¼Œä¸€èˆ¬ä¸ºæŒ‰ç…§OOPæ–¹å¼å°è£…çš„é«˜çº§APIï¼Œä¸»è¦ä¸ºtf.keras.modelsæä¾›çš„æ¨¡å‹çš„ç±»æ¥å£ã€‚
å¦‚æœæŠŠæ¨¡å‹æ¯”ä½œä¸€ä¸ªæˆ¿å­ï¼Œé‚£ä¹ˆç¬¬äº”å±‚APIå°±æ˜¯æ¨¡å‹æœ¬èº«ï¼Œå³ã€æ¨¡å‹ä¹‹å±‹ã€‘ã€‚


<img src="./data/tensorflow_structure.jpg">


å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·"Pythonä¸ç®—æ³•ä¹‹ç¾"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚

![](./data/Pythonä¸ç®—æ³•ä¹‹ç¾logo.jpg)



# 3-1,ä½é˜¶APIç¤ºèŒƒ

ä¸‹é¢çš„èŒƒä¾‹ä½¿ç”¨TensorFlowçš„ä½é˜¶APIå®ç°çº¿æ€§å›å½’æ¨¡å‹ã€‚

ä½é˜¶APIä¸»è¦åŒ…æ‹¬å¼ é‡æ“ä½œï¼Œè®¡ç®—å›¾å’Œè‡ªåŠ¨å¾®åˆ†ã€‚

```python
import tensorflow as tf

#æ‰“å°æ—¶é—´åˆ†å‰²çº¿
@tf.function
def printbar():
    ts = tf.timestamp()
    today_ts = ts%(24*60*60)

    hour = tf.cast(today_ts//3600+8,tf.int32)%tf.constant(24)
    minite = tf.cast((today_ts%3600)//60,tf.int32)
    second = tf.cast(tf.floor(today_ts%60),tf.int32)
    
    def timeformat(m):
        if tf.strings.length(tf.strings.format("{}",m))==1:
            return(tf.strings.format("0{}",m))
        else:
            return(tf.strings.format("{}",m))
    
    timestring = tf.strings.join([timeformat(hour),timeformat(minite),
                timeformat(second)],separator = ":")
    tf.print("=========="*8,end = "")
    tf.print(timestring)
    
```

```python
#æ ·æœ¬æ•°é‡
n = 400

# ç”Ÿæˆæµ‹è¯•ç”¨æ•°æ®é›†
X = tf.random.uniform([n,2],minval=-10,maxval=10) 
w0 = tf.constant([[2.0],[-1.0]])
b0 = tf.constant(3.0)
Y = X@w0 + b0 + tf.random.normal([n,1],mean = 0.0,stddev= 2.0)  # @è¡¨ç¤ºçŸ©é˜µä¹˜æ³•,å¢åŠ æ­£æ€æ‰°åŠ¨

```

```python
#ä½¿ç”¨åŠ¨æ€å›¾è°ƒè¯•

w = tf.Variable(tf.random.normal(w0.shape))
b = tf.Variable(0.0)

def train(epoches):
    for epoch in tf.range(1,epoches+1):
        with tf.GradientTape() as tape:
            #æ­£å‘ä¼ æ’­æ±‚æŸå¤±
            Y_hat = X@w + b
            loss = tf.squeeze(tf.transpose(Y-Y_hat)@(Y-Y_hat))/(2.0*n)   

        # åå‘ä¼ æ’­æ±‚æ¢¯åº¦
        dloss_dw,dloss_db = tape.gradient(loss,[w,b])
        # æ¢¯åº¦ä¸‹é™æ³•æ›´æ–°å‚æ•°
        w.assign(w - 0.001*dloss_dw)
        b.assign(b - 0.001*dloss_db)
        if epoch%1000 == 0:
            printbar()
            tf.print("epoch =",epoch," loss =",loss,)
            tf.print("w =",w)
            tf.print("b =",b)
            tf.print("")
            
train(5000)
```

![](./data/3-1-è¾“å‡º01.jpg)

```python
##ä½¿ç”¨autographæœºåˆ¶è½¬æ¢æˆé™æ€å›¾åŠ é€Ÿ

w = tf.Variable(tf.random.normal(w0.shape))
b = tf.Variable(0.0)

@tf.function
def train(epoches):
    for epoch in tf.range(1,epoches+1):
        with tf.GradientTape() as tape:
            #æ­£å‘ä¼ æ’­æ±‚æŸå¤±
            Y_hat = X@w + b
            loss = tf.squeeze(tf.transpose(Y-Y_hat)@(Y-Y_hat))/(2.0*n)   

        # åå‘ä¼ æ’­æ±‚æ¢¯åº¦
        dloss_dw,dloss_db = tape.gradient(loss,[w,b])
        # æ¢¯åº¦ä¸‹é™æ³•æ›´æ–°å‚æ•°
        w.assign(w - 0.001*dloss_dw)
        b.assign(b - 0.001*dloss_db)
        if epoch%1000 == 0:
            printbar()
            tf.print("epoch =",epoch," loss =",loss,)
            tf.print("w =",w)
            tf.print("b =",b)
            tf.print("")
train(5000)
```

![](./data/3-1-è¾“å‡º02.jpg)


å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·"Pythonä¸ç®—æ³•ä¹‹ç¾"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚

![image.png](./data/Pythonä¸ç®—æ³•ä¹‹ç¾logo.jpg)



# 3-2,ä¸­é˜¶APIç¤ºèŒƒ

ä¸‹é¢çš„èŒƒä¾‹ä½¿ç”¨TensorFlowçš„ä¸­é˜¶APIå®ç°çº¿æ€§å›å½’æ¨¡å‹ã€‚

TensorFlowçš„ä¸­é˜¶APIä¸»è¦åŒ…æ‹¬å„ç§æ¨¡å‹å±‚ï¼ŒæŸå¤±å‡½æ•°ï¼Œä¼˜åŒ–å™¨ï¼Œæ•°æ®ç®¡é“ï¼Œç‰¹å¾åˆ—ç­‰ç­‰ã€‚

```python
import tensorflow as tf
from tensorflow.keras import layers,losses,metrics,optimizers


#æ‰“å°æ—¶é—´åˆ†å‰²çº¿
@tf.function
def printbar():
    ts = tf.timestamp()
    today_ts = ts%(24*60*60)

    hour = tf.cast(today_ts//3600+8,tf.int32)%tf.constant(24)
    minite = tf.cast((today_ts%3600)//60,tf.int32)
    second = tf.cast(tf.floor(today_ts%60),tf.int32)
    
    def timeformat(m):
        if tf.strings.length(tf.strings.format("{}",m))==1:
            return(tf.strings.format("0{}",m))
        else:
            return(tf.strings.format("{}",m))
    
    timestring = tf.strings.join([timeformat(hour),timeformat(minite),
                timeformat(second)],separator = ":")
    tf.print("=========="*8,end = "")
    tf.print(timestring)
    
```

```python
#æ ·æœ¬æ•°é‡
n = 800

# ç”Ÿæˆæµ‹è¯•ç”¨æ•°æ®é›†
X = tf.random.uniform([n,2],minval=-10,maxval=10) 
w0 = tf.constant([[2.0],[-1.0]])
b0 = tf.constant(3.0)
Y = X@w0 + b0 + tf.random.normal([n,1],mean = 0.0,stddev= 2.0)  # @è¡¨ç¤ºçŸ©é˜µä¹˜æ³•,å¢åŠ æ­£æ€æ‰°åŠ¨

#æ„å»ºè¾“å…¥æ•°æ®ç®¡é“
ds = tf.data.Dataset.from_tensor_slices((X,Y)) \
     .shuffle(buffer_size = 1000).batch(100) \
     .prefetch(tf.data.experimental.AUTOTUNE)  

#å®šä¹‰ä¼˜åŒ–å™¨
optimizer = optimizers.SGD(learning_rate=0.001)

```

```python
linear = layers.Dense(units = 1)
linear.build(input_shape = (2,)) 

@tf.function
def train(epoches):
    for epoch in tf.range(1,epoches+1):
        L = tf.constant(0.0) #ä½¿ç”¨Lè®°å½•losså€¼
        for X_batch,Y_batch in ds:
            with tf.GradientTape() as tape:
                Y_hat = linear(X_batch)
                loss = losses.mean_squared_error(tf.reshape(Y_hat,[-1]),tf.reshape(Y_batch,[-1]))
            grads = tape.gradient(loss,linear.variables)
            optimizer.apply_gradients(zip(grads,linear.variables))
            L = loss
        
        if(epoch%100==0):
            printbar()
            tf.print("epoch =",epoch,"loss =",L)
            tf.print("w =",linear.kernel)
            tf.print("b =",linear.bias)
            tf.print("")

train(500)
```

![](./data/3-2-è¾“å‡º01.jpg)


å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·"Pythonä¸ç®—æ³•ä¹‹ç¾"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚

![image.png](./data/Pythonä¸ç®—æ³•ä¹‹ç¾logo.jpg)



# 3-3,é«˜é˜¶APIç¤ºèŒƒ

ä¸‹é¢çš„èŒƒä¾‹ä½¿ç”¨TensorFlowçš„é«˜é˜¶APIå®ç°çº¿æ€§å›å½’æ¨¡å‹ã€‚

TensorFlowçš„é«˜é˜¶APIä¸»è¦ä¸ºtf.keras.modelsæä¾›çš„æ¨¡å‹çš„ç±»æ¥å£ã€‚


ä½¿ç”¨Kerasæ¥å£æœ‰ä»¥ä¸‹3ç§æ–¹å¼æ„å»ºæ¨¡å‹ï¼šä½¿ç”¨SequentialæŒ‰å±‚é¡ºåºæ„å»ºæ¨¡å‹ï¼Œä½¿ç”¨å‡½æ•°å¼APIæ„å»ºä»»æ„ç»“æ„æ¨¡å‹ï¼Œç»§æ‰¿ModelåŸºç±»æ„å»ºè‡ªå®šä¹‰æ¨¡å‹ã€‚

æ­¤å¤„åˆ†åˆ«æ¼”ç¤ºä½¿ç”¨SequentialæŒ‰å±‚é¡ºåºæ„å»ºæ¨¡å‹ä»¥åŠç»§æ‰¿ModelåŸºç±»æ„å»ºè‡ªå®šä¹‰æ¨¡å‹ã€‚


### ä¸€ï¼Œä½¿ç”¨SequentialæŒ‰å±‚é¡ºåºæ„å»ºæ¨¡å‹ã€é¢å‘æ–°æ‰‹ã€‘

```python
import tensorflow as tf
from tensorflow.keras import models,layers,optimizers

#æ ·æœ¬æ•°é‡
n = 800

# ç”Ÿæˆæµ‹è¯•ç”¨æ•°æ®é›†
X = tf.random.uniform([n,2],minval=-10,maxval=10) 
w0 = tf.constant([[2.0],[-1.0]])
b0 = tf.constant(3.0)

Y = X@w0 + b0 + tf.random.normal([n,1],mean = 0.0,stddev= 2.0)  # @è¡¨ç¤ºçŸ©é˜µä¹˜æ³•,å¢åŠ æ­£æ€æ‰°åŠ¨
```

```python
tf.keras.backend.clear_session()

linear = models.Sequential()
linear.add(layers.Dense(1,input_shape =(2,)))
linear.summary()
```

![](./data/3-3-åºåˆ—ç»“æ„.jpg)

```python
### ä½¿ç”¨fitæ–¹æ³•è¿›è¡Œè®­ç»ƒ

linear.compile(optimizer="adam",loss="mse",metrics=["mae"])
linear.fit(X,Y,batch_size = 20,epochs = 200)  

tf.print("w = ",linear.layers[0].kernel)
tf.print("b = ",linear.layers[0].bias)

```

![](./data/3-3-å†…ç½®è®­ç»ƒ.jpg)

```python

```

### äºŒï¼Œç»§æ‰¿ModelåŸºç±»æ„å»ºè‡ªå®šä¹‰æ¨¡å‹ã€é¢å‘ä¸“å®¶ã€‘

```python
import tensorflow as tf
from tensorflow.keras import models,layers,optimizers,losses,metrics


#æ‰“å°æ—¶é—´åˆ†å‰²çº¿
@tf.function
def printbar():
    ts = tf.timestamp()
    today_ts = ts%(24*60*60)

    hour = tf.cast(today_ts//3600+8,tf.int32)%tf.constant(24)
    minite = tf.cast((today_ts%3600)//60,tf.int32)
    second = tf.cast(tf.floor(today_ts%60),tf.int32)
    
    def timeformat(m):
        if tf.strings.length(tf.strings.format("{}",m))==1:
            return(tf.strings.format("0{}",m))
        else:
            return(tf.strings.format("{}",m))
    
    timestring = tf.strings.join([timeformat(hour),timeformat(minite),
                timeformat(second)],separator = ":")
    tf.print("=========="*8,end = "")
    tf.print(timestring)
    
```

```python
#æ ·æœ¬æ•°é‡
n = 800

# ç”Ÿæˆæµ‹è¯•ç”¨æ•°æ®é›†
X = tf.random.uniform([n,2],minval=-10,maxval=10) 
w0 = tf.constant([[2.0],[-1.0]])
b0 = tf.constant(3.0)

Y = X@w0 + b0 + tf.random.normal([n,1],mean = 0.0,stddev= 2.0)  # @è¡¨ç¤ºçŸ©é˜µä¹˜æ³•,å¢åŠ æ­£æ€æ‰°åŠ¨

ds_train = tf.data.Dataset.from_tensor_slices((X[0:n*3//4,:],Y[0:n*3//4,:])) \
     .shuffle(buffer_size = 1000).batch(20) \
     .prefetch(tf.data.experimental.AUTOTUNE) \
     .cache()

ds_valid = tf.data.Dataset.from_tensor_slices((X[n*3//4:,:],Y[n*3//4:,:])) \
     .shuffle(buffer_size = 1000).batch(20) \
     .prefetch(tf.data.experimental.AUTOTUNE) \
     .cache()

```

```python
tf.keras.backend.clear_session()

class MyModel(models.Model):
    def __init__(self):
        super(MyModel, self).__init__()
        
    def build(self,input_shape):
        self.dense1 = layers.Dense(1)   
        super(MyModel,self).build(input_shape)
    
    def call(self, x):
        y = self.dense1(x)
        return(y)

model = MyModel()
model.build(input_shape =(None,2))
model.summary()

```

![](./data/3-3-æ¨¡å‹ç»“æ„.jpg)

```python
### è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯(ä¸“å®¶æ•™ç¨‹)


optimizer = optimizers.Adam()
loss_func = losses.MeanSquaredError()

train_loss = tf.keras.metrics.Mean(name='train_loss')
train_metric = tf.keras.metrics.MeanAbsoluteError(name='train_mae')

valid_loss = tf.keras.metrics.Mean(name='valid_loss')
valid_metric = tf.keras.metrics.MeanAbsoluteError(name='valid_mae')


@tf.function
def train_step(model, features, labels):
    with tf.GradientTape() as tape:
        predictions = model(features)
        loss = loss_func(labels, predictions)
    gradients = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))

    train_loss.update_state(loss)
    train_metric.update_state(labels, predictions)

@tf.function
def valid_step(model, features, labels):
    predictions = model(features)
    batch_loss = loss_func(labels, predictions)
    valid_loss.update_state(batch_loss)
    valid_metric.update_state(labels, predictions)
    

@tf.function
def train_model(model,ds_train,ds_valid,epochs):
    for epoch in tf.range(1,epochs+1):
        for features, labels in ds_train:
            train_step(model,features,labels)

        for features, labels in ds_valid:
            valid_step(model,features,labels)

        logs = 'Epoch={},Loss:{},MAE:{},Valid Loss:{},Valid MAE:{}'
        
        if  epoch%100 ==0:
            printbar()
            tf.print(tf.strings.format(logs,
            (epoch,train_loss.result(),train_metric.result(),valid_loss.result(),valid_metric.result())))
            tf.print("w=",model.layers[0].kernel)
            tf.print("b=",model.layers[0].bias)
            tf.print("")
        
        train_loss.reset_states()
        valid_loss.reset_states()
        train_metric.reset_states()
        valid_metric.reset_states()

train_model(model,ds_train,ds_valid,400)

```

![](./data/3-3-è‡ªå®šä¹‰è®­ç»ƒ.jpg)


å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·"Pythonä¸ç®—æ³•ä¹‹ç¾"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚

![image.png](./data/Pythonä¸ç®—æ³•ä¹‹ç¾logo.jpg)



# å››ã€TensorFlowçš„ä½é˜¶API

TensorFlowçš„ä½é˜¶APIä¸»è¦åŒ…æ‹¬å¼ é‡æ“ä½œï¼Œè®¡ç®—å›¾å’Œè‡ªåŠ¨å¾®åˆ†ã€‚

å¦‚æœæŠŠæ¨¡å‹æ¯”ä½œä¸€ä¸ªæˆ¿å­ï¼Œé‚£ä¹ˆä½é˜¶APIå°±æ˜¯ã€æ¨¡å‹ä¹‹ç –ã€‘ã€‚

åœ¨ä½é˜¶APIå±‚æ¬¡ä¸Šï¼Œå¯ä»¥æŠŠTensorFlowå½“åšä¸€ä¸ªå¢å¼ºç‰ˆçš„numpyæ¥ä½¿ç”¨ã€‚

TensorFlowæä¾›çš„æ–¹æ³•æ¯”numpyæ›´å…¨é¢ï¼Œè¿ç®—é€Ÿåº¦æ›´å¿«ï¼Œå¦‚æœéœ€è¦çš„è¯ï¼Œè¿˜å¯ä»¥ä½¿ç”¨GPUè¿›è¡ŒåŠ é€Ÿã€‚

å‰é¢å‡ ç« æˆ‘ä»¬å¯¹ä½é˜¶APIå·²ç»æœ‰äº†ä¸€ä¸ªæ•´ä½“çš„è®¤è¯†ï¼Œæœ¬ç« æˆ‘ä»¬å°†é‡ç‚¹è¯¦ç»†ä»‹ç»å¼ é‡æ“ä½œå’ŒAutographè®¡ç®—å›¾ã€‚


å¼ é‡çš„æ“ä½œä¸»è¦åŒ…æ‹¬å¼ é‡çš„ç»“æ„æ“ä½œå’Œå¼ é‡çš„æ•°å­¦è¿ç®—ã€‚

å¼ é‡ç»“æ„æ“ä½œè¯¸å¦‚ï¼šå¼ é‡åˆ›å»ºï¼Œç´¢å¼•åˆ‡ç‰‡ï¼Œç»´åº¦å˜æ¢ï¼Œåˆå¹¶åˆ†å‰²ã€‚

å¼ é‡æ•°å­¦è¿ç®—ä¸»è¦æœ‰ï¼šæ ‡é‡è¿ç®—ï¼Œå‘é‡è¿ç®—ï¼ŒçŸ©é˜µè¿ç®—ã€‚å¦å¤–æˆ‘ä»¬ä¼šä»‹ç»å¼ é‡è¿ç®—çš„å¹¿æ’­æœºåˆ¶ã€‚

Autographè®¡ç®—å›¾æˆ‘ä»¬å°†ä»‹ç»ä½¿ç”¨Autographçš„è§„èŒƒå»ºè®®ï¼ŒAutographçš„æœºåˆ¶åŸç†ï¼ŒAutographå’Œtf.Module.



å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·"Pythonä¸ç®—æ³•ä¹‹ç¾"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚

![](./data/Pythonä¸ç®—æ³•ä¹‹ç¾logo.jpg)



# 4-1,å¼ é‡çš„ç»“æ„æ“ä½œ

å¼ é‡çš„æ“ä½œä¸»è¦åŒ…æ‹¬å¼ é‡çš„ç»“æ„æ“ä½œå’Œå¼ é‡çš„æ•°å­¦è¿ç®—ã€‚

å¼ é‡ç»“æ„æ“ä½œè¯¸å¦‚ï¼šå¼ é‡åˆ›å»ºï¼Œç´¢å¼•åˆ‡ç‰‡ï¼Œç»´åº¦å˜æ¢ï¼Œåˆå¹¶åˆ†å‰²ã€‚

å¼ é‡æ•°å­¦è¿ç®—ä¸»è¦æœ‰ï¼šæ ‡é‡è¿ç®—ï¼Œå‘é‡è¿ç®—ï¼ŒçŸ©é˜µè¿ç®—ã€‚å¦å¤–æˆ‘ä»¬ä¼šä»‹ç»å¼ é‡è¿ç®—çš„å¹¿æ’­æœºåˆ¶ã€‚

æœ¬ç¯‡æˆ‘ä»¬ä»‹ç»å¼ é‡çš„ç»“æ„æ“ä½œã€‚


### ä¸€ï¼Œåˆ›å»ºå¼ é‡


å¼ é‡åˆ›å»ºçš„è®¸å¤šæ–¹æ³•å’Œnumpyä¸­åˆ›å»ºarrayçš„æ–¹æ³•å¾ˆåƒã€‚

```python
import tensorflow as tf
import numpy as np 
```

```python
a = tf.constant([1,2,3],dtype = tf.float32)
tf.print(a)
```

```
[1 2 3]
```

```python
b = tf.range(1,10,delta = 2)
tf.print(b)
```

```
[1 3 5 7 9]
```

```python
c = tf.linspace(0.0,2*3.14,100)
tf.print(c)
```

```
[0 0.0634343475 0.126868695 ... 6.15313148 6.21656609 6.28]
```

```python
d = tf.zeros([3,3])
tf.print(d)
```

```
[[0 0 0]
 [0 0 0]
 [0 0 0]]
```

```python
a = tf.ones([3,3])
b = tf.zeros_like(a,dtype= tf.float32)
tf.print(a)
tf.print(b)
```

```
[[1 1 1]
 [1 1 1]
 [1 1 1]]
[[0 0 0]
 [0 0 0]
 [0 0 0]]
```

```python
b = tf.fill([3,2],5)
tf.print(b)
```

```
[[5 5]
 [5 5]
 [5 5]]
```

```python
#å‡åŒ€åˆ†å¸ƒéšæœº
tf.random.set_seed(1.0)
a = tf.random.uniform([5],minval=0,maxval=10)
tf.print(a)
```

```
[1.65130854 9.01481247 6.30974197 4.34546089 2.9193902]
```

```python
#æ­£æ€åˆ†å¸ƒéšæœº
b = tf.random.normal([3,3],mean=0.0,stddev=1.0)
tf.print(b)
```

```
[[0.403087884 -1.0880208 -0.0630953535]
 [1.33655667 0.711760104 -0.489286453]
 [-0.764221311 -1.03724861 -1.25193381]]
```

```python
#æ­£æ€åˆ†å¸ƒéšæœºï¼Œå‰”é™¤2å€æ–¹å·®ä»¥å¤–æ•°æ®é‡æ–°ç”Ÿæˆ
c = tf.random.truncated_normal((5,5), mean=0.0, stddev=1.0, dtype=tf.float32)
tf.print(c)
```

```
[[-0.457012236 -0.406867266 0.728577733 -0.892977774 -0.369404584]
 [0.323488563 1.19383323 0.888299048 1.25985599 -1.95951891]
 [-0.202244401 0.294496894 -0.468728036 1.29494202 1.48142183]
 [0.0810953453 1.63843894 0.556645 0.977199793 -1.17777884]
 [1.67368948 0.0647980496 -0.705142677 -0.281972528 0.126546144]]
```

```python
# ç‰¹æ®ŠçŸ©é˜µ
I = tf.eye(3,3) #å•ä½çŸ©é˜µ
tf.print(I)
tf.print(" ")
t = tf.linalg.diag([1,2,3]) #å¯¹è§’é˜µ
tf.print(t)
```

```
[[1 0 0]
 [0 1 0]
 [0 0 1]]
 
[[1 0 0]
 [0 2 0]
 [0 0 3]]
```

```python

```

### äºŒ ï¼Œç´¢å¼•åˆ‡ç‰‡


å¼ é‡çš„ç´¢å¼•åˆ‡ç‰‡æ–¹å¼å’Œnumpyå‡ ä¹æ˜¯ä¸€æ ·çš„ã€‚åˆ‡ç‰‡æ—¶æ”¯æŒç¼ºçœå‚æ•°å’Œçœç•¥å·ã€‚

å¯¹äºtf.Variable,å¯ä»¥é€šè¿‡ç´¢å¼•å’Œåˆ‡ç‰‡å¯¹éƒ¨åˆ†å…ƒç´ è¿›è¡Œä¿®æ”¹ã€‚

å¯¹äºæå–å¼ é‡çš„è¿ç»­å­åŒºåŸŸï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨tf.slice.

æ­¤å¤–ï¼Œå¯¹äºä¸è§„åˆ™çš„åˆ‡ç‰‡æå–,å¯ä»¥ä½¿ç”¨tf.gather,tf.gather_nd,tf.boolean_maskã€‚

tf.boolean_maskåŠŸèƒ½æœ€ä¸ºå¼ºå¤§ï¼Œå®ƒå¯ä»¥å®ç°tf.gather,tf.gather_ndçš„åŠŸèƒ½ï¼Œå¹¶ä¸”tf.boolean_maskè¿˜å¯ä»¥å®ç°å¸ƒå°”ç´¢å¼•ã€‚

å¦‚æœè¦é€šè¿‡ä¿®æ”¹å¼ é‡çš„æŸäº›å…ƒç´ å¾—åˆ°æ–°çš„å¼ é‡ï¼Œå¯ä»¥ä½¿ç”¨tf.whereï¼Œtf.scatter_ndã€‚

```python
tf.random.set_seed(3)
t = tf.random.uniform([5,5],minval=0,maxval=10,dtype=tf.int32)
tf.print(t)
```

```
[[4 7 4 2 9]
 [9 1 2 4 7]
 [7 2 7 4 0]
 [9 6 9 7 2]
 [3 7 0 0 3]]
```

```python
#ç¬¬0è¡Œ
tf.print(t[0])
```

```
[4 7 4 2 9]
```

```python
#å€’æ•°ç¬¬ä¸€è¡Œ
tf.print(t[-1])
```

```
[3 7 0 0 3]
```

```python
#ç¬¬1è¡Œç¬¬3åˆ—
tf.print(t[1,3])
tf.print(t[1][3])
```

```
4
4
```

```python
#ç¬¬1è¡Œè‡³ç¬¬3è¡Œ
tf.print(t[1:4,:])
tf.print(tf.slice(t,[1,0],[3,5])) #tf.slice(input,begin_vector,size_vector)
```

```
[[9 1 2 4 7]
 [7 2 7 4 0]
 [9 6 9 7 2]]
[[9 1 2 4 7]
 [7 2 7 4 0]
 [9 6 9 7 2]]
```

```python
#ç¬¬1è¡Œè‡³æœ€åä¸€è¡Œï¼Œç¬¬0åˆ—åˆ°æœ€åä¸€åˆ—æ¯éš”ä¸¤åˆ—å–ä¸€åˆ—
tf.print(t[1:4,:4:2])
```

```
[[9 2]
 [7 7]
 [9 9]]
```

```python
#å¯¹å˜é‡æ¥è¯´ï¼Œè¿˜å¯ä»¥ä½¿ç”¨ç´¢å¼•å’Œåˆ‡ç‰‡ä¿®æ”¹éƒ¨åˆ†å…ƒç´ 
x = tf.Variable([[1,2],[3,4]],dtype = tf.float32)
x[1,:].assign(tf.constant([0.0,0.0]))
tf.print(x)
```

```
[[1 2]
 [0 0]]
```

```python
a = tf.random.uniform([3,3,3],minval=0,maxval=10,dtype=tf.int32)
tf.print(a)
```

```
[[[7 3 9]
  [9 0 7]
  [9 6 7]]

 [[1 3 3]
  [0 8 1]
  [3 1 0]]

 [[4 0 6]
  [6 2 2]
  [7 9 5]]]
```

```python
#çœç•¥å·å¯ä»¥è¡¨ç¤ºå¤šä¸ªå†’å·
tf.print(a[...,1])
```

```
[[3 0 6]
 [3 8 1]
 [0 2 9]]
```


ä»¥ä¸Šåˆ‡ç‰‡æ–¹å¼ç›¸å¯¹è§„åˆ™ï¼Œå¯¹äºä¸è§„åˆ™çš„åˆ‡ç‰‡æå–,å¯ä»¥ä½¿ç”¨tf.gather,tf.gather_nd,tf.boolean_maskã€‚

è€ƒè™‘ç­çº§æˆç»©å†Œçš„ä¾‹å­ï¼Œæœ‰4ä¸ªç­çº§ï¼Œæ¯ä¸ªç­çº§10ä¸ªå­¦ç”Ÿï¼Œæ¯ä¸ªå­¦ç”Ÿ7é—¨ç§‘ç›®æˆç»©ã€‚å¯ä»¥ç”¨ä¸€ä¸ª4*10*7çš„å¼ é‡æ¥è¡¨ç¤ºã€‚

```python
scores = tf.random.uniform((4,10,7),minval=0,maxval=100,dtype=tf.int32)
tf.print(scores)
```

```
[[[52 82 66 ... 17 86 14]
  [8 36 94 ... 13 78 41]
  [77 53 51 ... 22 91 56]
  ...
  [11 19 26 ... 89 86 68]
  [60 72 0 ... 11 26 15]
  [24 99 38 ... 97 44 74]]

 [[79 73 73 ... 35 3 81]
  [83 36 31 ... 75 38 85]
  [54 26 67 ... 60 68 98]
  ...
  [20 5 18 ... 32 45 3]
  [72 52 81 ... 88 41 20]
  [0 21 89 ... 53 10 90]]

 [[52 80 22 ... 29 25 60]
  [78 71 54 ... 43 98 81]
  [21 66 53 ... 97 75 77]
  ...
  [6 74 3 ... 53 65 43]
  [98 36 72 ... 33 36 81]
  [61 78 70 ... 7 59 21]]

 [[56 57 45 ... 23 15 3]
  [35 8 82 ... 11 59 97]
  [44 6 99 ... 81 60 27]
  ...
  [76 26 35 ... 51 8 17]
  [33 52 53 ... 78 37 31]
  [71 27 44 ... 0 52 16]]]
```

```python
#æŠ½å–æ¯ä¸ªç­çº§ç¬¬0ä¸ªå­¦ç”Ÿï¼Œç¬¬5ä¸ªå­¦ç”Ÿï¼Œç¬¬9ä¸ªå­¦ç”Ÿçš„å…¨éƒ¨æˆç»©
p = tf.gather(scores,[0,5,9],axis=1)
tf.print(p)
```

```
[[[52 82 66 ... 17 86 14]
  [24 80 70 ... 72 63 96]
  [24 99 38 ... 97 44 74]]

 [[79 73 73 ... 35 3 81]
  [46 10 94 ... 23 18 92]
  [0 21 89 ... 53 10 90]]

 [[52 80 22 ... 29 25 60]
  [19 12 23 ... 87 86 25]
  [61 78 70 ... 7 59 21]]

 [[56 57 45 ... 23 15 3]
  [6 41 79 ... 97 43 13]
  [71 27 44 ... 0 52 16]]]
```

```python
#æŠ½å–æ¯ä¸ªç­çº§ç¬¬0ä¸ªå­¦ç”Ÿï¼Œç¬¬5ä¸ªå­¦ç”Ÿï¼Œç¬¬9ä¸ªå­¦ç”Ÿçš„ç¬¬1é—¨è¯¾ç¨‹ï¼Œç¬¬3é—¨è¯¾ç¨‹ï¼Œç¬¬6é—¨è¯¾ç¨‹æˆç»©
q = tf.gather(tf.gather(scores,[0,5,9],axis=1),[1,3,6],axis=2)
tf.print(q)
```

```
[[[82 55 14]
  [80 46 96]
  [99 58 74]]

 [[73 48 81]
  [10 38 92]
  [21 86 90]]

 [[80 57 60]
  [12 34 25]
  [78 71 21]]

 [[57 75 3]
  [41 47 13]
  [27 96 16]]]
```

```python
# æŠ½å–ç¬¬0ä¸ªç­çº§ç¬¬0ä¸ªå­¦ç”Ÿï¼Œç¬¬2ä¸ªç­çº§çš„ç¬¬4ä¸ªå­¦ç”Ÿï¼Œç¬¬3ä¸ªç­çº§çš„ç¬¬6ä¸ªå­¦ç”Ÿçš„å…¨éƒ¨æˆç»©
#indicesçš„é•¿åº¦ä¸ºé‡‡æ ·æ ·æœ¬çš„ä¸ªæ•°ï¼Œæ¯ä¸ªå…ƒç´ ä¸ºé‡‡æ ·ä½ç½®çš„åæ ‡
s = tf.gather_nd(scores,indices = [(0,0),(2,4),(3,6)])
s
```

```
<tf.Tensor: shape=(3, 7), dtype=int32, numpy=
array([[52, 82, 66, 55, 17, 86, 14],
       [99, 94, 46, 70,  1, 63, 41],
       [46, 83, 70, 80, 90, 85, 17]], dtype=int32)>
```


ä»¥ä¸Štf.gatherå’Œtf.gather_ndçš„åŠŸèƒ½ä¹Ÿå¯ä»¥ç”¨tf.boolean_maskæ¥å®ç°ã€‚

```python
#æŠ½å–æ¯ä¸ªç­çº§ç¬¬0ä¸ªå­¦ç”Ÿï¼Œç¬¬5ä¸ªå­¦ç”Ÿï¼Œç¬¬9ä¸ªå­¦ç”Ÿçš„å…¨éƒ¨æˆç»©
p = tf.boolean_mask(scores,[True,False,False,False,False,
                            True,False,False,False,True],axis=1)
tf.print(p)
```

```
[[[52 82 66 ... 17 86 14]
  [24 80 70 ... 72 63 96]
  [24 99 38 ... 97 44 74]]

 [[79 73 73 ... 35 3 81]
  [46 10 94 ... 23 18 92]
  [0 21 89 ... 53 10 90]]

 [[52 80 22 ... 29 25 60]
  [19 12 23 ... 87 86 25]
  [61 78 70 ... 7 59 21]]

 [[56 57 45 ... 23 15 3]
  [6 41 79 ... 97 43 13]
  [71 27 44 ... 0 52 16]]]
```

```python
#æŠ½å–ç¬¬0ä¸ªç­çº§ç¬¬0ä¸ªå­¦ç”Ÿï¼Œç¬¬2ä¸ªç­çº§çš„ç¬¬4ä¸ªå­¦ç”Ÿï¼Œç¬¬3ä¸ªç­çº§çš„ç¬¬6ä¸ªå­¦ç”Ÿçš„å…¨éƒ¨æˆç»©
s = tf.boolean_mask(scores,
    [[True,False,False,False,False,False,False,False,False,False],
     [False,False,False,False,False,False,False,False,False,False],
     [False,False,False,False,True,False,False,False,False,False],
     [False,False,False,False,False,False,True,False,False,False]])
tf.print(s)
```

```
[[52 82 66 ... 17 86 14]
 [99 94 46 ... 1 63 41]
 [46 83 70 ... 90 85 17]]
```

```python
#åˆ©ç”¨tf.boolean_maskå¯ä»¥å®ç°å¸ƒå°”ç´¢å¼•

#æ‰¾åˆ°çŸ©é˜µä¸­å°äº0çš„å…ƒç´ 
c = tf.constant([[-1,1,-1],[2,2,-2],[3,-3,3]],dtype=tf.float32)
tf.print(c,"\n")

tf.print(tf.boolean_mask(c,c<0),"\n") 
tf.print(c[c<0]) #å¸ƒå°”ç´¢å¼•ï¼Œä¸ºboolean_maskçš„è¯­æ³•ç³–å½¢å¼
```

```
[[-1 1 -1]
 [2 2 -2]
 [3 -3 3]] 

[-1 -1 -2 -3] 

[-1 -1 -2 -3]
```

```python

```

ä»¥ä¸Šè¿™äº›æ–¹æ³•ä»…èƒ½æå–å¼ é‡çš„éƒ¨åˆ†å…ƒç´ å€¼ï¼Œä½†ä¸èƒ½æ›´æ”¹å¼ é‡çš„éƒ¨åˆ†å…ƒç´ å€¼å¾—åˆ°æ–°çš„å¼ é‡ã€‚

å¦‚æœè¦é€šè¿‡ä¿®æ”¹å¼ é‡çš„éƒ¨åˆ†å…ƒç´ å€¼å¾—åˆ°æ–°çš„å¼ é‡ï¼Œå¯ä»¥ä½¿ç”¨tf.whereå’Œtf.scatter_ndã€‚

tf.whereå¯ä»¥ç†è§£ä¸ºifçš„å¼ é‡ç‰ˆæœ¬ï¼Œæ­¤å¤–å®ƒè¿˜å¯ä»¥ç”¨äºæ‰¾åˆ°æ»¡è¶³æ¡ä»¶çš„æ‰€æœ‰å…ƒç´ çš„ä½ç½®åæ ‡ã€‚

tf.scatter_ndçš„ä½œç”¨å’Œtf.gather_ndæœ‰äº›ç›¸åï¼Œtf.gather_ndç”¨äºæ”¶é›†å¼ é‡çš„ç»™å®šä½ç½®çš„å…ƒç´ ï¼Œ

è€Œtf.scatter_ndå¯ä»¥å°†æŸäº›å€¼æ’å…¥åˆ°ä¸€ä¸ªç»™å®šshapeçš„å…¨0çš„å¼ é‡çš„æŒ‡å®šä½ç½®å¤„ã€‚

```python
#æ‰¾åˆ°å¼ é‡ä¸­å°äº0çš„å…ƒç´ ,å°†å…¶æ¢æˆnp.nanå¾—åˆ°æ–°çš„å¼ é‡
#tf.whereå’Œnp.whereä½œç”¨ç±»ä¼¼ï¼Œå¯ä»¥ç†è§£ä¸ºifçš„å¼ é‡ç‰ˆæœ¬

c = tf.constant([[-1,1,-1],[2,2,-2],[3,-3,3]],dtype=tf.float32)
d = tf.where(c<0,tf.fill(c.shape,np.nan),c) 
d
```

```
<tf.Tensor: shape=(3, 3), dtype=float32, numpy=
array([[nan,  1., nan],
       [ 2.,  2., nan],
       [ 3., nan,  3.]], dtype=float32)>
```

```python

```

```python
#å¦‚æœwhereåªæœ‰ä¸€ä¸ªå‚æ•°ï¼Œå°†è¿”å›æ‰€æœ‰æ»¡è¶³æ¡ä»¶çš„ä½ç½®åæ ‡
indices = tf.where(c<0)
indices
```

```
<tf.Tensor: shape=(4, 2), dtype=int64, numpy=
array([[0, 0],
       [0, 2],
       [1, 2],
       [2, 1]])>
```

```python
#å°†å¼ é‡çš„ç¬¬[0,0]å’Œ[2,1]ä¸¤ä¸ªä½ç½®å…ƒç´ æ›¿æ¢ä¸º0å¾—åˆ°æ–°çš„å¼ é‡
d = c - tf.scatter_nd([[0,0],[2,1]],[c[0,0],c[2,1]],c.shape)
d
```

```
<tf.Tensor: shape=(3, 3), dtype=float32, numpy=
array([[ 0.,  1., -1.],
       [ 2.,  2., -2.],
       [ 3.,  0.,  3.]], dtype=float32)>

```

```python
#scatter_ndçš„ä½œç”¨å’Œgather_ndæœ‰äº›ç›¸å
#å¯ä»¥å°†æŸäº›å€¼æ’å…¥åˆ°ä¸€ä¸ªç»™å®šshapeçš„å…¨0çš„å¼ é‡çš„æŒ‡å®šä½ç½®å¤„ã€‚
indices = tf.where(c<0)
tf.scatter_nd(indices,tf.gather_nd(c,indices),c.shape)
```

```
<tf.Tensor: shape=(3, 3), dtype=float32, numpy=
array([[-1.,  0., -1.],
       [ 0.,  0., -2.],
       [ 0., -3.,  0.]], dtype=float32)>
```

```python

```

### ä¸‰ï¼Œç»´åº¦å˜æ¢


ç»´åº¦å˜æ¢ç›¸å…³å‡½æ•°ä¸»è¦æœ‰ tf.reshape, tf.squeeze, tf.expand_dims, tf.transpose.

tf.reshape å¯ä»¥æ”¹å˜å¼ é‡çš„å½¢çŠ¶ã€‚

tf.squeeze å¯ä»¥å‡å°‘ç»´åº¦ã€‚

tf.expand_dims å¯ä»¥å¢åŠ ç»´åº¦ã€‚

tf.transpose å¯ä»¥äº¤æ¢ç»´åº¦ã€‚



tf.reshapeå¯ä»¥æ”¹å˜å¼ é‡çš„å½¢çŠ¶ï¼Œä½†æ˜¯å…¶æœ¬è´¨ä¸Šä¸ä¼šæ”¹å˜å¼ é‡å…ƒç´ çš„å­˜å‚¨é¡ºåºï¼Œæ‰€ä»¥ï¼Œè¯¥æ“ä½œå®é™…ä¸Šéå¸¸è¿…é€Ÿï¼Œå¹¶ä¸”æ˜¯å¯é€†çš„ã€‚

```python
a = tf.random.uniform(shape=[1,3,3,2],
                      minval=0,maxval=255,dtype=tf.int32)
tf.print(a.shape)
tf.print(a)
```

```
TensorShape([1, 3, 3, 2])
[[[[135 178]
   [26 116]
   [29 224]]

  [[179 219]
   [153 209]
   [111 215]]

  [[39 7]
   [138 129]
   [59 205]]]]
```

```python
# æ”¹æˆ ï¼ˆ3,6ï¼‰å½¢çŠ¶çš„å¼ é‡
b = tf.reshape(a,[3,6])
tf.print(b.shape)
tf.print(b)
```

```
TensorShape([3, 6])
[[135 178 26 116 29 224]
 [179 219 153 209 111 215]
 [39 7 138 129 59 205]]
```




```python
# æ”¹å›æˆ [1,3,3,2] å½¢çŠ¶çš„å¼ é‡
c = tf.reshape(b,[1,3,3,2])
tf.print(c)
```

```
[[[[135 178]
   [26 116]
   [29 224]]

  [[179 219]
   [153 209]
   [111 215]]

  [[39 7]
   [138 129]
   [59 205]]]]
```

```python

```

å¦‚æœå¼ é‡åœ¨æŸä¸ªç»´åº¦ä¸Šåªæœ‰ä¸€ä¸ªå…ƒç´ ï¼Œåˆ©ç”¨tf.squeezeå¯ä»¥æ¶ˆé™¤è¿™ä¸ªç»´åº¦ã€‚

å’Œtf.reshapeç›¸ä¼¼ï¼Œå®ƒæœ¬è´¨ä¸Šä¸ä¼šæ”¹å˜å¼ é‡å…ƒç´ çš„å­˜å‚¨é¡ºåºã€‚

å¼ é‡çš„å„ä¸ªå…ƒç´ åœ¨å†…å­˜ä¸­æ˜¯çº¿æ€§å­˜å‚¨çš„ï¼Œå…¶ä¸€èˆ¬è§„å¾‹æ˜¯ï¼ŒåŒä¸€å±‚çº§ä¸­çš„ç›¸é‚»å…ƒç´ çš„ç‰©ç†åœ°å€ä¹Ÿç›¸é‚»ã€‚

```python
s = tf.squeeze(a)
tf.print(s.shape)
tf.print(s)
```

```
TensorShape([3, 3, 2])
[[[135 178]
  [26 116]
  [29 224]]

 [[179 219]
  [153 209]
  [111 215]]

 [[39 7]
  [138 129]
  [59 205]]]
```

```python
d = tf.expand_dims(s,axis=0) #åœ¨ç¬¬0ç»´æ’å…¥é•¿åº¦ä¸º1çš„ä¸€ä¸ªç»´åº¦
d
```

```
<tf.Tensor: shape=(1, 3, 3, 2), dtype=int32, numpy=
array([[[[135, 178],
         [ 26, 116],
         [ 29, 224]],

        [[179, 219],
         [153, 209],
         [111, 215]],

        [[ 39,   7],
         [138, 129],
         [ 59, 205]]]], dtype=int32)>
```


tf.transposeå¯ä»¥äº¤æ¢å¼ é‡çš„ç»´åº¦ï¼Œä¸tf.reshapeä¸åŒï¼Œå®ƒä¼šæ”¹å˜å¼ é‡å…ƒç´ çš„å­˜å‚¨é¡ºåºã€‚

tf.transposeå¸¸ç”¨äºå›¾ç‰‡å­˜å‚¨æ ¼å¼çš„å˜æ¢ä¸Šã€‚

```python
# Batch,Height,Width,Channel
a = tf.random.uniform(shape=[100,600,600,4],minval=0,maxval=255,dtype=tf.int32)
tf.print(a.shape)

# è½¬æ¢æˆ Channel,Height,Width,Batch
s= tf.transpose(a,perm=[3,1,2,0])
tf.print(s.shape)
```

```
TensorShape([100, 600, 600, 4])
TensorShape([4, 600, 600, 100])

```

```python

```

### å››ï¼Œåˆå¹¶åˆ†å‰²


å’Œnumpyç±»ä¼¼ï¼Œå¯ä»¥ç”¨tf.concatå’Œtf.stackæ–¹æ³•å¯¹å¤šä¸ªå¼ é‡è¿›è¡Œåˆå¹¶ï¼Œå¯ä»¥ç”¨tf.splitæ–¹æ³•æŠŠä¸€ä¸ªå¼ é‡åˆ†å‰²æˆå¤šä¸ªå¼ é‡ã€‚

tf.concatå’Œtf.stackæœ‰ç•¥å¾®çš„åŒºåˆ«ï¼Œtf.concatæ˜¯è¿æ¥ï¼Œä¸ä¼šå¢åŠ ç»´åº¦ï¼Œè€Œtf.stackæ˜¯å †å ï¼Œä¼šå¢åŠ ç»´åº¦ã€‚

```python
a = tf.constant([[1.0,2.0],[3.0,4.0]])
b = tf.constant([[5.0,6.0],[7.0,8.0]])
c = tf.constant([[9.0,10.0],[11.0,12.0]])

tf.concat([a,b,c],axis = 0)
```

```
<tf.Tensor: shape=(6, 2), dtype=float32, numpy=
array([[ 1.,  2.],
       [ 3.,  4.],
       [ 5.,  6.],
       [ 7.,  8.],
       [ 9., 10.],
       [11., 12.]], dtype=float32)>
```

```python
tf.concat([a,b,c],axis = 1)
```

```
<tf.Tensor: shape=(2, 6), dtype=float32, numpy=
array([[ 1.,  2.,  5.,  6.,  9., 10.],
       [ 3.,  4.,  7.,  8., 11., 12.]], dtype=float32)>
```

```python
tf.stack([a,b,c])
```

```
<tf.Tensor: shape=(3, 2, 2), dtype=float32, numpy=
array([[[ 1.,  2.],
        [ 3.,  4.]],

       [[ 5.,  6.],
        [ 7.,  8.]],

       [[ 9., 10.],
        [11., 12.]]], dtype=float32)>
```

```python
tf.stack([a,b,c],axis=1)
```

```
<tf.Tensor: shape=(2, 3, 2), dtype=float32, numpy=
array([[[ 1.,  2.],
        [ 5.,  6.],
        [ 9., 10.]],

       [[ 3.,  4.],
        [ 7.,  8.],
        [11., 12.]]], dtype=float32)>
```

```python
a = tf.constant([[1.0,2.0],[3.0,4.0]])
b = tf.constant([[5.0,6.0],[7.0,8.0]])
c = tf.constant([[9.0,10.0],[11.0,12.0]])

c = tf.concat([a,b,c],axis = 0)
```

tf.splitæ˜¯tf.concatçš„é€†è¿ç®—ï¼Œå¯ä»¥æŒ‡å®šåˆ†å‰²ä»½æ•°å¹³å‡åˆ†å‰²ï¼Œä¹Ÿå¯ä»¥é€šè¿‡æŒ‡å®šæ¯ä»½çš„è®°å½•æ•°é‡è¿›è¡Œåˆ†å‰²ã€‚

```python
#tf.split(value,num_or_size_splits,axis)
tf.split(c,3,axis = 0)  #æŒ‡å®šåˆ†å‰²ä»½æ•°ï¼Œå¹³å‡åˆ†å‰²
```

```
[<tf.Tensor: shape=(2, 2), dtype=float32, numpy=
 array([[1., 2.],
        [3., 4.]], dtype=float32)>,
 <tf.Tensor: shape=(2, 2), dtype=float32, numpy=
 array([[5., 6.],
        [7., 8.]], dtype=float32)>,
 <tf.Tensor: shape=(2, 2), dtype=float32, numpy=
 array([[ 9., 10.],
        [11., 12.]], dtype=float32)>]
```

```python
tf.split(c,[2,2,2],axis = 0) #æŒ‡å®šæ¯ä»½çš„è®°å½•æ•°é‡
```

```
[<tf.Tensor: shape=(2, 2), dtype=float32, numpy=
 array([[1., 2.],
        [3., 4.]], dtype=float32)>,
 <tf.Tensor: shape=(2, 2), dtype=float32, numpy=
 array([[5., 6.],
        [7., 8.]], dtype=float32)>,
 <tf.Tensor: shape=(2, 2), dtype=float32, numpy=
 array([[ 9., 10.],
        [11., 12.]], dtype=float32)>]
```

```python

```

å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·"Pythonä¸ç®—æ³•ä¹‹ç¾"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚

![image.png](./data/Pythonä¸ç®—æ³•ä¹‹ç¾logo.jpg)



# 4-2,å¼ é‡çš„æ•°å­¦è¿ç®—

å¼ é‡çš„æ“ä½œä¸»è¦åŒ…æ‹¬å¼ é‡çš„ç»“æ„æ“ä½œå’Œå¼ é‡çš„æ•°å­¦è¿ç®—ã€‚

å¼ é‡ç»“æ„æ“ä½œè¯¸å¦‚ï¼šå¼ é‡åˆ›å»ºï¼Œç´¢å¼•åˆ‡ç‰‡ï¼Œç»´åº¦å˜æ¢ï¼Œåˆå¹¶åˆ†å‰²ã€‚

å¼ é‡æ•°å­¦è¿ç®—ä¸»è¦æœ‰ï¼šæ ‡é‡è¿ç®—ï¼Œå‘é‡è¿ç®—ï¼ŒçŸ©é˜µè¿ç®—ã€‚å¦å¤–æˆ‘ä»¬ä¼šä»‹ç»å¼ é‡è¿ç®—çš„å¹¿æ’­æœºåˆ¶ã€‚

æœ¬ç¯‡æˆ‘ä»¬ä»‹ç»å¼ é‡çš„æ•°å­¦è¿ç®—ã€‚

```python

```

### ä¸€ï¼Œæ ‡é‡è¿ç®—


å¼ é‡çš„æ•°å­¦è¿ç®—ç¬¦å¯ä»¥åˆ†ä¸ºæ ‡é‡è¿ç®—ç¬¦ã€å‘é‡è¿ç®—ç¬¦ã€ä»¥åŠçŸ©é˜µè¿ç®—ç¬¦ã€‚

åŠ å‡ä¹˜é™¤ä¹˜æ–¹ï¼Œä»¥åŠä¸‰è§’å‡½æ•°ï¼ŒæŒ‡æ•°ï¼Œå¯¹æ•°ç­‰å¸¸è§å‡½æ•°ï¼Œé€»è¾‘æ¯”è¾ƒè¿ç®—ç¬¦ç­‰éƒ½æ˜¯æ ‡é‡è¿ç®—ç¬¦ã€‚

æ ‡é‡è¿ç®—ç¬¦çš„ç‰¹ç‚¹æ˜¯å¯¹å¼ é‡å®æ–½é€å…ƒç´ è¿ç®—ã€‚

æœ‰äº›æ ‡é‡è¿ç®—ç¬¦å¯¹å¸¸ç”¨çš„æ•°å­¦è¿ç®—ç¬¦è¿›è¡Œäº†é‡è½½ã€‚å¹¶ä¸”æ”¯æŒç±»ä¼¼numpyçš„å¹¿æ’­ç‰¹æ€§ã€‚

è®¸å¤šæ ‡é‡è¿ç®—ç¬¦éƒ½åœ¨ tf.mathæ¨¡å—ä¸‹ã€‚

```python
import tensorflow as tf 
import numpy as np 
```

```python
a = tf.constant([[1.0,2],[-3,4.0]])
b = tf.constant([[5.0,6],[7.0,8.0]])
a+b  #è¿ç®—ç¬¦é‡è½½
```

```
<tf.Tensor: shape=(2, 2), dtype=float32, numpy=
array([[ 6.,  8.],
       [ 4., 12.]], dtype=float32)>
```

```python
a-b 
```

```
<tf.Tensor: shape=(2, 2), dtype=float32, numpy=
array([[ -4.,  -4.],
       [-10.,  -4.]], dtype=float32)>
```

```python
a*b 
```

```
<tf.Tensor: shape=(2, 2), dtype=float32, numpy=
array([[  5.,  12.],
       [-21.,  32.]], dtype=float32)>
```

```python
a/b
```

```
<tf.Tensor: shape=(2, 2), dtype=float32, numpy=
array([[ 0.2       ,  0.33333334],
       [-0.42857143,  0.5       ]], dtype=float32)>
```

```python
a**2
```

```
<tf.Tensor: shape=(2, 2), dtype=float32, numpy=
array([[ 1.,  4.],
       [ 9., 16.]], dtype=float32)>
```

```python
a**(0.5)
```

```
<tf.Tensor: shape=(2, 2), dtype=float32, numpy=
array([[1.       , 1.4142135],
       [      nan, 2.       ]], dtype=float32)>
```

```python
a%3 #modçš„è¿ç®—ç¬¦é‡è½½ï¼Œç­‰ä»·äºm = tf.math.mod(a,3)
```

```
<tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 2, 0], dtype=int32)>
```

```python
a//3  #åœ°æ¿é™¤æ³•
```

```
<tf.Tensor: shape=(2, 2), dtype=float32, numpy=
array([[ 0.,  0.],
       [-1.,  1.]], dtype=float32)>
```

```python
(a>=2)
```

```
<tf.Tensor: shape=(2, 2), dtype=bool, numpy=
array([[False,  True],
       [False,  True]])>
```

```python
(a>=2)&(a<=3)
```

```
<tf.Tensor: shape=(2, 2), dtype=bool, numpy=
array([[False,  True],
       [False, False]])>
```

```python
(a>=2)|(a<=3)
```

```
<tf.Tensor: shape=(2, 2), dtype=bool, numpy=
array([[ True,  True],
       [ True,  True]])>
```

```python
a==5 #tf.equal(a,5)
```

```
<tf.Tensor: shape=(3,), dtype=bool, numpy=array([False, False, False])>
```

```python
tf.sqrt(a)
```

```
<tf.Tensor: shape=(2, 2), dtype=float32, numpy=
array([[1.       , 1.4142135],
       [      nan, 2.       ]], dtype=float32)>
```

```python
a = tf.constant([1.0,8.0])
b = tf.constant([5.0,6.0])
c = tf.constant([6.0,7.0])
tf.add_n([a,b,c])
```

```
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([12., 21.], dtype=float32)>
```

```python
tf.print(tf.maximum(a,b))
```

```
[5 8]
```

```python
tf.print(tf.minimum(a,b))
```

```
[1 6]
```

```python

```

### äºŒï¼Œå‘é‡è¿ç®—


å‘é‡è¿ç®—ç¬¦åªåœ¨ä¸€ä¸ªç‰¹å®šè½´ä¸Šè¿ç®—ï¼Œå°†ä¸€ä¸ªå‘é‡æ˜ å°„åˆ°ä¸€ä¸ªæ ‡é‡æˆ–è€…å¦å¤–ä¸€ä¸ªå‘é‡ã€‚
è®¸å¤šå‘é‡è¿ç®—ç¬¦éƒ½ä»¥reduceå¼€å¤´ã€‚

```python
#å‘é‡reduce
a = tf.range(1,10)
tf.print(tf.reduce_sum(a))
tf.print(tf.reduce_mean(a))
tf.print(tf.reduce_max(a))
tf.print(tf.reduce_min(a))
tf.print(tf.reduce_prod(a))
```

```
45
5
9
1
362880
```

```python
#å¼ é‡æŒ‡å®šç»´åº¦è¿›è¡Œreduce
b = tf.reshape(a,(3,3))
tf.print(tf.reduce_sum(b, axis=1, keepdims=True))
tf.print(tf.reduce_sum(b, axis=0, keepdims=True))
```

```
[[6]
 [15]
 [24]]
[[12 15 18]]
```

```python
#boolç±»å‹çš„reduce
p = tf.constant([True,False,False])
q = tf.constant([False,False,True])
tf.print(tf.reduce_all(p))
tf.print(tf.reduce_any(q))
```

```
0
1
```

```python
#åˆ©ç”¨tf.foldrå®ç°tf.reduce_sum
s = tf.foldr(lambda a,b:a+b,tf.range(10)) 
tf.print(s)
```

```
45
```

```python
#cumæ‰«æç´¯ç§¯
a = tf.range(1,10)
tf.print(tf.math.cumsum(a))
tf.print(tf.math.cumprod(a))
```

```
[1 3 6 ... 28 36 45]
[1 2 6 ... 5040 40320 362880]
```

```python
#argæœ€å¤§æœ€å°å€¼ç´¢å¼•
a = tf.range(1,10)
tf.print(tf.argmax(a))
tf.print(tf.argmin(a))
```

```
8
0
```

```python
#tf.math.top_kå¯ä»¥ç”¨äºå¯¹å¼ é‡æ’åº
a = tf.constant([1,3,7,5,4,8])

values,indices = tf.math.top_k(a,3,sorted=True)
tf.print(values)
tf.print(indices)

#åˆ©ç”¨tf.math.top_kå¯ä»¥åœ¨TensorFlowä¸­å®ç°KNNç®—æ³•
```

```
[8 7 5]
[5 2 3]
```

```python

```

### ä¸‰ï¼ŒçŸ©é˜µè¿ç®—


çŸ©é˜µå¿…é¡»æ˜¯äºŒç»´çš„ã€‚ç±»ä¼¼tf.constant([1,2,3])è¿™æ ·çš„ä¸æ˜¯çŸ©é˜µã€‚

çŸ©é˜µè¿ç®—åŒ…æ‹¬ï¼šçŸ©é˜µä¹˜æ³•ï¼ŒçŸ©é˜µè½¬ç½®ï¼ŒçŸ©é˜µé€†ï¼ŒçŸ©é˜µæ±‚è¿¹ï¼ŒçŸ©é˜µèŒƒæ•°ï¼ŒçŸ©é˜µè¡Œåˆ—å¼ï¼ŒçŸ©é˜µæ±‚ç‰¹å¾å€¼ï¼ŒçŸ©é˜µåˆ†è§£ç­‰è¿ç®—ã€‚

é™¤äº†ä¸€äº›å¸¸ç”¨çš„è¿ç®—å¤–ï¼Œå¤§éƒ¨åˆ†å’ŒçŸ©é˜µæœ‰å…³çš„è¿ç®—éƒ½åœ¨tf.linalgå­åŒ…ä¸­ã€‚

```python
#çŸ©é˜µä¹˜æ³•
a = tf.constant([[1,2],[3,4]])
b = tf.constant([[2,0],[0,2]])
a@b  #ç­‰ä»·äºtf.matmul(a,b)
```

```
<tf.Tensor: shape=(2, 2), dtype=int32, numpy=
array([[2, 4],
       [6, 8]], dtype=int32)>
```

```python
#çŸ©é˜µè½¬ç½®
a = tf.constant([[1.0,2],[3,4]])
tf.transpose(a)
```

```
<tf.Tensor: shape=(2, 2), dtype=float32, numpy=
array([[1., 3.],
       [2., 4.]], dtype=float32)>
```

```python
#çŸ©é˜µé€†ï¼Œå¿…é¡»ä¸ºtf.float32æˆ–tf.doubleç±»å‹
a = tf.constant([[1.0,2],[3.0,4]],dtype = tf.float32)
tf.linalg.inv(a)
```

```
<tf.Tensor: shape=(2, 2), dtype=float32, numpy=
array([[-2.0000002 ,  1.0000001 ],
       [ 1.5000001 , -0.50000006]], dtype=float32)>
```

```python
#çŸ©é˜µæ±‚trace
a = tf.constant([[1.0,2],[3,4]])
tf.linalg.trace(a)
```

```
<tf.Tensor: shape=(), dtype=float32, numpy=5.0>
```

```python
#çŸ©é˜µæ±‚èŒƒæ•°
a = tf.constant([[1.0,2],[3,4]])
tf.linalg.norm(a)
```

```
<tf.Tensor: shape=(), dtype=float32, numpy=5.477226>
```

```python
#çŸ©é˜µè¡Œåˆ—å¼
a = tf.constant([[1.0,2],[3,4]])
tf.linalg.det(a)
```

```
<tf.Tensor: shape=(), dtype=float32, numpy=-2.0>
```

```python
#çŸ©é˜µç‰¹å¾å€¼
tf.linalg.eigvalsh(a)
```

```
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([-0.8541021,  5.854102 ], dtype=float32)>
```

```python
#çŸ©é˜µqråˆ†è§£
a  = tf.constant([[1.0,2.0],[3.0,4.0]],dtype = tf.float32)
q,r = tf.linalg.qr(a)
tf.print(q)
tf.print(r)
tf.print(q@r)
```

```
[[-0.316227794 -0.948683321]
 [-0.948683321 0.316227734]]
[[-3.1622777 -4.4271884]
 [0 -0.632455349]]
[[1.00000012 1.99999976]
 [3 4]]
```

```python
#çŸ©é˜µsvdåˆ†è§£
a  = tf.constant([[1.0,2.0],[3.0,4.0]],dtype = tf.float32)
v,s,d = tf.linalg.svd(a)
tf.matmul(tf.matmul(s,tf.linalg.diag(v)),d)

#åˆ©ç”¨svdåˆ†è§£å¯ä»¥åœ¨TensorFlowä¸­å®ç°ä¸»æˆåˆ†åˆ†æé™ç»´

```

```
<tf.Tensor: shape=(2, 2), dtype=float32, numpy=
array([[0.9999996, 1.9999996],
       [2.9999998, 4.       ]], dtype=float32)>
```

```python

```

```python

```

### å››ï¼Œå¹¿æ’­æœºåˆ¶


TensorFlowçš„å¹¿æ’­è§„åˆ™å’Œnumpyæ˜¯ä¸€æ ·çš„:

* 1ã€å¦‚æœå¼ é‡çš„ç»´åº¦ä¸åŒï¼Œå°†ç»´åº¦è¾ƒå°çš„å¼ é‡è¿›è¡Œæ‰©å±•ï¼Œç›´åˆ°ä¸¤ä¸ªå¼ é‡çš„ç»´åº¦éƒ½ä¸€æ ·ã€‚
* 2ã€å¦‚æœä¸¤ä¸ªå¼ é‡åœ¨æŸä¸ªç»´åº¦ä¸Šçš„é•¿åº¦æ˜¯ç›¸åŒçš„ï¼Œæˆ–è€…å…¶ä¸­ä¸€ä¸ªå¼ é‡åœ¨è¯¥ç»´åº¦ä¸Šçš„é•¿åº¦ä¸º1ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±è¯´è¿™ä¸¤ä¸ªå¼ é‡åœ¨è¯¥ç»´åº¦ä¸Šæ˜¯ç›¸å®¹çš„ã€‚
* 3ã€å¦‚æœä¸¤ä¸ªå¼ é‡åœ¨æ‰€æœ‰ç»´åº¦ä¸Šéƒ½æ˜¯ç›¸å®¹çš„ï¼Œå®ƒä»¬å°±èƒ½ä½¿ç”¨å¹¿æ’­ã€‚
* 4ã€å¹¿æ’­ä¹‹åï¼Œæ¯ä¸ªç»´åº¦çš„é•¿åº¦å°†å–ä¸¤ä¸ªå¼ é‡åœ¨è¯¥ç»´åº¦é•¿åº¦çš„è¾ƒå¤§å€¼ã€‚
* 5ã€åœ¨ä»»ä½•ä¸€ä¸ªç»´åº¦ä¸Šï¼Œå¦‚æœä¸€ä¸ªå¼ é‡çš„é•¿åº¦ä¸º1ï¼Œå¦ä¸€ä¸ªå¼ é‡é•¿åº¦å¤§äº1ï¼Œé‚£ä¹ˆåœ¨è¯¥ç»´åº¦ä¸Šï¼Œå°±å¥½åƒæ˜¯å¯¹ç¬¬ä¸€ä¸ªå¼ é‡è¿›è¡Œäº†å¤åˆ¶ã€‚

tf.broadcast_to ä»¥æ˜¾å¼çš„æ–¹å¼æŒ‰ç…§å¹¿æ’­æœºåˆ¶æ‰©å±•å¼ é‡çš„ç»´åº¦ã€‚

```python
a = tf.constant([1,2,3])
b = tf.constant([[0,0,0],[1,1,1],[2,2,2]])
b + a  #ç­‰ä»·äº b + tf.broadcast_to(a,b.shape)
```

```
<tf.Tensor: shape=(3, 3), dtype=int32, numpy=
array([[1, 2, 3],
       [2, 3, 4],
       [3, 4, 5]], dtype=int32)>
```

```python
tf.broadcast_to(a,b.shape)
```

```
<tf.Tensor: shape=(3, 3), dtype=int32, numpy=
array([[1, 2, 3],
       [1, 2, 3],
       [1, 2, 3]], dtype=int32)>
```

```python
#è®¡ç®—å¹¿æ’­åè®¡ç®—ç»“æœçš„å½¢çŠ¶ï¼Œé™æ€å½¢çŠ¶ï¼ŒTensorShapeç±»å‹å‚æ•°
tf.broadcast_static_shape(a.shape,b.shape)
```

```
TensorShape([3, 3])
```

```python
#è®¡ç®—å¹¿æ’­åè®¡ç®—ç»“æœçš„å½¢çŠ¶ï¼ŒåŠ¨æ€å½¢çŠ¶ï¼ŒTensorç±»å‹å‚æ•°
c = tf.constant([1,2,3])
d = tf.constant([[1],[2],[3]])
tf.broadcast_dynamic_shape(tf.shape(c),tf.shape(d))
```

```
<tf.Tensor: shape=(2,), dtype=int32, numpy=array([3, 3], dtype=int32)>
```

```python
#å¹¿æ’­æ•ˆæœ
c+d #ç­‰ä»·äº tf.broadcast_to(c,[3,3]) + tf.broadcast_to(d,[3,3])
```

```
<tf.Tensor: shape=(2, 2), dtype=float32, numpy=
array([[6.5760484, 7.8174157],
       [6.8174157, 6.4239516]], dtype=float32)>
```

```python

```

å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·"Pythonä¸ç®—æ³•ä¹‹ç¾"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚

![image.png](./data/Pythonä¸ç®—æ³•ä¹‹ç¾logo.jpg)

```python

```
# 4-3,AutoGraphçš„ä½¿ç”¨è§„èŒƒ

æœ‰ä¸‰ç§è®¡ç®—å›¾çš„æ„å»ºæ–¹å¼ï¼šé™æ€è®¡ç®—å›¾ï¼ŒåŠ¨æ€è®¡ç®—å›¾ï¼Œä»¥åŠAutographã€‚

TensorFlow 2.0ä¸»è¦ä½¿ç”¨çš„æ˜¯åŠ¨æ€è®¡ç®—å›¾å’ŒAutographã€‚

åŠ¨æ€è®¡ç®—å›¾æ˜“äºè°ƒè¯•ï¼Œç¼–ç æ•ˆç‡è¾ƒé«˜ï¼Œä½†æ‰§è¡Œæ•ˆç‡åä½ã€‚

é™æ€è®¡ç®—å›¾æ‰§è¡Œæ•ˆç‡å¾ˆé«˜ï¼Œä½†è¾ƒéš¾è°ƒè¯•ã€‚

è€ŒAutographæœºåˆ¶å¯ä»¥å°†åŠ¨æ€å›¾è½¬æ¢æˆé™æ€è®¡ç®—å›¾ï¼Œå…¼æ”¶æ‰§è¡Œæ•ˆç‡å’Œç¼–ç æ•ˆç‡ä¹‹åˆ©ã€‚

å½“ç„¶Autographæœºåˆ¶èƒ½å¤Ÿè½¬æ¢çš„ä»£ç å¹¶ä¸æ˜¯æ²¡æœ‰ä»»ä½•çº¦æŸçš„ï¼Œæœ‰ä¸€äº›ç¼–ç è§„èŒƒéœ€è¦éµå¾ªï¼Œå¦åˆ™å¯èƒ½ä¼šè½¬æ¢å¤±è´¥æˆ–è€…ä¸ç¬¦åˆé¢„æœŸã€‚

æˆ‘ä»¬å°†ç€é‡ä»‹ç»Autographçš„ç¼–ç è§„èŒƒå’ŒAutographè½¬æ¢æˆé™æ€å›¾çš„åŸç†ã€‚

å¹¶ä»‹ç»ä½¿ç”¨tf.Moduleæ¥æ›´å¥½åœ°æ„å»ºAutographã€‚

æœ¬ç¯‡æˆ‘ä»¬ä»‹ç»ä½¿ç”¨Autographçš„ç¼–ç è§„èŒƒã€‚


### ä¸€ï¼ŒAutographç¼–ç è§„èŒƒæ€»ç»“


* 1ï¼Œè¢«@tf.functionä¿®é¥°çš„å‡½æ•°åº”å°½å¯èƒ½ä½¿ç”¨TensorFlowä¸­çš„å‡½æ•°è€Œä¸æ˜¯Pythonä¸­çš„å…¶ä»–å‡½æ•°ã€‚ä¾‹å¦‚ä½¿ç”¨tf.printè€Œä¸æ˜¯printï¼Œä½¿ç”¨tf.rangeè€Œä¸æ˜¯rangeï¼Œä½¿ç”¨tf.constant(True)è€Œä¸æ˜¯True.

* 2ï¼Œé¿å…åœ¨@tf.functionä¿®é¥°çš„å‡½æ•°å†…éƒ¨å®šä¹‰tf.Variable. 

* 3ï¼Œè¢«@tf.functionä¿®é¥°çš„å‡½æ•°ä¸å¯ä¿®æ”¹è¯¥å‡½æ•°å¤–éƒ¨çš„Pythonåˆ—è¡¨æˆ–å­—å…¸ç­‰æ•°æ®ç»“æ„å˜é‡ã€‚


```python

```

### äºŒï¼ŒAutographç¼–ç è§„èŒƒè§£æ


 **1ï¼Œè¢«@tf.functionä¿®é¥°çš„å‡½æ•°åº”å°½é‡ä½¿ç”¨TensorFlowä¸­çš„å‡½æ•°è€Œä¸æ˜¯Pythonä¸­çš„å…¶ä»–å‡½æ•°ã€‚**

```python
import numpy as np
import tensorflow as tf

@tf.function
def np_random():
    a = np.random.randn(3,3)
    tf.print(a)
    
@tf.function
def tf_random():
    a = tf.random.normal((3,3))
    tf.print(a)
```

```python
#np_randomæ¯æ¬¡æ‰§è¡Œéƒ½æ˜¯ä¸€æ ·çš„ç»“æœã€‚
np_random()
np_random()
```

```
array([[ 0.22619201, -0.4550123 , -0.42587565],
       [ 0.05429906,  0.2312667 , -1.44819738],
       [ 0.36571796,  1.45578986, -1.05348983]])
array([[ 0.22619201, -0.4550123 , -0.42587565],
       [ 0.05429906,  0.2312667 , -1.44819738],
       [ 0.36571796,  1.45578986, -1.05348983]])
```

```python
#tf_randomæ¯æ¬¡æ‰§è¡Œéƒ½ä¼šæœ‰é‡æ–°ç”Ÿæˆéšæœºæ•°ã€‚
tf_random()
tf_random()
```

```
[[-1.38956189 -0.394843668 0.420657277]
 [2.87235498 -1.33740318 -0.533843279]
 [0.918233037 0.118598573 -0.399486482]]
[[-0.858178258 1.67509317 0.511889517]
 [-0.545829177 -2.20118237 -0.968222201]
 [0.733958483 -0.61904633 0.77440238]]
```

```python

```

**2ï¼Œé¿å…åœ¨@tf.functionä¿®é¥°çš„å‡½æ•°å†…éƒ¨å®šä¹‰tf.Variable.**

```python
# é¿å…åœ¨@tf.functionä¿®é¥°çš„å‡½æ•°å†…éƒ¨å®šä¹‰tf.Variable.

x = tf.Variable(1.0,dtype=tf.float32)
@tf.function
def outer_var():
    x.assign_add(1.0)
    tf.print(x)
    return(x)

outer_var() 
outer_var()

```

```python
@tf.function
def inner_var():
    x = tf.Variable(1.0,dtype = tf.float32)
    x.assign_add(1.0)
    tf.print(x)
    return(x)

#æ‰§è¡Œå°†æŠ¥é”™
#inner_var()
#inner_var()

```

```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-12-c95a7c3c1ddd> in <module>
      7 
      8 #æ‰§è¡Œå°†æŠ¥é”™
----> 9 inner_var()
     10 inner_var()

~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py in __call__(self, *args, **kwds)
    566         xla_context.Exit()
    567     else:
--> 568       result = self._call(*args, **kwds)
    569 
    570     if tracing_count == self._get_tracing_count():
......
ValueError: tf.function-decorated function tried to create variables on non-first call.
```


**3,è¢«@tf.functionä¿®é¥°çš„å‡½æ•°ä¸å¯ä¿®æ”¹è¯¥å‡½æ•°å¤–éƒ¨çš„Pythonåˆ—è¡¨æˆ–å­—å…¸ç­‰ç»“æ„ç±»å‹å˜é‡ã€‚**

```python
tensor_list = []

#@tf.function #åŠ ä¸Šè¿™ä¸€è¡Œåˆ‡æ¢æˆAutographç»“æœå°†ä¸ç¬¦åˆé¢„æœŸï¼ï¼ï¼
def append_tensor(x):
    tensor_list.append(x)
    return tensor_list

append_tensor(tf.constant(5.0))
append_tensor(tf.constant(6.0))
print(tensor_list)

```

```
[<tf.Tensor: shape=(), dtype=float32, numpy=5.0>, <tf.Tensor: shape=(), dtype=float32, numpy=6.0>]
```

```python
tensor_list = []

@tf.function #åŠ ä¸Šè¿™ä¸€è¡Œåˆ‡æ¢æˆAutographç»“æœå°†ä¸ç¬¦åˆé¢„æœŸï¼ï¼ï¼
def append_tensor(x):
    tensor_list.append(x)
    return tensor_list


append_tensor(tf.constant(5.0))
append_tensor(tf.constant(6.0))
print(tensor_list)

```

```
[<tf.Tensor 'x:0' shape=() dtype=float32>]
```

```python

```

å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·"Pythonä¸ç®—æ³•ä¹‹ç¾"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚

![image.png](./data/Pythonä¸ç®—æ³•ä¹‹ç¾logo.jpg)



# 4-4,AutoGraphçš„æœºåˆ¶åŸç†

æœ‰ä¸‰ç§è®¡ç®—å›¾çš„æ„å»ºæ–¹å¼ï¼šé™æ€è®¡ç®—å›¾ï¼ŒåŠ¨æ€è®¡ç®—å›¾ï¼Œä»¥åŠAutographã€‚

TensorFlow 2.0ä¸»è¦ä½¿ç”¨çš„æ˜¯åŠ¨æ€è®¡ç®—å›¾å’ŒAutographã€‚

åŠ¨æ€è®¡ç®—å›¾æ˜“äºè°ƒè¯•ï¼Œç¼–ç æ•ˆç‡è¾ƒé«˜ï¼Œä½†æ‰§è¡Œæ•ˆç‡åä½ã€‚

é™æ€è®¡ç®—å›¾æ‰§è¡Œæ•ˆç‡å¾ˆé«˜ï¼Œä½†è¾ƒéš¾è°ƒè¯•ã€‚

è€ŒAutographæœºåˆ¶å¯ä»¥å°†åŠ¨æ€å›¾è½¬æ¢æˆé™æ€è®¡ç®—å›¾ï¼Œå…¼æ”¶æ‰§è¡Œæ•ˆç‡å’Œç¼–ç æ•ˆç‡ä¹‹åˆ©ã€‚

å½“ç„¶Autographæœºåˆ¶èƒ½å¤Ÿè½¬æ¢çš„ä»£ç å¹¶ä¸æ˜¯æ²¡æœ‰ä»»ä½•çº¦æŸçš„ï¼Œæœ‰ä¸€äº›ç¼–ç è§„èŒƒéœ€è¦éµå¾ªï¼Œå¦åˆ™å¯èƒ½ä¼šè½¬æ¢å¤±è´¥æˆ–è€…ä¸ç¬¦åˆé¢„æœŸã€‚

æˆ‘ä»¬ä¼šä»‹ç»Autographçš„ç¼–ç è§„èŒƒå’ŒAutographè½¬æ¢æˆé™æ€å›¾çš„åŸç†ã€‚

å¹¶ä»‹ç»ä½¿ç”¨tf.Moduleæ¥æ›´å¥½åœ°æ„å»ºAutographã€‚

ä¸Šç¯‡æˆ‘ä»¬ä»‹ç»äº†Autographçš„ç¼–ç è§„èŒƒï¼Œæœ¬ç¯‡æˆ‘ä»¬ä»‹ç»Autographçš„æœºåˆ¶åŸç†ã€‚



### ä¸€ï¼ŒAutographçš„æœºåˆ¶åŸç†


**å½“æˆ‘ä»¬ä½¿ç”¨@tf.functionè£…é¥°ä¸€ä¸ªå‡½æ•°çš„æ—¶å€™ï¼Œåé¢åˆ°åº•å‘ç”Ÿäº†ä»€ä¹ˆå‘¢ï¼Ÿ**

ä¾‹å¦‚æˆ‘ä»¬å†™ä¸‹å¦‚ä¸‹ä»£ç ã€‚

```python
import tensorflow as tf
import numpy as np 

@tf.function(autograph=True)
def myadd(a,b):
    for i in tf.range(3):
        tf.print(i)
    c = a+b
    print("tracing")
    return c
```

åé¢ä»€ä¹ˆéƒ½æ²¡æœ‰å‘ç”Ÿã€‚ä»…ä»…æ˜¯åœ¨Pythonå †æ ˆä¸­è®°å½•äº†è¿™æ ·ä¸€ä¸ªå‡½æ•°çš„ç­¾åã€‚

**å½“æˆ‘ä»¬ç¬¬ä¸€æ¬¡è°ƒç”¨è¿™ä¸ªè¢«@tf.functionè£…é¥°çš„å‡½æ•°æ—¶ï¼Œåé¢åˆ°åº•å‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿ**

ä¾‹å¦‚æˆ‘ä»¬å†™ä¸‹å¦‚ä¸‹ä»£ç ã€‚

```python
myadd(tf.constant("hello"),tf.constant("world"))
```

```
tracing
0
1
2
```


å‘ç”Ÿäº†2ä»¶äº‹æƒ…ï¼Œ

ç¬¬ä¸€ä»¶äº‹æƒ…æ˜¯åˆ›å»ºè®¡ç®—å›¾ã€‚

å³åˆ›å»ºä¸€ä¸ªé™æ€è®¡ç®—å›¾ï¼Œè·Ÿè¸ªæ‰§è¡Œä¸€éå‡½æ•°ä½“ä¸­çš„Pythonä»£ç ï¼Œç¡®å®šå„ä¸ªå˜é‡çš„Tensorç±»å‹ï¼Œå¹¶æ ¹æ®æ‰§è¡Œé¡ºåºå°†ç®—å­æ·»åŠ åˆ°è®¡ç®—å›¾ä¸­ã€‚
åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œå¦‚æœå¼€å¯äº†autograph=True(é»˜è®¤å¼€å¯),ä¼šå°†Pythonæ§åˆ¶æµè½¬æ¢æˆTensorFlowå›¾å†…æ§åˆ¶æµã€‚
ä¸»è¦æ˜¯å°†ifè¯­å¥è½¬æ¢æˆ tf.condç®—å­è¡¨è¾¾ï¼Œå°†whileå’Œforå¾ªç¯è¯­å¥è½¬æ¢æˆtf.while_loopç®—å­è¡¨è¾¾ï¼Œå¹¶åœ¨å¿…è¦çš„æ—¶å€™æ·»åŠ 
tf.control_dependenciesæŒ‡å®šæ‰§è¡Œé¡ºåºä¾èµ–å…³ç³»ã€‚

ç›¸å½“äºåœ¨ tensorflow1.0æ‰§è¡Œäº†ç±»ä¼¼ä¸‹é¢çš„è¯­å¥ï¼š

```python
g = tf.Graph()
with g.as_default():
    a = tf.placeholder(shape=[],dtype=tf.string)
    b = tf.placeholder(shape=[],dtype=tf.string)
    cond = lambda i: i<tf.constant(3)
    def body(i):
        tf.print(i)
        return(i+1)
    loop = tf.while_loop(cond,body,loop_vars=[0])
    loop
    with tf.control_dependencies(loop):
        c = tf.strings.join([a,b])
    print("tracing")
```

ç¬¬äºŒä»¶äº‹æƒ…æ˜¯æ‰§è¡Œè®¡ç®—å›¾ã€‚

ç›¸å½“äºåœ¨ tensorflow1.0ä¸­æ‰§è¡Œäº†ä¸‹é¢çš„è¯­å¥ï¼š

```python
with tf.Session(graph=g) as sess:
    sess.run(c,feed_dict={a:tf.constant("hello"),b:tf.constant("world")})
```

å› æ­¤æˆ‘ä»¬å…ˆçœ‹åˆ°çš„æ˜¯ç¬¬ä¸€ä¸ªæ­¥éª¤çš„ç»“æœï¼šå³Pythonè°ƒç”¨æ ‡å‡†è¾“å‡ºæµæ‰“å°"tracing"è¯­å¥ã€‚

ç„¶åçœ‹åˆ°ç¬¬äºŒä¸ªæ­¥éª¤çš„ç»“æœï¼šTensorFlowè°ƒç”¨æ ‡å‡†è¾“å‡ºæµæ‰“å°1,2,3ã€‚



**å½“æˆ‘ä»¬å†æ¬¡ç”¨ç›¸åŒçš„è¾“å…¥å‚æ•°ç±»å‹è°ƒç”¨è¿™ä¸ªè¢«@tf.functionè£…é¥°çš„å‡½æ•°æ—¶ï¼Œåé¢åˆ°åº•å‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿ**

ä¾‹å¦‚æˆ‘ä»¬å†™ä¸‹å¦‚ä¸‹ä»£ç ã€‚

```python
myadd(tf.constant("good"),tf.constant("morning"))
```

```
0
1
2
```


åªä¼šå‘ç”Ÿä¸€ä»¶äº‹æƒ…ï¼Œé‚£å°±æ˜¯ä¸Šé¢æ­¥éª¤çš„ç¬¬äºŒæ­¥ï¼Œæ‰§è¡Œè®¡ç®—å›¾ã€‚

æ‰€ä»¥è¿™ä¸€æ¬¡æˆ‘ä»¬æ²¡æœ‰çœ‹åˆ°æ‰“å°"tracing"çš„ç»“æœã€‚


**å½“æˆ‘ä»¬å†æ¬¡ç”¨ä¸åŒçš„çš„è¾“å…¥å‚æ•°ç±»å‹è°ƒç”¨è¿™ä¸ªè¢«@tf.functionè£…é¥°çš„å‡½æ•°æ—¶ï¼Œåé¢åˆ°åº•å‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿ**

ä¾‹å¦‚æˆ‘ä»¬å†™ä¸‹å¦‚ä¸‹ä»£ç ã€‚

```python
myadd(tf.constant(1),tf.constant(2))
```

```
tracing
0
1
2
```


ç”±äºè¾“å…¥å‚æ•°çš„ç±»å‹å·²ç»å‘ç”Ÿå˜åŒ–ï¼Œå·²ç»åˆ›å»ºçš„è®¡ç®—å›¾ä¸èƒ½å¤Ÿå†æ¬¡ä½¿ç”¨ã€‚

éœ€è¦é‡æ–°åš2ä»¶äº‹æƒ…ï¼šåˆ›å»ºæ–°çš„è®¡ç®—å›¾ã€æ‰§è¡Œè®¡ç®—å›¾ã€‚

æ‰€ä»¥æˆ‘ä»¬åˆä¼šå…ˆçœ‹åˆ°çš„æ˜¯ç¬¬ä¸€ä¸ªæ­¥éª¤çš„ç»“æœï¼šå³Pythonè°ƒç”¨æ ‡å‡†è¾“å‡ºæµæ‰“å°"tracing"è¯­å¥ã€‚

ç„¶åå†çœ‹åˆ°ç¬¬äºŒä¸ªæ­¥éª¤çš„ç»“æœï¼šTensorFlowè°ƒç”¨æ ‡å‡†è¾“å‡ºæµæ‰“å°1,2,3ã€‚


**éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå¦‚æœè°ƒç”¨è¢«@tf.functionè£…é¥°çš„å‡½æ•°æ—¶è¾“å…¥çš„å‚æ•°ä¸æ˜¯Tensorç±»å‹ï¼Œåˆ™æ¯æ¬¡éƒ½ä¼šé‡æ–°åˆ›å»ºè®¡ç®—å›¾ã€‚**

ä¾‹å¦‚æˆ‘ä»¬å†™ä¸‹å¦‚ä¸‹ä»£ç ã€‚ä¸¤æ¬¡éƒ½ä¼šé‡æ–°åˆ›å»ºè®¡ç®—å›¾ã€‚å› æ­¤ï¼Œä¸€èˆ¬å»ºè®®è°ƒç”¨@tf.functionæ—¶åº”ä¼ å…¥Tensorç±»å‹ã€‚

```python
myadd("hello","world")
myadd("good","morning")
```

```
tracing
0
1
2
tracing
0
1
2
```

```python

```

```python

```

### äºŒï¼Œé‡æ–°ç†è§£Autographçš„ç¼–ç è§„èŒƒ


äº†è§£äº†ä»¥ä¸ŠAutographçš„æœºåˆ¶åŸç†ï¼Œæˆ‘ä»¬ä¹Ÿå°±èƒ½å¤Ÿç†è§£Autographç¼–ç è§„èŒƒçš„3æ¡å»ºè®®äº†ã€‚

1ï¼Œè¢«@tf.functionä¿®é¥°çš„å‡½æ•°åº”å°½é‡ä½¿ç”¨TensorFlowä¸­çš„å‡½æ•°è€Œä¸æ˜¯Pythonä¸­çš„å…¶ä»–å‡½æ•°ã€‚ä¾‹å¦‚ä½¿ç”¨tf.printè€Œä¸æ˜¯print.

è§£é‡Šï¼šPythonä¸­çš„å‡½æ•°ä»…ä»…ä¼šåœ¨è·Ÿè¸ªæ‰§è¡Œå‡½æ•°ä»¥åˆ›å»ºé™æ€å›¾çš„é˜¶æ®µä½¿ç”¨ï¼Œæ™®é€šPythonå‡½æ•°æ˜¯æ— æ³•åµŒå…¥åˆ°é™æ€è®¡ç®—å›¾ä¸­çš„ï¼Œæ‰€ä»¥
åœ¨è®¡ç®—å›¾æ„å»ºå¥½ä¹‹åå†æ¬¡è°ƒç”¨çš„æ—¶å€™ï¼Œè¿™äº›Pythonå‡½æ•°å¹¶æ²¡æœ‰è¢«è®¡ç®—ï¼Œè€ŒTensorFlowä¸­çš„å‡½æ•°åˆ™å¯ä»¥åµŒå…¥åˆ°è®¡ç®—å›¾ä¸­ã€‚ä½¿ç”¨æ™®é€šçš„Pythonå‡½æ•°ä¼šå¯¼è‡´
è¢«@tf.functionä¿®é¥°å‰ã€eageræ‰§è¡Œã€‘å’Œè¢«@tf.functionä¿®é¥°åã€é™æ€å›¾æ‰§è¡Œã€‘çš„è¾“å‡ºä¸ä¸€è‡´ã€‚

2ï¼Œé¿å…åœ¨@tf.functionä¿®é¥°çš„å‡½æ•°å†…éƒ¨å®šä¹‰tf.Variable. 

è§£é‡Šï¼šå¦‚æœå‡½æ•°å†…éƒ¨å®šä¹‰äº†tf.Variable,é‚£ä¹ˆåœ¨ã€eageræ‰§è¡Œã€‘æ—¶ï¼Œè¿™ç§åˆ›å»ºtf.Variableçš„è¡Œä¸ºåœ¨æ¯æ¬¡å‡½æ•°è°ƒç”¨æ—¶å€™éƒ½ä¼šå‘ç”Ÿã€‚ä½†æ˜¯åœ¨ã€é™æ€å›¾æ‰§è¡Œã€‘æ—¶ï¼Œè¿™ç§åˆ›å»ºtf.Variableçš„è¡Œä¸ºåªä¼šå‘ç”Ÿåœ¨ç¬¬ä¸€æ­¥è·Ÿè¸ªPythonä»£ç é€»è¾‘åˆ›å»ºè®¡ç®—å›¾æ—¶ï¼Œè¿™ä¼šå¯¼è‡´è¢«@tf.functionä¿®é¥°å‰ã€eageræ‰§è¡Œã€‘å’Œè¢«@tf.functionä¿®é¥°åã€é™æ€å›¾æ‰§è¡Œã€‘çš„è¾“å‡ºä¸ä¸€è‡´ã€‚å®é™…ä¸Šï¼ŒTensorFlowåœ¨è¿™ç§æƒ…å†µä¸‹ä¸€èˆ¬ä¼šæŠ¥é”™ã€‚

3ï¼Œè¢«@tf.functionä¿®é¥°çš„å‡½æ•°ä¸å¯ä¿®æ”¹è¯¥å‡½æ•°å¤–éƒ¨çš„Pythonåˆ—è¡¨æˆ–å­—å…¸ç­‰æ•°æ®ç»“æ„å˜é‡ã€‚

è§£é‡Šï¼šé™æ€è®¡ç®—å›¾æ˜¯è¢«ç¼–è¯‘æˆC++ä»£ç åœ¨TensorFlowå†…æ ¸ä¸­æ‰§è¡Œçš„ã€‚Pythonä¸­çš„åˆ—è¡¨å’Œå­—å…¸ç­‰æ•°æ®ç»“æ„å˜é‡æ˜¯æ— æ³•åµŒå…¥åˆ°è®¡ç®—å›¾ä¸­ï¼Œå®ƒä»¬ä»…ä»…èƒ½å¤Ÿåœ¨åˆ›å»ºè®¡ç®—å›¾æ—¶è¢«è¯»å–ï¼Œåœ¨æ‰§è¡Œè®¡ç®—å›¾æ—¶æ˜¯æ— æ³•ä¿®æ”¹Pythonä¸­çš„åˆ—è¡¨æˆ–å­—å…¸è¿™æ ·çš„æ•°æ®ç»“æ„å˜é‡çš„ã€‚


```python

```

å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·"Pythonä¸ç®—æ³•ä¹‹ç¾"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚

![image.png](./data/Pythonä¸ç®—æ³•ä¹‹ç¾logo.jpg)

```python

```
# 4-5,AutoGraphå’Œtf.Module


æœ‰ä¸‰ç§è®¡ç®—å›¾çš„æ„å»ºæ–¹å¼ï¼šé™æ€è®¡ç®—å›¾ï¼ŒåŠ¨æ€è®¡ç®—å›¾ï¼Œä»¥åŠAutographã€‚

TensorFlow 2.0ä¸»è¦ä½¿ç”¨çš„æ˜¯åŠ¨æ€è®¡ç®—å›¾å’ŒAutographã€‚

åŠ¨æ€è®¡ç®—å›¾æ˜“äºè°ƒè¯•ï¼Œç¼–ç æ•ˆç‡è¾ƒé«˜ï¼Œä½†æ‰§è¡Œæ•ˆç‡åä½ã€‚

é™æ€è®¡ç®—å›¾æ‰§è¡Œæ•ˆç‡å¾ˆé«˜ï¼Œä½†è¾ƒéš¾è°ƒè¯•ã€‚

è€ŒAutographæœºåˆ¶å¯ä»¥å°†åŠ¨æ€å›¾è½¬æ¢æˆé™æ€è®¡ç®—å›¾ï¼Œå…¼æ”¶æ‰§è¡Œæ•ˆç‡å’Œç¼–ç æ•ˆç‡ä¹‹åˆ©ã€‚

å½“ç„¶Autographæœºåˆ¶èƒ½å¤Ÿè½¬æ¢çš„ä»£ç å¹¶ä¸æ˜¯æ²¡æœ‰ä»»ä½•çº¦æŸçš„ï¼Œæœ‰ä¸€äº›ç¼–ç è§„èŒƒéœ€è¦éµå¾ªï¼Œå¦åˆ™å¯èƒ½ä¼šè½¬æ¢å¤±è´¥æˆ–è€…ä¸ç¬¦åˆé¢„æœŸã€‚

å‰é¢æˆ‘ä»¬ä»‹ç»äº†Autographçš„ç¼–ç è§„èŒƒå’ŒAutographè½¬æ¢æˆé™æ€å›¾çš„åŸç†ã€‚

æœ¬ç¯‡æˆ‘ä»¬ä»‹ç»ä½¿ç”¨tf.Moduleæ¥æ›´å¥½åœ°æ„å»ºAutographã€‚




### ä¸€ï¼ŒAutographå’Œtf.Moduleæ¦‚è¿°


å‰é¢åœ¨ä»‹ç»Autographçš„ç¼–ç è§„èŒƒæ—¶æåˆ°æ„å»ºAutographæ—¶åº”è¯¥é¿å…åœ¨@tf.functionä¿®é¥°çš„å‡½æ•°å†…éƒ¨å®šä¹‰tf.Variable. 

ä½†æ˜¯å¦‚æœåœ¨å‡½æ•°å¤–éƒ¨å®šä¹‰tf.Variableçš„è¯ï¼Œåˆä¼šæ˜¾å¾—è¿™ä¸ªå‡½æ•°æœ‰å¤–éƒ¨å˜é‡ä¾èµ–ï¼Œå°è£…ä¸å¤Ÿå®Œç¾ã€‚

ä¸€ç§ç®€å•çš„æ€è·¯æ˜¯å®šä¹‰ä¸€ä¸ªç±»ï¼Œå¹¶å°†ç›¸å…³çš„tf.Variableåˆ›å»ºæ”¾åœ¨ç±»çš„åˆå§‹åŒ–æ–¹æ³•ä¸­ã€‚è€Œå°†å‡½æ•°çš„é€»è¾‘æ”¾åœ¨å…¶ä»–æ–¹æ³•ä¸­ã€‚

è¿™æ ·ä¸€é¡¿çŒ›å¦‚è™çš„æ“ä½œä¹‹åï¼Œæˆ‘ä»¬ä¼šè§‰å¾—ä¸€åˆ‡éƒ½å¦‚åŒäººæ³•åœ°åœ°æ³•å¤©å¤©æ³•é“é“æ³•è‡ªç„¶èˆ¬çš„è‡ªç„¶ã€‚

æƒŠå–œçš„æ˜¯ï¼ŒTensorFlowæä¾›äº†ä¸€ä¸ªåŸºç±»tf.Moduleï¼Œé€šè¿‡ç»§æ‰¿å®ƒæ„å»ºå­ç±»ï¼Œæˆ‘ä»¬ä¸ä»…å¯ä»¥è·å¾—ä»¥ä¸Šçš„è‡ªç„¶è€Œç„¶ï¼Œè€Œä¸”å¯ä»¥éå¸¸æ–¹ä¾¿åœ°ç®¡ç†å˜é‡ï¼Œè¿˜å¯ä»¥éå¸¸æ–¹ä¾¿åœ°ç®¡ç†å®ƒå¼•ç”¨çš„å…¶å®ƒModuleï¼Œæœ€é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬èƒ½å¤Ÿåˆ©ç”¨tf.saved_modelä¿å­˜æ¨¡å‹å¹¶å®ç°è·¨å¹³å°éƒ¨ç½²ä½¿ç”¨ã€‚

å®é™…ä¸Šï¼Œtf.keras.models.Model,tf.keras.layers.Layer éƒ½æ˜¯ç»§æ‰¿è‡ªtf.Moduleçš„ï¼Œæä¾›äº†æ–¹ä¾¿çš„å˜é‡ç®¡ç†å’Œæ‰€å¼•ç”¨çš„å­æ¨¡å—ç®¡ç†çš„åŠŸèƒ½ã€‚

**å› æ­¤ï¼Œåˆ©ç”¨tf.Moduleæä¾›çš„å°è£…ï¼Œå†ç»“åˆTensoFlowä¸°å¯Œçš„ä½é˜¶APIï¼Œå®é™…ä¸Šæˆ‘ä»¬èƒ½å¤ŸåŸºäºTensorFlowå¼€å‘ä»»æ„æœºå™¨å­¦ä¹ æ¨¡å‹(è€Œéä»…ä»…æ˜¯ç¥ç»ç½‘ç»œæ¨¡å‹)ï¼Œå¹¶å®ç°è·¨å¹³å°éƒ¨ç½²ä½¿ç”¨ã€‚**





### äºŒï¼Œåº”ç”¨tf.Moduleå°è£…Autograph


å®šä¹‰ä¸€ä¸ªç®€å•çš„functionã€‚

```python
import tensorflow as tf 
x = tf.Variable(1.0,dtype=tf.float32)

#åœ¨tf.functionä¸­ç”¨input_signatureé™å®šè¾“å…¥å¼ é‡çš„ç­¾åç±»å‹ï¼šshapeå’Œdtype
@tf.function(input_signature=[tf.TensorSpec(shape = [], dtype = tf.float32)])    
def add_print(a):
    x.assign_add(a)
    tf.print(x)
    return(x)
```

```python
add_print(tf.constant(3.0))
#add_print(tf.constant(3)) #è¾“å…¥ä¸ç¬¦åˆå¼ é‡ç­¾åçš„å‚æ•°å°†æŠ¥é”™
```

```
4
```


ä¸‹é¢åˆ©ç”¨tf.Moduleçš„å­ç±»åŒ–å°†å…¶å°è£…ä¸€ä¸‹ã€‚

```python
class DemoModule(tf.Module):
    def __init__(self,init_value = tf.constant(0.0),name=None):
        super(DemoModule, self).__init__(name=name)
        with self.name_scope:  #ç›¸å½“äºwith tf.name_scope("demo_module")
            self.x = tf.Variable(init_value,dtype = tf.float32,trainable=True)

     
    @tf.function(input_signature=[tf.TensorSpec(shape = [], dtype = tf.float32)])  
    def addprint(self,a):
        with self.name_scope:
            self.x.assign_add(a)
            tf.print(self.x)
            return(self.x)

```

```python
#æ‰§è¡Œ
demo = DemoModule(init_value = tf.constant(1.0))
result = demo.addprint(tf.constant(5.0))
```

```
6
```

```python
#æŸ¥çœ‹æ¨¡å—ä¸­çš„å…¨éƒ¨å˜é‡å’Œå…¨éƒ¨å¯è®­ç»ƒå˜é‡
print(demo.variables)
print(demo.trainable_variables)
```

```
(<tf.Variable 'demo_module/Variable:0' shape=() dtype=float32, numpy=6.0>,)
(<tf.Variable 'demo_module/Variable:0' shape=() dtype=float32, numpy=6.0>,)
```

```python
#æŸ¥çœ‹æ¨¡å—ä¸­çš„å…¨éƒ¨å­æ¨¡å—
demo.submodules
```

```python
#ä½¿ç”¨tf.saved_model ä¿å­˜æ¨¡å‹ï¼Œå¹¶æŒ‡å®šéœ€è¦è·¨å¹³å°éƒ¨ç½²çš„æ–¹æ³•
tf.saved_model.save(demo,"./data/demo/1",signatures = {"serving_default":demo.addprint})
```

```python
#åŠ è½½æ¨¡å‹
demo2 = tf.saved_model.load("./data/demo/1")
demo2.addprint(tf.constant(5.0))
```

```
11
```

```python
# æŸ¥çœ‹æ¨¡å‹æ–‡ä»¶ç›¸å…³ä¿¡æ¯ï¼Œçº¢æ¡†æ ‡å‡ºæ¥çš„è¾“å‡ºä¿¡æ¯åœ¨æ¨¡å‹éƒ¨ç½²å’Œè·¨å¹³å°ä½¿ç”¨æ—¶æœ‰å¯èƒ½ä¼šç”¨åˆ°
!saved_model_cli show --dir ./data/demo/1 --all
```

![](./data/æŸ¥çœ‹æ¨¡å‹æ–‡ä»¶ä¿¡æ¯.jpg)

```python

```

åœ¨tensorboardä¸­æŸ¥çœ‹è®¡ç®—å›¾ï¼Œæ¨¡å—ä¼šè¢«æ·»åŠ æ¨¡å—ådemo_module,æ–¹ä¾¿å±‚æ¬¡åŒ–å‘ˆç°è®¡ç®—å›¾ç»“æ„ã€‚

```python
import datetime

# åˆ›å»ºæ—¥å¿—
stamp = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
logdir = './data/demomodule/%s' % stamp
writer = tf.summary.create_file_writer(logdir)

#å¼€å¯autographè·Ÿè¸ª
tf.summary.trace_on(graph=True, profiler=True) 

#æ‰§è¡Œautograph
demo = DemoModule(init_value = tf.constant(0.0))
result = demo.addprint(tf.constant(5.0))

#å°†è®¡ç®—å›¾ä¿¡æ¯å†™å…¥æ—¥å¿—
with writer.as_default():
    tf.summary.trace_export(
        name="demomodule",
        step=0,
        profiler_outdir=logdir)
    
```

```python

```

```python
#å¯åŠ¨ tensorboardåœ¨jupyterä¸­çš„é­”æ³•å‘½ä»¤
%reload_ext tensorboard
```

```python
from tensorboard import notebook
notebook.list() 
```

```python
notebook.start("--logdir ./data/demomodule/")
```

![](./data/demomoduleçš„è®¡ç®—å›¾ç»“æ„.jpg)

```python

```

é™¤äº†åˆ©ç”¨tf.Moduleçš„å­ç±»åŒ–å®ç°å°è£…ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥é€šè¿‡ç»™tf.Moduleæ·»åŠ å±æ€§çš„æ–¹æ³•è¿›è¡Œå°è£…ã€‚

```python
mymodule = tf.Module()
mymodule.x = tf.Variable(0.0)

@tf.function(input_signature=[tf.TensorSpec(shape = [], dtype = tf.float32)])  
def addprint(a):
    mymodule.x.assign_add(a)
    tf.print(mymodule.x)
    return (mymodule.x)

mymodule.addprint = addprint
```

```python
mymodule.addprint(tf.constant(1.0)).numpy()
```

```
1.0
```

```python
print(mymodule.variables)
```

```
(<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.0>,)
```

```python
#ä½¿ç”¨tf.saved_model ä¿å­˜æ¨¡å‹
tf.saved_model.save(mymodule,"./data/mymodule",
    signatures = {"serving_default":mymodule.addprint})

#åŠ è½½æ¨¡å‹
mymodule2 = tf.saved_model.load("./data/mymodule")
mymodule2.addprint(tf.constant(5.0))
```

```
INFO:tensorflow:Assets written to: ./data/mymodule/assets
5
```

```python

```

### ä¸‰ï¼Œtf.Moduleå’Œtf.keras.Modelï¼Œtf.keras.layers.Layer


tf.kerasä¸­çš„æ¨¡å‹å’Œå±‚éƒ½æ˜¯ç»§æ‰¿tf.Moduleå®ç°çš„ï¼Œä¹Ÿå…·æœ‰å˜é‡ç®¡ç†å’Œå­æ¨¡å—ç®¡ç†åŠŸèƒ½ã€‚

```python
import tensorflow as tf
from tensorflow.keras import models,layers,losses,metrics
```

```python
print(issubclass(tf.keras.Model,tf.Module))
print(issubclass(tf.keras.layers.Layer,tf.Module))
print(issubclass(tf.keras.Model,tf.keras.layers.Layer))
```

```
True
True
True
```

```python
tf.keras.backend.clear_session() 

model = models.Sequential()

model.add(layers.Dense(4,input_shape = (10,)))
model.add(layers.Dense(2))
model.add(layers.Dense(1))
model.summary()
```

```
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense (Dense)                (None, 4)                 44        
_________________________________________________________________
dense_1 (Dense)              (None, 2)                 10        
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 3         
=================================================================
Total params: 57
Trainable params: 57
Non-trainable params: 0
_________________________________________________________________
```

```python
model.variables
```

```
[<tf.Variable 'dense/kernel:0' shape=(10, 4) dtype=float32, numpy=
 array([[-0.06741005,  0.45534766,  0.5190817 , -0.01806331],
        [-0.14258742, -0.49711505,  0.26030976,  0.18607801],
        [-0.62806034,  0.5327399 ,  0.42206633,  0.29201728],
        [-0.16602087, -0.18901917,  0.55159235, -0.01091868],
        [ 0.04533798,  0.326845  , -0.582667  ,  0.19431782],
        [ 0.6494713 , -0.16174704,  0.4062966 ,  0.48760796],
        [ 0.58400524, -0.6280886 , -0.11265379, -0.6438277 ],
        [ 0.26642334,  0.49275804,  0.20793378, -0.43889117],
        [ 0.4092741 ,  0.09871006, -0.2073121 ,  0.26047975],
        [ 0.43910992,  0.00199282, -0.07711256, -0.27966842]],
       dtype=float32)>,
 <tf.Variable 'dense/bias:0' shape=(4,) dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>,
 <tf.Variable 'dense_1/kernel:0' shape=(4, 2) dtype=float32, numpy=
 array([[ 0.5022683 , -0.0507431 ],
        [-0.61540484,  0.9369011 ],
        [-0.14412141, -0.54607415],
        [ 0.2027781 , -0.4651153 ]], dtype=float32)>,
 <tf.Variable 'dense_1/bias:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)>,
 <tf.Variable 'dense_2/kernel:0' shape=(2, 1) dtype=float32, numpy=
 array([[-0.244825 ],
        [-1.2101456]], dtype=float32)>,
 <tf.Variable 'dense_2/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>]
```

```python
model.layers[0].trainable = False #å†»ç»“ç¬¬0å±‚çš„å˜é‡,ä½¿å…¶ä¸å¯è®­ç»ƒ
model.trainable_variables
```

```
[<tf.Variable 'dense_1/kernel:0' shape=(4, 2) dtype=float32, numpy=
 array([[ 0.5022683 , -0.0507431 ],
        [-0.61540484,  0.9369011 ],
        [-0.14412141, -0.54607415],
        [ 0.2027781 , -0.4651153 ]], dtype=float32)>,
 <tf.Variable 'dense_1/bias:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)>,
 <tf.Variable 'dense_2/kernel:0' shape=(2, 1) dtype=float32, numpy=
 array([[-0.244825 ],
        [-1.2101456]], dtype=float32)>,
 <tf.Variable 'dense_2/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>]
```

```python
model.submodules
```

```
(<tensorflow.python.keras.engine.input_layer.InputLayer at 0x144d8c080>,
 <tensorflow.python.keras.layers.core.Dense at 0x144daada0>,
 <tensorflow.python.keras.layers.core.Dense at 0x144d8c5c0>,
 <tensorflow.python.keras.layers.core.Dense at 0x144d7aa20>)
```

```python
model.layers
```

```
[<tensorflow.python.keras.layers.core.Dense at 0x144daada0>,
 <tensorflow.python.keras.layers.core.Dense at 0x144d8c5c0>,
 <tensorflow.python.keras.layers.core.Dense at 0x144d7aa20>]
```

```python
print(model.name)
print(model.name_scope())
```

```
sequential
sequential
```

```python

```

å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·"Pythonä¸ç®—æ³•ä¹‹ç¾"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚

![image.png](./data/Pythonä¸ç®—æ³•ä¹‹ç¾logo.jpg)



# äº”ã€TensorFlowçš„ä¸­é˜¶API

TensorFlowçš„ä¸­é˜¶APIä¸»è¦åŒ…æ‹¬: 

* æ•°æ®ç®¡é“(tf.data)

* ç‰¹å¾åˆ—(tf.feature_column)

* æ¿€æ´»å‡½æ•°(tf.nn)

* æ¨¡å‹å±‚(tf.keras.layers)

* æŸå¤±å‡½æ•°(tf.keras.losses)

* è¯„ä¼°å‡½æ•°(tf.keras.metrics)

* ä¼˜åŒ–å™¨(tf.keras.optimizers)

* å›è°ƒå‡½æ•°(tf.keras.callbacks)

å¦‚æœæŠŠæ¨¡å‹æ¯”ä½œä¸€ä¸ªæˆ¿å­ï¼Œé‚£ä¹ˆä¸­é˜¶APIå°±æ˜¯ã€æ¨¡å‹ä¹‹å¢™ã€‘ã€‚


å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·"Pythonä¸ç®—æ³•ä¹‹ç¾"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚

![image.png](./data/Pythonä¸ç®—æ³•ä¹‹ç¾logo.jpg)

```python

```
# 5-1,æ•°æ®ç®¡é“Dataset

å¦‚æœéœ€è¦è®­ç»ƒçš„æ•°æ®å¤§å°ä¸å¤§ï¼Œä¾‹å¦‚ä¸åˆ°1Gï¼Œé‚£ä¹ˆå¯ä»¥ç›´æ¥å…¨éƒ¨è¯»å…¥å†…å­˜ä¸­è¿›è¡Œè®­ç»ƒï¼Œè¿™æ ·ä¸€èˆ¬æ•ˆç‡æœ€é«˜ã€‚

ä½†å¦‚æœéœ€è¦è®­ç»ƒçš„æ•°æ®å¾ˆå¤§ï¼Œä¾‹å¦‚è¶…è¿‡10Gï¼Œæ— æ³•ä¸€æ¬¡è½½å…¥å†…å­˜ï¼Œé‚£ä¹ˆé€šå¸¸éœ€è¦åœ¨è®­ç»ƒçš„è¿‡ç¨‹ä¸­åˆ†æ‰¹é€æ¸è¯»å…¥ã€‚

ä½¿ç”¨ tf.data API å¯ä»¥æ„å»ºæ•°æ®è¾“å…¥ç®¡é“ï¼Œè½»æ¾å¤„ç†å¤§é‡çš„æ•°æ®ï¼Œä¸åŒçš„æ•°æ®æ ¼å¼ï¼Œä»¥åŠä¸åŒçš„æ•°æ®è½¬æ¢ã€‚

```python

```

### ä¸€ï¼Œæ„å»ºæ•°æ®ç®¡é“


å¯ä»¥ä» Numpy array, Pandas DataFrame, Python generator, csvæ–‡ä»¶, æ–‡æœ¬æ–‡ä»¶, æ–‡ä»¶è·¯å¾„, tfrecordsæ–‡ä»¶ç­‰æ–¹å¼æ„å»ºæ•°æ®ç®¡é“ã€‚

å…¶ä¸­é€šè¿‡Numpy array, Pandas DataFrame, æ–‡ä»¶è·¯å¾„æ„å»ºæ•°æ®ç®¡é“æ˜¯æœ€å¸¸ç”¨çš„æ–¹æ³•ã€‚

é€šè¿‡tfrecordsæ–‡ä»¶æ–¹å¼æ„å»ºæ•°æ®ç®¡é“è¾ƒä¸ºå¤æ‚ï¼Œéœ€è¦å¯¹æ ·æœ¬æ„å»ºtf.Exampleåå‹ç¼©æˆå­—ç¬¦ä¸²å†™åˆ°tfrecoredsæ–‡ä»¶ï¼Œè¯»å–åå†è§£ææˆtf.Exampleã€‚

ä½†tfrecoredsæ–‡ä»¶çš„ä¼˜ç‚¹æ˜¯å‹ç¼©åæ–‡ä»¶è¾ƒå°ï¼Œä¾¿äºç½‘ç»œä¼ æ’­ï¼ŒåŠ è½½é€Ÿåº¦è¾ƒå¿«ã€‚


**1,ä»Numpy arrayæ„å»ºæ•°æ®ç®¡é“**

```python
# ä»Numpy arrayæ„å»ºæ•°æ®ç®¡é“

import tensorflow as tf
import numpy as np 
from sklearn import datasets 
iris = datasets.load_iris()


ds1 = tf.data.Dataset.from_tensor_slices((iris["data"],iris["target"]))
for features,label in ds1.take(5):
    print(features,label)

```

```
tf.Tensor([5.1 3.5 1.4 0.2], shape=(4,), dtype=float64) tf.Tensor(0, shape=(), dtype=int64)
tf.Tensor([4.9 3.  1.4 0.2], shape=(4,), dtype=float64) tf.Tensor(0, shape=(), dtype=int64)
tf.Tensor([4.7 3.2 1.3 0.2], shape=(4,), dtype=float64) tf.Tensor(0, shape=(), dtype=int64)
tf.Tensor([4.6 3.1 1.5 0.2], shape=(4,), dtype=float64) tf.Tensor(0, shape=(), dtype=int64)
tf.Tensor([5.  3.6 1.4 0.2], shape=(4,), dtype=float64) tf.Tensor(0, shape=(), dtype=int64)
```

```python

```

**2,ä» Pandas DataFrameæ„å»ºæ•°æ®ç®¡é“**

```python
# ä» Pandas DataFrameæ„å»ºæ•°æ®ç®¡é“
import tensorflow as tf
from sklearn import datasets 
import pandas as pd
iris = datasets.load_iris()
dfiris = pd.DataFrame(iris["data"],columns = iris.feature_names)
ds2 = tf.data.Dataset.from_tensor_slices((dfiris.to_dict("list"),iris["target"]))

for features,label in ds2.take(3):
    print(features,label)
```

```
{'sepal length (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=5.1>, 'sepal width (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=3.5>, 'petal length (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=1.4>, 'petal width (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=0.2>} tf.Tensor(0, shape=(), dtype=int64)
{'sepal length (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=4.9>, 'sepal width (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=3.0>, 'petal length (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=1.4>, 'petal width (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=0.2>} tf.Tensor(0, shape=(), dtype=int64)
{'sepal length (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=4.7>, 'sepal width (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=3.2>, 'petal length (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=1.3>, 'petal width (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=0.2>} tf.Tensor(0, shape=(), dtype=int64)
```

```python

```

```python

```

**3,ä»Python generatoræ„å»ºæ•°æ®ç®¡é“**

```python
# ä»Python generatoræ„å»ºæ•°æ®ç®¡é“
import tensorflow as tf
from matplotlib import pyplot as plt 
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# å®šä¹‰ä¸€ä¸ªä»æ–‡ä»¶ä¸­è¯»å–å›¾ç‰‡çš„generator
image_generator = ImageDataGenerator(rescale=1.0/255).flow_from_directory(
                    "./data/cifar2/test/",
                    target_size=(32, 32),
                    batch_size=20,
                    class_mode='binary')

classdict = image_generator.class_indices
print(classdict)

def generator():
    for features,label in image_generator:
        yield (features,label)

ds3 = tf.data.Dataset.from_generator(generator,output_types=(tf.float32,tf.int32))
```

```python
%matplotlib inline
%config InlineBackend.figure_format = 'svg'
plt.figure(figsize=(6,6)) 
for i,(img,label) in enumerate(ds3.unbatch().take(9)):
    ax=plt.subplot(3,3,i+1)
    ax.imshow(img.numpy())
    ax.set_title("label = %d"%label)
    ax.set_xticks([])
    ax.set_yticks([]) 
plt.show()
```

![](./data/5-1-cifar2é¢„è§ˆ.jpg)

```python

```

**4,ä»csvæ–‡ä»¶æ„å»ºæ•°æ®ç®¡é“**

```python
# ä»csvæ–‡ä»¶æ„å»ºæ•°æ®ç®¡é“
ds4 = tf.data.experimental.make_csv_dataset(
      file_pattern = ["./data/titanic/train.csv","./data/titanic/test.csv"],
      batch_size=3, 
      label_name="Survived",
      na_value="",
      num_epochs=1,
      ignore_errors=True)

for data,label in ds4.take(2):
    print(data,label)
```

```
OrderedDict([('PassengerId', <tf.Tensor: shape=(3,), dtype=int32, numpy=array([540,  58, 764], dtype=int32)>), ('Pclass', <tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 3, 1], dtype=int32)>), ('Name', <tf.Tensor: shape=(3,), dtype=string, numpy=
array([b'Frolicher, Miss. Hedwig Margaritha', b'Novel, Mr. Mansouer',
       b'Carter, Mrs. William Ernest (Lucile Polk)'], dtype=object)>), ('Sex', <tf.Tensor: shape=(3,), dtype=string, numpy=array([b'female', b'male', b'female'], dtype=object)>), ('Age', <tf.Tensor: shape=(3,), dtype=float32, numpy=array([22. , 28.5, 36. ], dtype=float32)>), ('SibSp', <tf.Tensor: shape=(3,), dtype=int32, numpy=array([0, 0, 1], dtype=int32)>), ('Parch', <tf.Tensor: shape=(3,), dtype=int32, numpy=array([2, 0, 2], dtype=int32)>), ('Ticket', <tf.Tensor: shape=(3,), dtype=string, numpy=array([b'13568', b'2697', b'113760'], dtype=object)>), ('Fare', <tf.Tensor: shape=(3,), dtype=float32, numpy=array([ 49.5   ,   7.2292, 120.    ], dtype=float32)>), ('Cabin', <tf.Tensor: shape=(3,), dtype=string, numpy=array([b'B39', b'', b'B96 B98'], dtype=object)>), ('Embarked', <tf.Tensor: shape=(3,), dtype=string, numpy=array([b'C', b'C', b'S'], dtype=object)>)]) tf.Tensor([1 0 1], shape=(3,), dtype=int32)
OrderedDict([('PassengerId', <tf.Tensor: shape=(3,), dtype=int32, numpy=array([845,  66, 390], dtype=int32)>), ('Pclass', <tf.Tensor: shape=(3,), dtype=int32, numpy=array([3, 3, 2], dtype=int32)>), ('Name', <tf.Tensor: shape=(3,), dtype=string, numpy=
array([b'Culumovic, Mr. Jeso', b'Moubarek, Master. Gerios',
       b'Lehmann, Miss. Bertha'], dtype=object)>), ('Sex', <tf.Tensor: shape=(3,), dtype=string, numpy=array([b'male', b'male', b'female'], dtype=object)>), ('Age', <tf.Tensor: shape=(3,), dtype=float32, numpy=array([17.,  0., 17.], dtype=float32)>), ('SibSp', <tf.Tensor: shape=(3,), dtype=int32, numpy=array([0, 1, 0], dtype=int32)>), ('Parch', <tf.Tensor: shape=(3,), dtype=int32, numpy=array([0, 1, 0], dtype=int32)>), ('Ticket', <tf.Tensor: shape=(3,), dtype=string, numpy=array([b'315090', b'2661', b'SC 1748'], dtype=object)>), ('Fare', <tf.Tensor: shape=(3,), dtype=float32, numpy=array([ 8.6625, 15.2458, 12.    ], dtype=float32)>), ('Cabin', <tf.Tensor: shape=(3,), dtype=string, numpy=array([b'', b'', b''], dtype=object)>), ('Embarked', <tf.Tensor: shape=(3,), dtype=string, numpy=array([b'S', b'C', b'C'], dtype=object)>)]) tf.Tensor([0 1 1], shape=(3,), dtype=int32)
```

```python

```

**5,ä»æ–‡æœ¬æ–‡ä»¶æ„å»ºæ•°æ®ç®¡é“**

```python
# ä»æ–‡æœ¬æ–‡ä»¶æ„å»ºæ•°æ®ç®¡é“

ds5 = tf.data.TextLineDataset(
    filenames = ["./data/titanic/train.csv","./data/titanic/test.csv"]
    ).skip(1) #ç•¥å»ç¬¬ä¸€è¡Œheader

for line in ds5.take(5):
    print(line)
```

```
tf.Tensor(b'493,0,1,"Molson, Mr. Harry Markland",male,55.0,0,0,113787,30.5,C30,S', shape=(), dtype=string)
tf.Tensor(b'53,1,1,"Harper, Mrs. Henry Sleeper (Myna Haxtun)",female,49.0,1,0,PC 17572,76.7292,D33,C', shape=(), dtype=string)
tf.Tensor(b'388,1,2,"Buss, Miss. Kate",female,36.0,0,0,27849,13.0,,S', shape=(), dtype=string)
tf.Tensor(b'192,0,2,"Carbines, Mr. William",male,19.0,0,0,28424,13.0,,S', shape=(), dtype=string)
tf.Tensor(b'687,0,3,"Panula, Mr. Jaako Arnold",male,14.0,4,1,3101295,39.6875,,S', shape=(), dtype=string)
```

```python

```

**6,ä»æ–‡ä»¶è·¯å¾„æ„å»ºæ•°æ®ç®¡é“**

```python
ds6 = tf.data.Dataset.list_files("./data/cifar2/train/*/*.jpg")
for file in ds6.take(5):
    print(file)
```

```
tf.Tensor(b'./data/cifar2/train/automobile/1263.jpg', shape=(), dtype=string)
tf.Tensor(b'./data/cifar2/train/airplane/2837.jpg', shape=(), dtype=string)
tf.Tensor(b'./data/cifar2/train/airplane/4264.jpg', shape=(), dtype=string)
tf.Tensor(b'./data/cifar2/train/automobile/4241.jpg', shape=(), dtype=string)
tf.Tensor(b'./data/cifar2/train/automobile/192.jpg', shape=(), dtype=string)
```

```python
from matplotlib import pyplot as plt 
def load_image(img_path,size = (32,32)):
    label = 1 if tf.strings.regex_full_match(img_path,".*/automobile/.*") else 0
    img = tf.io.read_file(img_path)
    img = tf.image.decode_jpeg(img) #æ³¨æ„æ­¤å¤„ä¸ºjpegæ ¼å¼
    img = tf.image.resize(img,size)
    return(img,label)

%matplotlib inline
%config InlineBackend.figure_format = 'svg'
for i,(img,label) in enumerate(ds6.map(load_image).take(2)):
    plt.figure(i)
    plt.imshow((img/255.0).numpy())
    plt.title("label = %d"%label)
    plt.xticks([])
    plt.yticks([])
```

![](./data/5-1-car2.jpg)

```python

```

**7,ä»tfrecordsæ–‡ä»¶æ„å»ºæ•°æ®ç®¡é“**

```python
import os
import numpy as np

# inpathï¼šåŸå§‹æ•°æ®è·¯å¾„ outpath:TFRecordæ–‡ä»¶è¾“å‡ºè·¯å¾„
def create_tfrecords(inpath,outpath): 
    writer = tf.io.TFRecordWriter(outpath)
    dirs = os.listdir(inpath)
    for index, name in enumerate(dirs):
        class_path = inpath +"/"+ name+"/"
        for img_name in os.listdir(class_path):
            img_path = class_path + img_name
            img = tf.io.read_file(img_path)
            #img = tf.image.decode_image(img)
            #img = tf.image.encode_jpeg(img) #ç»Ÿä¸€æˆjpegæ ¼å¼å‹ç¼©
            example = tf.train.Example(
               features=tf.train.Features(feature={
                    'label': tf.train.Feature(int64_list=tf.train.Int64List(value=[index])),
                    'img_raw': tf.train.Feature(bytes_list=tf.train.BytesList(value=[img.numpy()]))
               }))
            writer.write(example.SerializeToString())
    writer.close()
    
create_tfrecords("./data/cifar2/test/","./data/cifar2_test.tfrecords/")

```

```python
from matplotlib import pyplot as plt 

def parse_example(proto):
    description ={ 'img_raw' : tf.io.FixedLenFeature([], tf.string),
                   'label': tf.io.FixedLenFeature([], tf.int64)} 
    example = tf.io.parse_single_example(proto, description)
    img = tf.image.decode_jpeg(example["img_raw"])   #æ³¨æ„æ­¤å¤„ä¸ºjpegæ ¼å¼
    img = tf.image.resize(img, (32,32))
    label = example["label"]
    return(img,label)

ds7 = tf.data.TFRecordDataset("./data/cifar2_test.tfrecords").map(parse_example).shuffle(3000)

%matplotlib inline
%config InlineBackend.figure_format = 'svg'
plt.figure(figsize=(6,6)) 
for i,(img,label) in enumerate(ds7.take(9)):
    ax=plt.subplot(3,3,i+1)
    ax.imshow((img/255.0).numpy())
    ax.set_title("label = %d"%label)
    ax.set_xticks([])
    ax.set_yticks([]) 
plt.show()

```

![](./data/5-1-car9.jpg)

```python

```

```python

```

### äºŒï¼Œåº”ç”¨æ•°æ®è½¬æ¢


Datasetæ•°æ®ç»“æ„åº”ç”¨éå¸¸çµæ´»ï¼Œå› ä¸ºå®ƒæœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªSequeceåºåˆ—ï¼Œå…¶æ¯ä¸ªå…ƒç´ å¯ä»¥æ˜¯å„ç§ç±»å‹ï¼Œä¾‹å¦‚å¯ä»¥æ˜¯å¼ é‡ï¼Œåˆ—è¡¨ï¼Œå­—å…¸ï¼Œä¹Ÿå¯ä»¥æ˜¯Datasetã€‚

DatasetåŒ…å«äº†éå¸¸ä¸°å¯Œçš„æ•°æ®è½¬æ¢åŠŸèƒ½ã€‚

* map: å°†è½¬æ¢å‡½æ•°æ˜ å°„åˆ°æ•°æ®é›†æ¯ä¸€ä¸ªå…ƒç´ ã€‚

* flat_map: å°†è½¬æ¢å‡½æ•°æ˜ å°„åˆ°æ•°æ®é›†çš„æ¯ä¸€ä¸ªå…ƒç´ ï¼Œå¹¶å°†åµŒå¥—çš„Datasetå‹å¹³ã€‚

* interleave: æ•ˆæœç±»ä¼¼flat_map,ä½†å¯ä»¥å°†ä¸åŒæ¥æºçš„æ•°æ®å¤¹åœ¨ä¸€èµ·ã€‚

* filter: è¿‡æ»¤æ‰æŸäº›å…ƒç´ ã€‚

* zip: å°†ä¸¤ä¸ªé•¿åº¦ç›¸åŒçš„Datasetæ¨ªå‘é“°åˆã€‚

* concatenate: å°†ä¸¤ä¸ªDatasetçºµå‘è¿æ¥ã€‚

* reduce: æ‰§è¡Œå½’å¹¶æ“ä½œã€‚

* batch : æ„å»ºæ‰¹æ¬¡ï¼Œæ¯æ¬¡æ”¾ä¸€ä¸ªæ‰¹æ¬¡ã€‚æ¯”åŸå§‹æ•°æ®å¢åŠ ä¸€ä¸ªç»´åº¦ã€‚ å…¶é€†æ“ä½œä¸ºunbatchã€‚

* padded_batch: æ„å»ºæ‰¹æ¬¡ï¼Œç±»ä¼¼batch, ä½†å¯ä»¥å¡«å……åˆ°ç›¸åŒçš„å½¢çŠ¶ã€‚

* window :æ„å»ºæ»‘åŠ¨çª—å£ï¼Œè¿”å›Dataset of Dataset.

* shuffle: æ•°æ®é¡ºåºæ´—ç‰Œã€‚

* repeat: é‡å¤æ•°æ®è‹¥å¹²æ¬¡ï¼Œä¸å¸¦å‚æ•°æ—¶ï¼Œé‡å¤æ— æ•°æ¬¡ã€‚

* shard: é‡‡æ ·ï¼Œä»æŸä¸ªä½ç½®å¼€å§‹éš”å›ºå®šè·ç¦»é‡‡æ ·ä¸€ä¸ªå…ƒç´ ã€‚

* take: é‡‡æ ·ï¼Œä»å¼€å§‹ä½ç½®å–å‰å‡ ä¸ªå…ƒç´ ã€‚


```python
#map:å°†è½¬æ¢å‡½æ•°æ˜ å°„åˆ°æ•°æ®é›†æ¯ä¸€ä¸ªå…ƒç´ 

ds = tf.data.Dataset.from_tensor_slices(["hello world","hello China","hello Beijing"])
ds_map = ds.map(lambda x:tf.strings.split(x," "))
for x in ds_map:
    print(x)
```

```
tf.Tensor([b'hello' b'world'], shape=(2,), dtype=string)
tf.Tensor([b'hello' b'China'], shape=(2,), dtype=string)
tf.Tensor([b'hello' b'Beijing'], shape=(2,), dtype=string)
```

```python
#flat_map:å°†è½¬æ¢å‡½æ•°æ˜ å°„åˆ°æ•°æ®é›†çš„æ¯ä¸€ä¸ªå…ƒç´ ï¼Œå¹¶å°†åµŒå¥—çš„Datasetå‹å¹³ã€‚

ds = tf.data.Dataset.from_tensor_slices(["hello world","hello China","hello Beijing"])
ds_flatmap = ds.flat_map(lambda x:tf.data.Dataset.from_tensor_slices(tf.strings.split(x," ")))
for x in ds_flatmap:
    print(x)
```

```
tf.Tensor(b'hello', shape=(), dtype=string)
tf.Tensor(b'world', shape=(), dtype=string)
tf.Tensor(b'hello', shape=(), dtype=string)
tf.Tensor(b'China', shape=(), dtype=string)
tf.Tensor(b'hello', shape=(), dtype=string)
tf.Tensor(b'Beijing', shape=(), dtype=string)
```

```python

```

```python
# interleave: æ•ˆæœç±»ä¼¼flat_map,ä½†å¯ä»¥å°†ä¸åŒæ¥æºçš„æ•°æ®å¤¹åœ¨ä¸€èµ·ã€‚

ds = tf.data.Dataset.from_tensor_slices(["hello world","hello China","hello Beijing"])
ds_interleave = ds.interleave(lambda x:tf.data.Dataset.from_tensor_slices(tf.strings.split(x," ")))
for x in ds_interleave:
    print(x)
    
```

```
tf.Tensor(b'hello', shape=(), dtype=string)
tf.Tensor(b'hello', shape=(), dtype=string)
tf.Tensor(b'hello', shape=(), dtype=string)
tf.Tensor(b'world', shape=(), dtype=string)
tf.Tensor(b'China', shape=(), dtype=string)
tf.Tensor(b'Beijing', shape=(), dtype=string)
```

```python

```

```python
#filter:è¿‡æ»¤æ‰æŸäº›å…ƒç´ ã€‚

ds = tf.data.Dataset.from_tensor_slices(["hello world","hello China","hello Beijing"])
#æ‰¾å‡ºå«æœ‰å­—æ¯aæˆ–Bçš„å…ƒç´ 
ds_filter = ds.filter(lambda x: tf.strings.regex_full_match(x, ".*[a|B].*"))
for x in ds_filter:
    print(x)
    
```

```
tf.Tensor(b'hello China', shape=(), dtype=string)
tf.Tensor(b'hello Beijing', shape=(), dtype=string)
```

```python

```

```python
#zip:å°†ä¸¤ä¸ªé•¿åº¦ç›¸åŒçš„Datasetæ¨ªå‘é“°åˆã€‚

ds1 = tf.data.Dataset.range(0,3)
ds2 = tf.data.Dataset.range(3,6)
ds3 = tf.data.Dataset.range(6,9)
ds_zip = tf.data.Dataset.zip((ds1,ds2,ds3))
for x,y,z in ds_zip:
    print(x.numpy(),y.numpy(),z.numpy())

```

```
0 3 6
1 4 7
2 5 8
```

```python
#condatenate:å°†ä¸¤ä¸ªDatasetçºµå‘è¿æ¥ã€‚

ds1 = tf.data.Dataset.range(0,3)
ds2 = tf.data.Dataset.range(3,6)
ds_concat = tf.data.Dataset.concatenate(ds1,ds2)
for x in ds_concat:
    print(x)
```

```
tf.Tensor(0, shape=(), dtype=int64)
tf.Tensor(1, shape=(), dtype=int64)
tf.Tensor(2, shape=(), dtype=int64)
tf.Tensor(3, shape=(), dtype=int64)
tf.Tensor(4, shape=(), dtype=int64)
tf.Tensor(5, shape=(), dtype=int64)
```

```python
#reduce:æ‰§è¡Œå½’å¹¶æ“ä½œã€‚

ds = tf.data.Dataset.from_tensor_slices([1,2,3,4,5.0])
result = ds.reduce(0.0,lambda x,y:tf.add(x,y))
result
```

```
<tf.Tensor: shape=(), dtype=float32, numpy=15.0>
```

```python

```

```python
#batch:æ„å»ºæ‰¹æ¬¡ï¼Œæ¯æ¬¡æ”¾ä¸€ä¸ªæ‰¹æ¬¡ã€‚æ¯”åŸå§‹æ•°æ®å¢åŠ ä¸€ä¸ªç»´åº¦ã€‚ å…¶é€†æ“ä½œä¸ºunbatchã€‚ 

ds = tf.data.Dataset.range(12)
ds_batch = ds.batch(4)
for x in ds_batch:
    print(x)
```

```
tf.Tensor([0 1 2 3], shape=(4,), dtype=int64)
tf.Tensor([4 5 6 7], shape=(4,), dtype=int64)
tf.Tensor([ 8  9 10 11], shape=(4,), dtype=int64)
```

```python

```

```python
#padded_batch:æ„å»ºæ‰¹æ¬¡ï¼Œç±»ä¼¼batch, ä½†å¯ä»¥å¡«å……åˆ°ç›¸åŒçš„å½¢çŠ¶ã€‚

elements = [[1, 2],[3, 4, 5],[6, 7],[8]]
ds = tf.data.Dataset.from_generator(lambda: iter(elements), tf.int32)

ds_padded_batch = ds.padded_batch(2,padded_shapes = [4,])
for x in ds_padded_batch:
    print(x)    
```

```
tf.Tensor(
[[1 2 0 0]
 [3 4 5 0]], shape=(2, 4), dtype=int32)
tf.Tensor(
[[6 7 0 0]
 [8 0 0 0]], shape=(2, 4), dtype=int32)
```

```python

```

```python
#window:æ„å»ºæ»‘åŠ¨çª—å£ï¼Œè¿”å›Dataset of Dataset.

ds = tf.data.Dataset.range(12)
#windowè¿”å›çš„æ˜¯Dataset of Dataset,å¯ä»¥ç”¨flat_mapå‹å¹³
ds_window = ds.window(3, shift=1).flat_map(lambda x: x.batch(3,drop_remainder=True)) 
for x in ds_window:
    print(x)
```

```
tf.Tensor([0 1 2], shape=(3,), dtype=int64)
tf.Tensor([1 2 3], shape=(3,), dtype=int64)
tf.Tensor([2 3 4], shape=(3,), dtype=int64)
tf.Tensor([3 4 5], shape=(3,), dtype=int64)
tf.Tensor([4 5 6], shape=(3,), dtype=int64)
tf.Tensor([5 6 7], shape=(3,), dtype=int64)
tf.Tensor([6 7 8], shape=(3,), dtype=int64)
tf.Tensor([7 8 9], shape=(3,), dtype=int64)
tf.Tensor([ 8  9 10], shape=(3,), dtype=int64)
tf.Tensor([ 9 10 11], shape=(3,), dtype=int64)
```

```python

```

```python
#shuffle:æ•°æ®é¡ºåºæ´—ç‰Œã€‚

ds = tf.data.Dataset.range(12)
ds_shuffle = ds.shuffle(buffer_size = 5)
for x in ds_shuffle:
    print(x)
    
```

```
tf.Tensor(1, shape=(), dtype=int64)
tf.Tensor(4, shape=(), dtype=int64)
tf.Tensor(0, shape=(), dtype=int64)
tf.Tensor(6, shape=(), dtype=int64)
tf.Tensor(5, shape=(), dtype=int64)
tf.Tensor(2, shape=(), dtype=int64)
tf.Tensor(7, shape=(), dtype=int64)
tf.Tensor(11, shape=(), dtype=int64)
tf.Tensor(3, shape=(), dtype=int64)
tf.Tensor(9, shape=(), dtype=int64)
tf.Tensor(10, shape=(), dtype=int64)
tf.Tensor(8, shape=(), dtype=int64)
```

```python

```

```python
#repeat:é‡å¤æ•°æ®è‹¥å¹²æ¬¡ï¼Œä¸å¸¦å‚æ•°æ—¶ï¼Œé‡å¤æ— æ•°æ¬¡ã€‚

ds = tf.data.Dataset.range(3)
ds_repeat = ds.repeat(3)
for x in ds_repeat:
    print(x)
```

```
tf.Tensor(0, shape=(), dtype=int64)
tf.Tensor(1, shape=(), dtype=int64)
tf.Tensor(2, shape=(), dtype=int64)
tf.Tensor(0, shape=(), dtype=int64)
tf.Tensor(1, shape=(), dtype=int64)
tf.Tensor(2, shape=(), dtype=int64)
tf.Tensor(0, shape=(), dtype=int64)
tf.Tensor(1, shape=(), dtype=int64)
tf.Tensor(2, shape=(), dtype=int64)
```

```python
#shard:é‡‡æ ·ï¼Œä»æŸä¸ªä½ç½®å¼€å§‹éš”å›ºå®šè·ç¦»é‡‡æ ·ä¸€ä¸ªå…ƒç´ ã€‚

ds = tf.data.Dataset.range(12)
ds_shard = ds.shard(3,index = 1)

for x in ds_shard:
    print(x)
```

```
tf.Tensor(1, shape=(), dtype=int64)
tf.Tensor(4, shape=(), dtype=int64)
tf.Tensor(7, shape=(), dtype=int64)
tf.Tensor(10, shape=(), dtype=int64)
```

```python
#take:é‡‡æ ·ï¼Œä»å¼€å§‹ä½ç½®å–å‰å‡ ä¸ªå…ƒç´ ã€‚

ds = tf.data.Dataset.range(12)
ds_take = ds.take(3)

list(ds_take.as_numpy_iterator())

```

```
[0, 1, 2]
```

```python

```

```python

```

### ä¸‰ï¼Œæå‡ç®¡é“æ€§èƒ½


è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹å¸¸å¸¸ä¼šéå¸¸è€—æ—¶ã€‚

æ¨¡å‹è®­ç»ƒçš„è€—æ—¶ä¸»è¦æ¥è‡ªäºä¸¤ä¸ªéƒ¨åˆ†ï¼Œä¸€éƒ¨åˆ†æ¥è‡ª**æ•°æ®å‡†å¤‡**ï¼Œå¦ä¸€éƒ¨åˆ†æ¥è‡ª**å‚æ•°è¿­ä»£**ã€‚

å‚æ•°è¿­ä»£è¿‡ç¨‹çš„è€—æ—¶é€šå¸¸ä¾èµ–äºGPUæ¥æå‡ã€‚

è€Œæ•°æ®å‡†å¤‡è¿‡ç¨‹çš„è€—æ—¶åˆ™å¯ä»¥é€šè¿‡æ„å»ºé«˜æ•ˆçš„æ•°æ®ç®¡é“è¿›è¡Œæå‡ã€‚

ä»¥ä¸‹æ˜¯ä¸€äº›æ„å»ºé«˜æ•ˆæ•°æ®ç®¡é“çš„å»ºè®®ã€‚

* 1ï¼Œä½¿ç”¨ prefetch æ–¹æ³•è®©æ•°æ®å‡†å¤‡å’Œå‚æ•°è¿­ä»£ä¸¤ä¸ªè¿‡ç¨‹ç›¸äº’å¹¶è¡Œã€‚

* 2ï¼Œä½¿ç”¨ interleave æ–¹æ³•å¯ä»¥è®©æ•°æ®è¯»å–è¿‡ç¨‹å¤šè¿›ç¨‹æ‰§è¡Œ,å¹¶å°†ä¸åŒæ¥æºæ•°æ®å¤¹åœ¨ä¸€èµ·ã€‚

* 3ï¼Œä½¿ç”¨ map æ—¶è®¾ç½®num_parallel_calls è®©æ•°æ®è½¬æ¢è¿‡ç¨‹å¤šè¿›è¡Œæ‰§è¡Œã€‚

* 4ï¼Œä½¿ç”¨ cache æ–¹æ³•è®©æ•°æ®åœ¨ç¬¬ä¸€ä¸ªepochåç¼“å­˜åˆ°å†…å­˜ä¸­ï¼Œä»…é™äºæ•°æ®é›†ä¸å¤§æƒ…å½¢ã€‚

* 5ï¼Œä½¿ç”¨ mapè½¬æ¢æ—¶ï¼Œå…ˆbatch, ç„¶åé‡‡ç”¨å‘é‡åŒ–çš„è½¬æ¢æ–¹æ³•å¯¹æ¯ä¸ªbatchè¿›è¡Œè½¬æ¢ã€‚

```python

```

**1ï¼Œä½¿ç”¨ prefetch æ–¹æ³•è®©æ•°æ®å‡†å¤‡å’Œå‚æ•°è¿­ä»£ä¸¤ä¸ªè¿‡ç¨‹ç›¸äº’å¹¶è¡Œã€‚**

```python
import tensorflow as tf

#æ‰“å°æ—¶é—´åˆ†å‰²çº¿
@tf.function
def printbar():
    ts = tf.timestamp()
    today_ts = ts%(24*60*60)

    hour = tf.cast(today_ts//3600+8,tf.int32)%tf.constant(24)
    minite = tf.cast((today_ts%3600)//60,tf.int32)
    second = tf.cast(tf.floor(today_ts%60),tf.int32)
    
    def timeformat(m):
        if tf.strings.length(tf.strings.format("{}",m))==1:
            return(tf.strings.format("0{}",m))
        else:
            return(tf.strings.format("{}",m))
    
    timestring = tf.strings.join([timeformat(hour),timeformat(minite),
                timeformat(second)],separator = ":")
    tf.print("=========="*8,end = "")
    tf.print(timestring)
    
```

```python
import time

# æ•°æ®å‡†å¤‡å’Œå‚æ•°è¿­ä»£ä¸¤ä¸ªè¿‡ç¨‹é»˜è®¤æƒ…å†µä¸‹æ˜¯ä¸²è¡Œçš„ã€‚

# æ¨¡æ‹Ÿæ•°æ®å‡†å¤‡
def generator():
    for i in range(10):
        #å‡è®¾æ¯æ¬¡å‡†å¤‡æ•°æ®éœ€è¦2s
        time.sleep(2) 
        yield i 
ds = tf.data.Dataset.from_generator(generator,output_types = (tf.int32))

# æ¨¡æ‹Ÿå‚æ•°è¿­ä»£
def train_step():
    #å‡è®¾æ¯ä¸€æ­¥è®­ç»ƒéœ€è¦1s
    time.sleep(1) 
    
```

```python
# è®­ç»ƒè¿‡ç¨‹é¢„è®¡è€—æ—¶ 10*2+10*1+ = 30s
printbar()
tf.print(tf.constant("start training..."))
for x in ds:
    train_step()  
printbar()
tf.print(tf.constant("end training..."))
```

```python
# ä½¿ç”¨ prefetch æ–¹æ³•è®©æ•°æ®å‡†å¤‡å’Œå‚æ•°è¿­ä»£ä¸¤ä¸ªè¿‡ç¨‹ç›¸äº’å¹¶è¡Œã€‚

# è®­ç»ƒè¿‡ç¨‹é¢„è®¡è€—æ—¶ max(10*2,10*1) = 20s
printbar()
tf.print(tf.constant("start training with prefetch..."))

# tf.data.experimental.AUTOTUNE å¯ä»¥è®©ç¨‹åºè‡ªåŠ¨é€‰æ‹©åˆé€‚çš„å‚æ•°
for x in ds.prefetch(buffer_size = tf.data.experimental.AUTOTUNE):
    train_step()  
    
printbar()
tf.print(tf.constant("end training..."))

```

```python

```

**2ï¼Œä½¿ç”¨ interleave æ–¹æ³•å¯ä»¥è®©æ•°æ®è¯»å–è¿‡ç¨‹å¤šè¿›ç¨‹æ‰§è¡Œ,å¹¶å°†ä¸åŒæ¥æºæ•°æ®å¤¹åœ¨ä¸€èµ·ã€‚**

```python
ds_files = tf.data.Dataset.list_files("./data/titanic/*.csv")
ds = ds_files.flat_map(lambda x:tf.data.TextLineDataset(x).skip(1))
for line in ds.take(4):
    print(line)
```

```
tf.Tensor(b'493,0,1,"Molson, Mr. Harry Markland",male,55.0,0,0,113787,30.5,C30,S', shape=(), dtype=string)
tf.Tensor(b'53,1,1,"Harper, Mrs. Henry Sleeper (Myna Haxtun)",female,49.0,1,0,PC 17572,76.7292,D33,C', shape=(), dtype=string)
tf.Tensor(b'388,1,2,"Buss, Miss. Kate",female,36.0,0,0,27849,13.0,,S', shape=(), dtype=string)
tf.Tensor(b'192,0,2,"Carbines, Mr. William",male,19.0,0,0,28424,13.0,,S', shape=(), dtype=string)
```

```python
ds_files = tf.data.Dataset.list_files("./data/titanic/*.csv")
ds = ds_files.interleave(lambda x:tf.data.TextLineDataset(x).skip(1))
for line in ds.take(8):
    print(line)
```

```
tf.Tensor(b'181,0,3,"Sage, Miss. Constance Gladys",female,,8,2,CA. 2343,69.55,,S', shape=(), dtype=string)
tf.Tensor(b'493,0,1,"Molson, Mr. Harry Markland",male,55.0,0,0,113787,30.5,C30,S', shape=(), dtype=string)
tf.Tensor(b'405,0,3,"Oreskovic, Miss. Marija",female,20.0,0,0,315096,8.6625,,S', shape=(), dtype=string)
tf.Tensor(b'53,1,1,"Harper, Mrs. Henry Sleeper (Myna Haxtun)",female,49.0,1,0,PC 17572,76.7292,D33,C', shape=(), dtype=string)
tf.Tensor(b'635,0,3,"Skoog, Miss. Mabel",female,9.0,3,2,347088,27.9,,S', shape=(), dtype=string)
tf.Tensor(b'388,1,2,"Buss, Miss. Kate",female,36.0,0,0,27849,13.0,,S', shape=(), dtype=string)
tf.Tensor(b'701,1,1,"Astor, Mrs. John Jacob (Madeleine Talmadge Force)",female,18.0,1,0,PC 17757,227.525,C62 C64,C', shape=(), dtype=string)
tf.Tensor(b'192,0,2,"Carbines, Mr. William",male,19.0,0,0,28424,13.0,,S', shape=(), dtype=string)
```

```python

```

**3ï¼Œä½¿ç”¨ map æ—¶è®¾ç½®num_parallel_calls è®©æ•°æ®è½¬æ¢è¿‡ç¨‹å¤šè¿›è¡Œæ‰§è¡Œã€‚**

```python
ds = tf.data.Dataset.list_files("./data/cifar2/train/*/*.jpg")
def load_image(img_path,size = (32,32)):
    label = 1 if tf.strings.regex_full_match(img_path,".*/automobile/.*") else 0
    img = tf.io.read_file(img_path)
    img = tf.image.decode_jpeg(img) #æ³¨æ„æ­¤å¤„ä¸ºjpegæ ¼å¼
    img = tf.image.resize(img,size)
    return(img,label)
```

```python
#å•è¿›ç¨‹è½¬æ¢
printbar()
tf.print(tf.constant("start transformation..."))

ds_map = ds.map(load_image)
for _ in ds_map:
    pass

printbar()
tf.print(tf.constant("end transformation..."))
```

```python
#å¤šè¿›ç¨‹è½¬æ¢
printbar()
tf.print(tf.constant("start parallel transformation..."))

ds_map_parallel = ds.map(load_image,num_parallel_calls = tf.data.experimental.AUTOTUNE)
for _ in ds_map_parallel:
    pass

printbar()
tf.print(tf.constant("end parallel transformation..."))
```

```python

```

**4ï¼Œä½¿ç”¨ cache æ–¹æ³•è®©æ•°æ®åœ¨ç¬¬ä¸€ä¸ªepochåç¼“å­˜åˆ°å†…å­˜ä¸­ï¼Œä»…é™äºæ•°æ®é›†ä¸å¤§æƒ…å½¢ã€‚**

```python
import time

# æ¨¡æ‹Ÿæ•°æ®å‡†å¤‡
def generator():
    for i in range(5):
        #å‡è®¾æ¯æ¬¡å‡†å¤‡æ•°æ®éœ€è¦2s
        time.sleep(2) 
        yield i 
ds = tf.data.Dataset.from_generator(generator,output_types = (tf.int32))

# æ¨¡æ‹Ÿå‚æ•°è¿­ä»£
def train_step():
    #å‡è®¾æ¯ä¸€æ­¥è®­ç»ƒéœ€è¦0s
    pass

# è®­ç»ƒè¿‡ç¨‹é¢„è®¡è€—æ—¶ (5*2+5*0)*3 = 30s
printbar()
tf.print(tf.constant("start training..."))
for epoch in tf.range(3):
    for x in ds:
        train_step()  
    printbar()
    tf.print("epoch =",epoch," ended")
printbar()
tf.print(tf.constant("end training..."))

```

```python
import time

# æ¨¡æ‹Ÿæ•°æ®å‡†å¤‡
def generator():
    for i in range(5):
        #å‡è®¾æ¯æ¬¡å‡†å¤‡æ•°æ®éœ€è¦2s
        time.sleep(2) 
        yield i 

# ä½¿ç”¨ cache æ–¹æ³•è®©æ•°æ®åœ¨ç¬¬ä¸€ä¸ªepochåç¼“å­˜åˆ°å†…å­˜ä¸­ï¼Œä»…é™äºæ•°æ®é›†ä¸å¤§æƒ…å½¢ã€‚
ds = tf.data.Dataset.from_generator(generator,output_types = (tf.int32)).cache()

# æ¨¡æ‹Ÿå‚æ•°è¿­ä»£
def train_step():
    #å‡è®¾æ¯ä¸€æ­¥è®­ç»ƒéœ€è¦0s
    time.sleep(0) 

# è®­ç»ƒè¿‡ç¨‹é¢„è®¡è€—æ—¶ (5*2+5*0)+(5*0+5*0)*2 = 10s
printbar()
tf.print(tf.constant("start training..."))
for epoch in tf.range(3):
    for x in ds:
        train_step()  
    printbar()
    tf.print("epoch =",epoch," ended")
printbar()
tf.print(tf.constant("end training..."))
```

```python

```

**5ï¼Œä½¿ç”¨ mapè½¬æ¢æ—¶ï¼Œå…ˆbatch, ç„¶åé‡‡ç”¨å‘é‡åŒ–çš„è½¬æ¢æ–¹æ³•å¯¹æ¯ä¸ªbatchè¿›è¡Œè½¬æ¢ã€‚**

```python
#å…ˆmapåbatch
ds = tf.data.Dataset.range(100000)
ds_map_batch = ds.map(lambda x:x**2).batch(20)

printbar()
tf.print(tf.constant("start scalar transformation..."))
for x in ds_map_batch:
    pass
printbar()
tf.print(tf.constant("end scalar transformation..."))

```

```python
#å…ˆbatchåmap
ds = tf.data.Dataset.range(100000)
ds_batch_map = ds.batch(20).map(lambda x:x**2)

printbar()
tf.print(tf.constant("start vector transformation..."))
for x in ds_batch_map:
    pass
printbar()
tf.print(tf.constant("end vector transformation..."))

```

```python

```

å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·"Pythonä¸ç®—æ³•ä¹‹ç¾"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚

![image.png](./data/Pythonä¸ç®—æ³•ä¹‹ç¾logo.jpg)

```python

```
# 5-2,ç‰¹å¾åˆ—feature_column

ç‰¹å¾åˆ— é€šå¸¸ç”¨äºå¯¹ç»“æ„åŒ–æ•°æ®å®æ–½ç‰¹å¾å·¥ç¨‹æ—¶å€™ä½¿ç”¨ï¼Œå›¾åƒæˆ–è€…æ–‡æœ¬æ•°æ®ä¸€èˆ¬ä¸ä¼šç”¨åˆ°ç‰¹å¾åˆ—ã€‚


### ä¸€ï¼Œç‰¹å¾åˆ—ç”¨æ³•æ¦‚è¿°


ä½¿ç”¨ç‰¹å¾åˆ—å¯ä»¥å°†ç±»åˆ«ç‰¹å¾è½¬æ¢ä¸ºone-hotç¼–ç ç‰¹å¾ï¼Œå°†è¿ç»­ç‰¹å¾æ„å»ºåˆ†æ¡¶ç‰¹å¾ï¼Œä»¥åŠå¯¹å¤šä¸ªç‰¹å¾ç”Ÿæˆäº¤å‰ç‰¹å¾ç­‰ç­‰ã€‚


è¦åˆ›å»ºç‰¹å¾åˆ—ï¼Œè¯·è°ƒç”¨ tf.feature_column æ¨¡å—çš„å‡½æ•°ã€‚è¯¥æ¨¡å—ä¸­å¸¸ç”¨çš„ä¹ä¸ªå‡½æ•°å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œæ‰€æœ‰ä¹ä¸ªå‡½æ•°éƒ½ä¼šè¿”å›ä¸€ä¸ª Categorical-Column æˆ–ä¸€ä¸ª 
Dense-Column å¯¹è±¡ï¼Œä½†å´ä¸ä¼šè¿”å› bucketized_columnï¼Œåè€…ç»§æ‰¿è‡ªè¿™ä¸¤ä¸ªç±»ã€‚

æ³¨æ„ï¼šæ‰€æœ‰çš„Catogorical Columnç±»å‹æœ€ç»ˆéƒ½è¦é€šè¿‡indicator_columnè½¬æ¢æˆDense Columnç±»å‹æ‰èƒ½ä¼ å…¥æ¨¡å‹ï¼


![](./data/ç‰¹å¾åˆ—9ç§.jpg)


* numeric_column æ•°å€¼åˆ—ï¼Œæœ€å¸¸ç”¨ã€‚


* bucketized_column åˆ†æ¡¶åˆ—ï¼Œç”±æ•°å€¼åˆ—ç”Ÿæˆï¼Œå¯ä»¥ç”±ä¸€ä¸ªæ•°å€¼åˆ—å‡ºå¤šä¸ªç‰¹å¾ï¼Œone-hotç¼–ç ã€‚


* categorical_column_with_identity åˆ†ç±»æ ‡è¯†åˆ—ï¼Œone-hotç¼–ç ï¼Œç›¸å½“äºåˆ†æ¡¶åˆ—æ¯ä¸ªæ¡¶ä¸º1ä¸ªæ•´æ•°çš„æƒ…å†µã€‚


* categorical_column_with_vocabulary_list åˆ†ç±»è¯æ±‡åˆ—ï¼Œone-hotç¼–ç ï¼Œç”±listæŒ‡å®šè¯å…¸ã€‚


* categorical_column_with_vocabulary_file åˆ†ç±»è¯æ±‡åˆ—ï¼Œç”±æ–‡ä»¶fileæŒ‡å®šè¯å…¸ã€‚


* categorical_column_with_hash_bucket å“ˆå¸Œåˆ—ï¼Œæ•´æ•°æˆ–è¯å…¸è¾ƒå¤§æ—¶é‡‡ç”¨ã€‚


* indicator_column æŒ‡æ ‡åˆ—ï¼Œç”±Categorical Columnç”Ÿæˆï¼Œone-hotç¼–ç 


* embedding_column åµŒå…¥åˆ—ï¼Œç”±Categorical Columnç”Ÿæˆï¼ŒåµŒå…¥çŸ¢é‡åˆ†å¸ƒå‚æ•°éœ€è¦å­¦ä¹ ã€‚åµŒå…¥çŸ¢é‡ç»´æ•°å»ºè®®å–ç±»åˆ«æ•°é‡çš„ 4 æ¬¡æ–¹æ ¹ã€‚


* crossed_column äº¤å‰åˆ—ï¼Œå¯ä»¥ç”±é™¤categorical_column_with_hash_bucketçš„ä»»æ„åˆ†ç±»åˆ—æ„æˆã€‚


### äºŒï¼Œç‰¹å¾åˆ—ä½¿ç”¨èŒƒä¾‹


ä»¥ä¸‹æ˜¯ä¸€ä¸ªä½¿ç”¨ç‰¹å¾åˆ—è§£å†³Titanicç”Ÿå­˜é—®é¢˜çš„å®Œæ•´èŒƒä¾‹ã€‚

```python
import datetime
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
import tensorflow as tf
from tensorflow.keras import layers,models


#æ‰“å°æ—¥å¿—
def printlog(info):
    nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    print("\n"+"=========="*8 + "%s"%nowtime)
    print(info+'...\n\n')


    
```

```python
#================================================================================
# ä¸€ï¼Œæ„å»ºæ•°æ®ç®¡é“
#================================================================================
printlog("step1: prepare dataset...")


dftrain_raw = pd.read_csv("./data/titanic/train.csv")
dftest_raw = pd.read_csv("./data/titanic/test.csv")

dfraw = pd.concat([dftrain_raw,dftest_raw])

def prepare_dfdata(dfraw):
    dfdata = dfraw.copy()
    dfdata.columns = [x.lower() for x in dfdata.columns]
    dfdata = dfdata.rename(columns={'survived':'label'})
    dfdata = dfdata.drop(['passengerid','name'],axis = 1)
    for col,dtype in dict(dfdata.dtypes).items():
        # åˆ¤æ–­æ˜¯å¦åŒ…å«ç¼ºå¤±å€¼
        if dfdata[col].hasnans:
            # æ·»åŠ æ ‡è¯†æ˜¯å¦ç¼ºå¤±åˆ—
            dfdata[col + '_nan'] = pd.isna(dfdata[col]).astype('int32')
            # å¡«å……
            if dtype not in [np.object,np.str,np.unicode]:
                dfdata[col].fillna(dfdata[col].mean(),inplace = True)
            else:
                dfdata[col].fillna('',inplace = True)
    return(dfdata)

dfdata = prepare_dfdata(dfraw)
dftrain = dfdata.iloc[0:len(dftrain_raw),:]
dftest = dfdata.iloc[len(dftrain_raw):,:]



# ä» dataframe å¯¼å…¥æ•°æ® 
def df_to_dataset(df, shuffle=True, batch_size=32):
    dfdata = df.copy()
    if 'label' not in dfdata.columns:
        ds = tf.data.Dataset.from_tensor_slices(dfdata.to_dict(orient = 'list'))
    else: 
        labels = dfdata.pop('label')
        ds = tf.data.Dataset.from_tensor_slices((dfdata.to_dict(orient = 'list'), labels))  
    if shuffle:
        ds = ds.shuffle(buffer_size=len(dfdata))
    ds = ds.batch(batch_size)
    return ds

ds_train = df_to_dataset(dftrain)
ds_test = df_to_dataset(dftest)
```

```python
#================================================================================
# äºŒï¼Œå®šä¹‰ç‰¹å¾åˆ—
#================================================================================
printlog("step2: make feature columns...")

feature_columns = []

# æ•°å€¼åˆ—
for col in ['age','fare','parch','sibsp'] + [
    c for c in dfdata.columns if c.endswith('_nan')]:
    feature_columns.append(tf.feature_column.numeric_column(col))

# åˆ†æ¡¶åˆ—
age = tf.feature_column.numeric_column('age')
age_buckets = tf.feature_column.bucketized_column(age, 
             boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])
feature_columns.append(age_buckets)

# ç±»åˆ«åˆ—
# æ³¨æ„ï¼šæ‰€æœ‰çš„Catogorical Columnç±»å‹æœ€ç»ˆéƒ½è¦é€šè¿‡indicator_columnè½¬æ¢æˆDense Columnç±»å‹æ‰èƒ½ä¼ å…¥æ¨¡å‹ï¼ï¼
sex = tf.feature_column.indicator_column(
      tf.feature_column.categorical_column_with_vocabulary_list(
      key='sex',vocabulary_list=["male", "female"]))
feature_columns.append(sex)

pclass = tf.feature_column.indicator_column(
      tf.feature_column.categorical_column_with_vocabulary_list(
      key='pclass',vocabulary_list=[1,2,3]))
feature_columns.append(pclass)

ticket = tf.feature_column.indicator_column(
     tf.feature_column.categorical_column_with_hash_bucket('ticket',3))
feature_columns.append(ticket)

embarked = tf.feature_column.indicator_column(
      tf.feature_column.categorical_column_with_vocabulary_list(
      key='embarked',vocabulary_list=['S','C','B']))
feature_columns.append(embarked)

# åµŒå…¥åˆ—
cabin = tf.feature_column.embedding_column(
    tf.feature_column.categorical_column_with_hash_bucket('cabin',32),2)
feature_columns.append(cabin)

# äº¤å‰åˆ—
pclass_cate = tf.feature_column.categorical_column_with_vocabulary_list(
          key='pclass',vocabulary_list=[1,2,3])

crossed_feature = tf.feature_column.indicator_column(
    tf.feature_column.crossed_column([age_buckets, pclass_cate],hash_bucket_size=15))

feature_columns.append(crossed_feature)

```

```python
#================================================================================
# ä¸‰ï¼Œå®šä¹‰æ¨¡å‹
#================================================================================
printlog("step3: define model...")

tf.keras.backend.clear_session()
model = tf.keras.Sequential([
  layers.DenseFeatures(feature_columns), #å°†ç‰¹å¾åˆ—æ”¾å…¥åˆ°tf.keras.layers.DenseFeaturesä¸­!!!
  layers.Dense(64, activation='relu'),
  layers.Dense(64, activation='relu'),
  layers.Dense(1, activation='sigmoid')
])

```

```python
#================================================================================
# å››ï¼Œè®­ç»ƒæ¨¡å‹
#================================================================================
printlog("step4: train model...")

model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

history = model.fit(ds_train,
          validation_data=ds_test,
          epochs=10)
```

```python
#================================================================================
# äº”ï¼Œè¯„ä¼°æ¨¡å‹
#================================================================================
printlog("step5: eval model...")

model.summary()


%matplotlib inline
%config InlineBackend.figure_format = 'svg'

import matplotlib.pyplot as plt

def plot_metric(history, metric):
    train_metrics = history.history[metric]
    val_metrics = history.history['val_'+metric]
    epochs = range(1, len(train_metrics) + 1)
    plt.plot(epochs, train_metrics, 'bo--')
    plt.plot(epochs, val_metrics, 'ro-')
    plt.title('Training and validation '+ metric)
    plt.xlabel("Epochs")
    plt.ylabel(metric)
    plt.legend(["train_"+metric, 'val_'+metric])
    plt.show()

plot_metric(history,"accuracy")
```

```
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_features (DenseFeature multiple                  64        
_________________________________________________________________
dense (Dense)                multiple                  3008      
_________________________________________________________________
dense_1 (Dense)              multiple                  4160      
_________________________________________________________________
dense_2 (Dense)              multiple                  65        
=================================================================
Total params: 7,297
Trainable params: 7,297
Non-trainable params: 0
_________________________________________________________________
```


![](./data/5-2-01-æ¨¡å‹è¯„ä¼°.jpg)

```python

```

å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·"Pythonä¸ç®—æ³•ä¹‹ç¾"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚

![image.png](./data/Pythonä¸ç®—æ³•ä¹‹ç¾logo.jpg)



# 5-3,æ¿€æ´»å‡½æ•°activation

æ¿€æ´»å‡½æ•°åœ¨æ·±åº¦å­¦ä¹ ä¸­æ‰®æ¼”ç€éå¸¸é‡è¦çš„è§’è‰²ï¼Œå®ƒç»™ç½‘ç»œèµ‹äºˆäº†éçº¿æ€§ï¼Œä»è€Œä½¿å¾—ç¥ç»ç½‘ç»œèƒ½å¤Ÿæ‹Ÿåˆä»»æ„å¤æ‚çš„å‡½æ•°ã€‚

å¦‚æœæ²¡æœ‰æ¿€æ´»å‡½æ•°ï¼Œæ— è®ºå¤šå¤æ‚çš„ç½‘ç»œï¼Œéƒ½ç­‰ä»·äºå•ä¸€çš„çº¿æ€§å˜æ¢ï¼Œæ— æ³•å¯¹éçº¿æ€§å‡½æ•°è¿›è¡Œæ‹Ÿåˆã€‚

ç›®å‰ï¼Œæ·±åº¦å­¦ä¹ ä¸­æœ€æµè¡Œçš„æ¿€æ´»å‡½æ•°ä¸º relu, ä½†ä¹Ÿæœ‰äº›æ–°æ¨å‡ºçš„æ¿€æ´»å‡½æ•°ï¼Œä¾‹å¦‚ swishã€GELU æ®ç§°æ•ˆæœä¼˜äºreluæ¿€æ´»å‡½æ•°ã€‚

æ¿€æ´»å‡½æ•°çš„ç»¼è¿°ä»‹ç»å¯ä»¥å‚è€ƒä¸‹é¢ä¸¤ç¯‡æ–‡ç« ã€‚

[ã€Šä¸€æ–‡æ¦‚è§ˆæ·±åº¦å­¦ä¹ ä¸­çš„æ¿€æ´»å‡½æ•°ã€‹](https://zhuanlan.zhihu.com/p/98472075)

https://zhuanlan.zhihu.com/p/98472075

[ã€Šä»ReLUåˆ°GELU,ä¸€æ–‡æ¦‚è§ˆç¥ç»ç½‘ç»œä¸­çš„æ¿€æ´»å‡½æ•°ã€‹](https://zhuanlan.zhihu.com/p/98863801)

https://zhuanlan.zhihu.com/p/98863801



### ä¸€ï¼Œå¸¸ç”¨æ¿€æ´»å‡½æ•°


* tf.nn.sigmoidï¼šå°†å®æ•°å‹ç¼©åˆ°0åˆ°1ä¹‹é—´ï¼Œä¸€èˆ¬åªåœ¨äºŒåˆ†ç±»çš„æœ€åè¾“å‡ºå±‚ä½¿ç”¨ã€‚ä¸»è¦ç¼ºé™·ä¸ºå­˜åœ¨æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Œè®¡ç®—å¤æ‚åº¦é«˜ï¼Œè¾“å‡ºä¸ä»¥0ä¸ºä¸­å¿ƒã€‚

![](./data/sigmoid.png)

* tf.nn.softmaxï¼šsigmoidçš„å¤šåˆ†ç±»æ‰©å±•ï¼Œä¸€èˆ¬åªåœ¨å¤šåˆ†ç±»é—®é¢˜çš„æœ€åè¾“å‡ºå±‚ä½¿ç”¨ã€‚

![](./data/softmaxè¯´æ˜.jpg)

* tf.nn.tanhï¼šå°†å®æ•°å‹ç¼©åˆ°-1åˆ°1ä¹‹é—´ï¼Œè¾“å‡ºæœŸæœ›ä¸º0ã€‚ä¸»è¦ç¼ºé™·ä¸ºå­˜åœ¨æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Œè®¡ç®—å¤æ‚åº¦é«˜ã€‚

![](./data/tanh.png)

* tf.nn.reluï¼šä¿®æ­£çº¿æ€§å•å…ƒï¼Œæœ€æµè¡Œçš„æ¿€æ´»å‡½æ•°ã€‚ä¸€èˆ¬éšè—å±‚ä½¿ç”¨ã€‚ä¸»è¦ç¼ºé™·æ˜¯ï¼šè¾“å‡ºä¸ä»¥0ä¸ºä¸­å¿ƒï¼Œè¾“å…¥å°äº0æ—¶å­˜åœ¨æ¢¯åº¦æ¶ˆå¤±é—®é¢˜(æ­»äº¡relu)ã€‚

![](./data/relu.png)

* tf.nn.leaky_reluï¼šå¯¹ä¿®æ­£çº¿æ€§å•å…ƒçš„æ”¹è¿›ï¼Œè§£å†³äº†æ­»äº¡relué—®é¢˜ã€‚

![](./data/leaky_relu.png)

* tf.nn.eluï¼šæŒ‡æ•°çº¿æ€§å•å…ƒã€‚å¯¹reluçš„æ”¹è¿›ï¼Œèƒ½å¤Ÿç¼“è§£æ­»äº¡relué—®é¢˜ã€‚

![](./data/elu.png)

* tf.nn.seluï¼šæ‰©å±•å‹æŒ‡æ•°çº¿æ€§å•å…ƒã€‚åœ¨æƒé‡ç”¨tf.keras.initializers.lecun_normalåˆå§‹åŒ–å‰æä¸‹èƒ½å¤Ÿå¯¹ç¥ç»ç½‘ç»œè¿›è¡Œè‡ªå½’ä¸€åŒ–ã€‚ä¸å¯èƒ½å‡ºç°æ¢¯åº¦çˆ†ç‚¸æˆ–è€…æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚éœ€è¦å’ŒDropoutçš„å˜ç§AlphaDropoutä¸€èµ·ä½¿ç”¨ã€‚

![](./data/selu.png)

* tf.nn.swishï¼šè‡ªé—¨æ§æ¿€æ´»å‡½æ•°ã€‚è°·æ­Œå‡ºå“ï¼Œç›¸å…³ç ”ç©¶æŒ‡å‡ºç”¨swishæ›¿ä»£reluå°†è·å¾—è½»å¾®æ•ˆæœæå‡ã€‚

![](./data/swish.png)

* geluï¼šé«˜æ–¯è¯¯å·®çº¿æ€§å•å…ƒæ¿€æ´»å‡½æ•°ã€‚åœ¨Transformerä¸­è¡¨ç°æœ€å¥½ã€‚tf.nnæ¨¡å—å°šæ²¡æœ‰å®ç°è¯¥å‡½æ•°ã€‚

![](./data/gelu.png)

```python

```

### äºŒï¼Œåœ¨æ¨¡å‹ä¸­ä½¿ç”¨æ¿€æ´»å‡½æ•°


åœ¨kerasæ¨¡å‹ä¸­ä½¿ç”¨æ¿€æ´»å‡½æ•°ä¸€èˆ¬æœ‰ä¸¤ç§æ–¹å¼ï¼Œä¸€ç§æ˜¯ä½œä¸ºæŸäº›å±‚çš„activationå‚æ•°æŒ‡å®šï¼Œå¦ä¸€ç§æ˜¯æ˜¾å¼æ·»åŠ layers.Activationæ¿€æ´»å±‚ã€‚

```python
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras import layers,models

tf.keras.backend.clear_session()

model = models.Sequential()
model.add(layers.Dense(32,input_shape = (None,16),activation = tf.nn.relu)) #é€šè¿‡activationå‚æ•°æŒ‡å®š
model.add(layers.Dense(10))
model.add(layers.Activation(tf.nn.softmax))  # æ˜¾å¼æ·»åŠ layers.Activationæ¿€æ´»å±‚
model.summary()

```

```python

```

å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·"Pythonä¸ç®—æ³•ä¹‹ç¾"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚

![image.png](./data/Pythonä¸ç®—æ³•ä¹‹ç¾logo.jpg)

```python

```
# 5-4,æ¨¡å‹å±‚layers

æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸€èˆ¬ç”±å„ç§æ¨¡å‹å±‚ç»„åˆè€Œæˆã€‚

tf.keras.layerså†…ç½®äº†éå¸¸ä¸°å¯Œçš„å„ç§åŠŸèƒ½çš„æ¨¡å‹å±‚ã€‚ä¾‹å¦‚ï¼Œ

layers.Dense,layers.Flatten,layers.Input,layers.DenseFeature,layers.Dropout

layers.Conv2D,layers.MaxPooling2D,layers.Conv1D

layers.Embedding,layers.GRU,layers.LSTM,layers.Bidirectionalç­‰ç­‰ã€‚

å¦‚æœè¿™äº›å†…ç½®æ¨¡å‹å±‚ä¸èƒ½å¤Ÿæ»¡è¶³éœ€æ±‚ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥é€šè¿‡ç¼–å†™tf.keras.LambdaåŒ¿åæ¨¡å‹å±‚æˆ–ç»§æ‰¿tf.keras.layers.LayeråŸºç±»æ„å»ºè‡ªå®šä¹‰çš„æ¨¡å‹å±‚ã€‚

å…¶ä¸­tf.keras.LambdaåŒ¿åæ¨¡å‹å±‚åªé€‚ç”¨äºæ„é€ æ²¡æœ‰å­¦ä¹ å‚æ•°çš„æ¨¡å‹å±‚ã€‚

```python

```

### ä¸€ï¼Œå†…ç½®æ¨¡å‹å±‚

```python

```

ä¸€äº›å¸¸ç”¨çš„å†…ç½®æ¨¡å‹å±‚ç®€å•ä»‹ç»å¦‚ä¸‹ã€‚

**åŸºç¡€å±‚**

* Denseï¼šå¯†é›†è¿æ¥å±‚ã€‚å‚æ•°ä¸ªæ•° = è¾“å…¥å±‚ç‰¹å¾æ•°Ã— è¾“å‡ºå±‚ç‰¹å¾æ•°(weight)ï¼‹ è¾“å‡ºå±‚ç‰¹å¾æ•°(bias)

* Activationï¼šæ¿€æ´»å‡½æ•°å±‚ã€‚ä¸€èˆ¬æ”¾åœ¨Denseå±‚åé¢ï¼Œç­‰ä»·äºåœ¨Denseå±‚ä¸­æŒ‡å®šactivationã€‚

* Dropoutï¼šéšæœºç½®é›¶å±‚ã€‚è®­ç»ƒæœŸé—´ä»¥ä¸€å®šå‡ ç‡å°†è¾“å…¥ç½®0ï¼Œä¸€ç§æ­£åˆ™åŒ–æ‰‹æ®µã€‚

* BatchNormalizationï¼šæ‰¹æ ‡å‡†åŒ–å±‚ã€‚é€šè¿‡çº¿æ€§å˜æ¢å°†è¾“å…¥æ‰¹æ¬¡ç¼©æ”¾å¹³ç§»åˆ°ç¨³å®šçš„å‡å€¼å’Œæ ‡å‡†å·®ã€‚å¯ä»¥å¢å¼ºæ¨¡å‹å¯¹è¾“å…¥ä¸åŒåˆ†å¸ƒçš„é€‚åº”æ€§ï¼ŒåŠ å¿«æ¨¡å‹è®­ç»ƒé€Ÿåº¦ï¼Œæœ‰è½»å¾®æ­£åˆ™åŒ–æ•ˆæœã€‚ä¸€èˆ¬åœ¨æ¿€æ´»å‡½æ•°ä¹‹å‰ä½¿ç”¨ã€‚

* SpatialDropout2Dï¼šç©ºé—´éšæœºç½®é›¶å±‚ã€‚è®­ç»ƒæœŸé—´ä»¥ä¸€å®šå‡ ç‡å°†æ•´ä¸ªç‰¹å¾å›¾ç½®0ï¼Œä¸€ç§æ­£åˆ™åŒ–æ‰‹æ®µï¼Œæœ‰åˆ©äºé¿å…ç‰¹å¾å›¾ä¹‹é—´è¿‡é«˜çš„ç›¸å…³æ€§ã€‚

* Inputï¼šè¾“å…¥å±‚ã€‚é€šå¸¸ä½¿ç”¨Functional APIæ–¹å¼æ„å»ºæ¨¡å‹æ—¶ä½œä¸ºç¬¬ä¸€å±‚ã€‚

* DenseFeatureï¼šç‰¹å¾åˆ—æ¥å…¥å±‚ï¼Œç”¨äºæ¥æ”¶ä¸€ä¸ªç‰¹å¾åˆ—åˆ—è¡¨å¹¶äº§ç”Ÿä¸€ä¸ªå¯†é›†è¿æ¥å±‚ã€‚

* Flattenï¼šå‹å¹³å±‚ï¼Œç”¨äºå°†å¤šç»´å¼ é‡å‹æˆä¸€ç»´ã€‚

* Reshapeï¼šå½¢çŠ¶é‡å¡‘å±‚ï¼Œæ”¹å˜è¾“å…¥å¼ é‡çš„å½¢çŠ¶ã€‚

* Concatenateï¼šæ‹¼æ¥å±‚ï¼Œå°†å¤šä¸ªå¼ é‡åœ¨æŸä¸ªç»´åº¦ä¸Šæ‹¼æ¥ã€‚

* Addï¼šåŠ æ³•å±‚ã€‚

* Subtractï¼š å‡æ³•å±‚ã€‚

* Maximumï¼šå–æœ€å¤§å€¼å±‚ã€‚

* Minimumï¼šå–æœ€å°å€¼å±‚ã€‚


**å·ç§¯ç½‘ç»œç›¸å…³å±‚**

* Conv1Dï¼šæ™®é€šä¸€ç»´å·ç§¯ï¼Œå¸¸ç”¨äºæ–‡æœ¬ã€‚å‚æ•°ä¸ªæ•° = è¾“å…¥é€šé“æ•°Ã—å·ç§¯æ ¸å°ºå¯¸(å¦‚3)Ã—å·ç§¯æ ¸ä¸ªæ•°

* Conv2Dï¼šæ™®é€šäºŒç»´å·ç§¯ï¼Œå¸¸ç”¨äºå›¾åƒã€‚å‚æ•°ä¸ªæ•° = è¾“å…¥é€šé“æ•°Ã—å·ç§¯æ ¸å°ºå¯¸(å¦‚3ä¹˜3)Ã—å·ç§¯æ ¸ä¸ªæ•°

* Conv3Dï¼šæ™®é€šä¸‰ç»´å·ç§¯ï¼Œå¸¸ç”¨äºè§†é¢‘ã€‚å‚æ•°ä¸ªæ•° = è¾“å…¥é€šé“æ•°Ã—å·ç§¯æ ¸å°ºå¯¸(å¦‚3ä¹˜3ä¹˜3)Ã—å·ç§¯æ ¸ä¸ªæ•°

* SeparableConv2Dï¼šäºŒç»´æ·±åº¦å¯åˆ†ç¦»å·ç§¯å±‚ã€‚ä¸åŒäºæ™®é€šå·ç§¯åŒæ—¶å¯¹åŒºåŸŸå’Œé€šé“æ“ä½œï¼Œæ·±åº¦å¯åˆ†ç¦»å·ç§¯å…ˆæ“ä½œåŒºåŸŸï¼Œå†æ“ä½œé€šé“ã€‚å³å…ˆå¯¹æ¯ä¸ªé€šé“åšç‹¬ç«‹å·å³å…ˆæ“ä½œåŒºåŸŸï¼Œå†ç”¨1ä¹˜1å·ç§¯è·¨é€šé“ç»„åˆå³å†æ“ä½œé€šé“ã€‚å‚æ•°ä¸ªæ•° = è¾“å…¥é€šé“æ•°Ã—å·ç§¯æ ¸å°ºå¯¸ + è¾“å…¥é€šé“æ•°Ã—1Ã—1Ã—è¾“å‡ºé€šé“æ•°ã€‚æ·±åº¦å¯åˆ†ç¦»å·ç§¯çš„å‚æ•°æ•°é‡ä¸€èˆ¬è¿œå°äºæ™®é€šå·ç§¯ï¼Œæ•ˆæœä¸€èˆ¬ä¹Ÿæ›´å¥½ã€‚

* DepthwiseConv2Dï¼šäºŒç»´æ·±åº¦å·ç§¯å±‚ã€‚ä»…æœ‰SeparableConv2Då‰åŠéƒ¨åˆ†æ“ä½œï¼Œå³åªæ“ä½œåŒºåŸŸï¼Œä¸æ“ä½œé€šé“ï¼Œä¸€èˆ¬è¾“å‡ºé€šé“æ•°å’Œè¾“å…¥é€šé“æ•°ç›¸åŒï¼Œä½†ä¹Ÿå¯ä»¥é€šè¿‡è®¾ç½®depth_multiplierè®©è¾“å‡ºé€šé“ä¸ºè¾“å…¥é€šé“çš„è‹¥å¹²å€æ•°ã€‚è¾“å‡ºé€šé“æ•° = è¾“å…¥é€šé“æ•° Ã— depth_multiplierã€‚å‚æ•°ä¸ªæ•° = è¾“å…¥é€šé“æ•°Ã—å·ç§¯æ ¸å°ºå¯¸Ã— depth_multiplierã€‚

* Conv2DTransposeï¼šäºŒç»´å·ç§¯è½¬ç½®å±‚ï¼Œä¿—ç§°åå·ç§¯å±‚ã€‚å¹¶éå·ç§¯çš„é€†æ“ä½œï¼Œä½†åœ¨å·ç§¯æ ¸ç›¸åŒçš„æƒ…å†µä¸‹ï¼Œå½“å…¶è¾“å…¥å°ºå¯¸æ˜¯å·ç§¯æ“ä½œè¾“å‡ºå°ºå¯¸çš„æƒ…å†µä¸‹ï¼Œå·ç§¯è½¬ç½®çš„è¾“å‡ºå°ºå¯¸æ°å¥½æ˜¯å·ç§¯æ“ä½œçš„è¾“å…¥å°ºå¯¸ã€‚

* LocallyConnected2D: äºŒç»´å±€éƒ¨è¿æ¥å±‚ã€‚ç±»ä¼¼Conv2Dï¼Œå”¯ä¸€çš„å·®åˆ«æ˜¯æ²¡æœ‰ç©ºé—´ä¸Šçš„æƒå€¼å…±äº«ï¼Œæ‰€ä»¥å…¶å‚æ•°ä¸ªæ•°è¿œé«˜äºäºŒç»´å·ç§¯ã€‚

* MaxPooling2D: äºŒç»´æœ€å¤§æ± åŒ–å±‚ã€‚ä¹Ÿç§°ä½œä¸‹é‡‡æ ·å±‚ã€‚æ± åŒ–å±‚æ— å‚æ•°ï¼Œä¸»è¦ä½œç”¨æ˜¯é™ç»´ã€‚

* AveragePooling2D: äºŒç»´å¹³å‡æ± åŒ–å±‚ã€‚

* GlobalMaxPool2D: å…¨å±€æœ€å¤§æ± åŒ–å±‚ã€‚æ¯ä¸ªé€šé“ä»…ä¿ç•™ä¸€ä¸ªå€¼ã€‚ä¸€èˆ¬ä»å·ç§¯å±‚è¿‡æ¸¡åˆ°å…¨è¿æ¥å±‚æ—¶ä½¿ç”¨ï¼Œæ˜¯Flattençš„æ›¿ä»£æ–¹æ¡ˆã€‚

* GlobalAvgPool2D: å…¨å±€å¹³å‡æ± åŒ–å±‚ã€‚æ¯ä¸ªé€šé“ä»…ä¿ç•™ä¸€ä¸ªå€¼ã€‚


**å¾ªç¯ç½‘ç»œç›¸å…³å±‚**

* Embeddingï¼šåµŒå…¥å±‚ã€‚ä¸€ç§æ¯”Onehotæ›´åŠ æœ‰æ•ˆçš„å¯¹ç¦»æ•£ç‰¹å¾è¿›è¡Œç¼–ç çš„æ–¹æ³•ã€‚ä¸€èˆ¬ç”¨äºå°†è¾“å…¥ä¸­çš„å•è¯æ˜ å°„ä¸ºç¨ å¯†å‘é‡ã€‚åµŒå…¥å±‚çš„å‚æ•°éœ€è¦å­¦ä¹ ã€‚

* LSTMï¼šé•¿çŸ­è®°å¿†å¾ªç¯ç½‘ç»œå±‚ã€‚æœ€æ™®éä½¿ç”¨çš„å¾ªç¯ç½‘ç»œå±‚ã€‚å…·æœ‰æºå¸¦è½¨é“ï¼Œé—å¿˜é—¨ï¼Œæ›´æ–°é—¨ï¼Œè¾“å‡ºé—¨ã€‚å¯ä»¥è¾ƒä¸ºæœ‰æ•ˆåœ°ç¼“è§£æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Œä»è€Œèƒ½å¤Ÿé€‚ç”¨é•¿æœŸä¾èµ–é—®é¢˜ã€‚è®¾ç½®return_sequences = Trueæ—¶å¯ä»¥è¿”å›å„ä¸ªä¸­é—´æ­¥éª¤è¾“å‡ºï¼Œå¦åˆ™åªè¿”å›æœ€ç»ˆè¾“å‡ºã€‚

* GRUï¼šé—¨æ§å¾ªç¯ç½‘ç»œå±‚ã€‚LSTMçš„ä½é…ç‰ˆï¼Œä¸å…·æœ‰æºå¸¦è½¨é“ï¼Œå‚æ•°æ•°é‡å°‘äºLSTMï¼Œè®­ç»ƒé€Ÿåº¦æ›´å¿«ã€‚

* SimpleRNNï¼šç®€å•å¾ªç¯ç½‘ç»œå±‚ã€‚å®¹æ˜“å­˜åœ¨æ¢¯åº¦æ¶ˆå¤±ï¼Œä¸èƒ½å¤Ÿé€‚ç”¨é•¿æœŸä¾èµ–é—®é¢˜ã€‚ä¸€èˆ¬è¾ƒå°‘ä½¿ç”¨ã€‚

* ConvLSTM2Dï¼šå·ç§¯é•¿çŸ­è®°å¿†å¾ªç¯ç½‘ç»œå±‚ã€‚ç»“æ„ä¸Šç±»ä¼¼LSTMï¼Œä½†å¯¹è¾“å…¥çš„è½¬æ¢æ“ä½œå’Œå¯¹çŠ¶æ€çš„è½¬æ¢æ“ä½œéƒ½æ˜¯å·ç§¯è¿ç®—ã€‚

* Bidirectionalï¼šåŒå‘å¾ªç¯ç½‘ç»œåŒ…è£…å™¨ã€‚å¯ä»¥å°†LSTMï¼ŒGRUç­‰å±‚åŒ…è£…æˆåŒå‘å¾ªç¯ç½‘ç»œã€‚ä»è€Œå¢å¼ºç‰¹å¾æå–èƒ½åŠ›ã€‚

* RNNï¼šRNNåŸºæœ¬å±‚ã€‚æ¥å—ä¸€ä¸ªå¾ªç¯ç½‘ç»œå•å…ƒæˆ–ä¸€ä¸ªå¾ªç¯å•å…ƒåˆ—è¡¨ï¼Œé€šè¿‡è°ƒç”¨tf.keras.backend.rnnå‡½æ•°åœ¨åºåˆ—ä¸Šè¿›è¡Œè¿­ä»£ä»è€Œè½¬æ¢æˆå¾ªç¯ç½‘ç»œå±‚ã€‚

* LSTMCellï¼šLSTMå•å…ƒã€‚å’ŒLSTMåœ¨æ•´ä¸ªåºåˆ—ä¸Šè¿­ä»£ç›¸æ¯”ï¼Œå®ƒä»…åœ¨åºåˆ—ä¸Šè¿­ä»£ä¸€æ­¥ã€‚å¯ä»¥ç®€å•ç†è§£LSTMå³RNNåŸºæœ¬å±‚åŒ…è£¹LSTMCellã€‚

* GRUCellï¼šGRUå•å…ƒã€‚å’ŒGRUåœ¨æ•´ä¸ªåºåˆ—ä¸Šè¿­ä»£ç›¸æ¯”ï¼Œå®ƒä»…åœ¨åºåˆ—ä¸Šè¿­ä»£ä¸€æ­¥ã€‚

* SimpleRNNCellï¼šSimpleRNNå•å…ƒã€‚å’ŒSimpleRNNåœ¨æ•´ä¸ªåºåˆ—ä¸Šè¿­ä»£ç›¸æ¯”ï¼Œå®ƒä»…åœ¨åºåˆ—ä¸Šè¿­ä»£ä¸€æ­¥ã€‚

* AbstractRNNCellï¼šæŠ½è±¡RNNå•å…ƒã€‚é€šè¿‡å¯¹å®ƒçš„å­ç±»åŒ–ç”¨æˆ·å¯ä»¥è‡ªå®šä¹‰RNNå•å…ƒï¼Œå†é€šè¿‡RNNåŸºæœ¬å±‚çš„åŒ…è£¹å®ç°ç”¨æˆ·è‡ªå®šä¹‰å¾ªç¯ç½‘ç»œå±‚ã€‚

* Attentionï¼šDot-productç±»å‹æ³¨æ„åŠ›æœºåˆ¶å±‚ã€‚å¯ä»¥ç”¨äºæ„å»ºæ³¨æ„åŠ›æ¨¡å‹ã€‚

* AdditiveAttentionï¼šAdditiveç±»å‹æ³¨æ„åŠ›æœºåˆ¶å±‚ã€‚å¯ä»¥ç”¨äºæ„å»ºæ³¨æ„åŠ›æ¨¡å‹ã€‚

* TimeDistributedï¼šæ—¶é—´åˆ†å¸ƒåŒ…è£…å™¨ã€‚åŒ…è£…åå¯ä»¥å°†Denseã€Conv2Dç­‰ä½œç”¨åˆ°æ¯ä¸€ä¸ªæ—¶é—´ç‰‡æ®µä¸Šã€‚

```python

```

### äºŒï¼Œè‡ªå®šä¹‰æ¨¡å‹å±‚


å¦‚æœè‡ªå®šä¹‰æ¨¡å‹å±‚æ²¡æœ‰éœ€è¦è¢«è®­ç»ƒçš„å‚æ•°ï¼Œä¸€èˆ¬æ¨èä½¿ç”¨Lamdaå±‚å®ç°ã€‚

å¦‚æœè‡ªå®šä¹‰æ¨¡å‹å±‚æœ‰éœ€è¦è¢«è®­ç»ƒçš„å‚æ•°ï¼Œåˆ™å¯ä»¥é€šè¿‡å¯¹LayeråŸºç±»å­ç±»åŒ–å®ç°ã€‚

Lamdaå±‚ç”±äºæ²¡æœ‰éœ€è¦è¢«è®­ç»ƒçš„å‚æ•°ï¼Œåªéœ€è¦å®šä¹‰æ­£å‘ä¼ æ’­é€»è¾‘å³å¯ï¼Œä½¿ç”¨æ¯”LayeråŸºç±»å­ç±»åŒ–æ›´åŠ ç®€å•ã€‚

Lamdaå±‚çš„æ­£å‘é€»è¾‘å¯ä»¥ä½¿ç”¨Pythonçš„lambdaå‡½æ•°æ¥è¡¨è¾¾ï¼Œä¹Ÿå¯ä»¥ç”¨defå…³é”®å­—å®šä¹‰å‡½æ•°æ¥è¡¨è¾¾ã€‚

```python
import tensorflow as tf
from tensorflow.keras import layers,models,regularizers

mypower = layers.Lambda(lambda x:tf.math.pow(x,2))
mypower(tf.range(5))
```

```
<tf.Tensor: shape=(5,), dtype=int32, numpy=array([ 0,  1,  4,  9, 16], dtype=int32)>
```


Layerçš„å­ç±»åŒ–ä¸€èˆ¬éœ€è¦é‡æ–°å®ç°åˆå§‹åŒ–æ–¹æ³•ï¼ŒBuildæ–¹æ³•å’ŒCallæ–¹æ³•ã€‚ä¸‹é¢æ˜¯ä¸€ä¸ªç®€åŒ–çš„çº¿æ€§å±‚çš„èŒƒä¾‹ï¼Œç±»ä¼¼Dense.

```python
class Linear(layers.Layer):
    def __init__(self, units=32, **kwargs):
        super(Linear, self).__init__(**kwargs)
        self.units = units

    #buildæ–¹æ³•ä¸€èˆ¬å®šä¹‰Layeréœ€è¦è¢«è®­ç»ƒçš„å‚æ•°ã€‚    
    def build(self, input_shape): 
        self.w = self.add_weight(shape=(input_shape[-1], self.units),
                                 initializer='random_normal',
                                 trainable=True)
        self.b = self.add_weight(shape=(self.units,),
                                 initializer='random_normal',
                                 trainable=True)
        super(Linear,self).build(input_shape) # ç›¸å½“äºè®¾ç½®self.built = True

    #callæ–¹æ³•ä¸€èˆ¬å®šä¹‰æ­£å‘ä¼ æ’­è¿ç®—é€»è¾‘ï¼Œ__call__æ–¹æ³•è°ƒç”¨äº†å®ƒã€‚    
    def call(self, inputs): 
        return tf.matmul(inputs, self.w) + self.b
    
    #å¦‚æœè¦è®©è‡ªå®šä¹‰çš„Layeré€šè¿‡Functional API ç»„åˆæˆæ¨¡å‹æ—¶å¯ä»¥åºåˆ—åŒ–ï¼Œéœ€è¦è‡ªå®šä¹‰get_configæ–¹æ³•ã€‚
    def get_config(self):  
        config = super(Linear, self).get_config()
        config.update({'units': self.units})
        return config

```

```python
linear = Linear(units = 8)
print(linear.built)
#æŒ‡å®šinput_shapeï¼Œæ˜¾å¼è°ƒç”¨buildæ–¹æ³•ï¼Œç¬¬0ç»´ä»£è¡¨æ ·æœ¬æ•°é‡ï¼Œç”¨Noneå¡«å……
linear.build(input_shape = (None,16)) 
print(linear.built)
```

```
False
True
```

```python
linear = Linear(units = 8)
print(linear.built)
linear.build(input_shape = (None,16)) 
print(linear.compute_output_shape(input_shape = (None,16)))
```

```
False
(None, 8)
```

```python
linear = Linear(units = 16)
print(linear.built)
#å¦‚æœbuilt = Falseï¼Œè°ƒç”¨__call__æ—¶ä¼šå…ˆè°ƒç”¨buildæ–¹æ³•, å†è°ƒç”¨callæ–¹æ³•ã€‚
linear(tf.random.uniform((100,64))) 
print(linear.built)
config = linear.get_config()
print(config)
```

```
False
True
{'name': 'linear_3', 'trainable': True, 'dtype': 'float32', 'units': 16}
```

```python
tf.keras.backend.clear_session()

model = models.Sequential()
#æ³¨æ„è¯¥å¤„çš„input_shapeä¼šè¢«æ¨¡å‹åŠ å·¥ï¼Œæ— éœ€ä½¿ç”¨Noneä»£è¡¨æ ·æœ¬æ•°é‡ç»´
model.add(Linear(units = 16,input_shape = (64,)))  
print("model.input_shape: ",model.input_shape)
print("model.output_shape: ",model.output_shape)
model.summary()
```

```
model.input_shape:  (None, 64)
model.output_shape:  (None, 16)
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
linear (Linear)              (None, 16)                1040      
=================================================================
Total params: 1,040
Trainable params: 1,040
Non-trainable params: 0
```

```python

```

å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·"Pythonä¸ç®—æ³•ä¹‹ç¾"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚

![image.png](./data/Pythonä¸ç®—æ³•ä¹‹ç¾logo.jpg)

```python

```
# 5-5,æŸå¤±å‡½æ•°losses

ä¸€èˆ¬æ¥è¯´ï¼Œç›‘ç£å­¦ä¹ çš„ç›®æ ‡å‡½æ•°ç”±æŸå¤±å‡½æ•°å’Œæ­£åˆ™åŒ–é¡¹ç»„æˆã€‚ï¼ˆObjective = Loss + Regularizationï¼‰

å¯¹äºkerasæ¨¡å‹ï¼Œç›®æ ‡å‡½æ•°ä¸­çš„æ­£åˆ™åŒ–é¡¹ä¸€èˆ¬åœ¨å„å±‚ä¸­æŒ‡å®šï¼Œä¾‹å¦‚ä½¿ç”¨Denseçš„ kernel_regularizer å’Œ bias_regularizerç­‰å‚æ•°æŒ‡å®šæƒé‡ä½¿ç”¨l1æˆ–è€…l2æ­£åˆ™åŒ–é¡¹ï¼Œæ­¤å¤–è¿˜å¯ä»¥ç”¨kernel_constraint å’Œ bias_constraintç­‰å‚æ•°çº¦æŸæƒé‡çš„å–å€¼èŒƒå›´ï¼Œè¿™ä¹Ÿæ˜¯ä¸€ç§æ­£åˆ™åŒ–æ‰‹æ®µã€‚

æŸå¤±å‡½æ•°åœ¨æ¨¡å‹ç¼–è¯‘æ—¶å€™æŒ‡å®šã€‚å¯¹äºå›å½’æ¨¡å‹ï¼Œé€šå¸¸ä½¿ç”¨çš„æŸå¤±å‡½æ•°æ˜¯å¹³æ–¹æŸå¤±å‡½æ•° mean_squared_errorã€‚

å¯¹äºäºŒåˆ†ç±»æ¨¡å‹ï¼Œé€šå¸¸ä½¿ç”¨çš„æ˜¯äºŒå…ƒäº¤å‰ç†µæŸå¤±å‡½æ•° binary_crossentropyã€‚

å¯¹äºå¤šåˆ†ç±»æ¨¡å‹ï¼Œå¦‚æœlabelæ˜¯ç±»åˆ«åºå·ç¼–ç çš„ï¼Œåˆ™ä½¿ç”¨ç±»åˆ«äº¤å‰ç†µæŸå¤±å‡½æ•° categorical_crossentropyã€‚å¦‚æœlabelè¿›è¡Œäº†one-hotç¼–ç ï¼Œåˆ™éœ€è¦ä½¿ç”¨ç¨€ç–ç±»åˆ«äº¤å‰ç†µæŸå¤±å‡½æ•° sparse_categorical_crossentropyã€‚

å¦‚æœæœ‰éœ€è¦ï¼Œä¹Ÿå¯ä»¥è‡ªå®šä¹‰æŸå¤±å‡½æ•°ï¼Œè‡ªå®šä¹‰æŸå¤±å‡½æ•°éœ€è¦æ¥æ”¶ä¸¤ä¸ªå¼ é‡y_true,y_predä½œä¸ºè¾“å…¥å‚æ•°ï¼Œå¹¶è¾“å‡ºä¸€ä¸ªæ ‡é‡ä½œä¸ºæŸå¤±å‡½æ•°å€¼ã€‚


```python
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras import layers,models,losses,regularizers,constraints
```

### ä¸€ï¼ŒæŸå¤±å‡½æ•°å’Œæ­£åˆ™åŒ–é¡¹

```python
tf.keras.backend.clear_session()

model = models.Sequential()
model.add(layers.Dense(64, input_dim=64,
                kernel_regularizer=regularizers.l2(0.01), 
                activity_regularizer=regularizers.l1(0.01),
                kernel_constraint = constraints.MaxNorm(max_value=2, axis=0))) 
model.add(layers.Dense(10,
        kernel_regularizer=regularizers.l1_l2(0.01,0.01),activation = "sigmoid"))
model.compile(optimizer = "rmsprop",
        loss = "sparse_categorical_crossentropy",metrics = ["AUC"])
model.summary()

```

```
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense (Dense)                (None, 64)                4160      
_________________________________________________________________
dense_1 (Dense)              (None, 10)                650       
=================================================================
Total params: 4,810
Trainable params: 4,810
Non-trainable params: 0
_________________________________________________________________
```


### äºŒï¼Œå†…ç½®æŸå¤±å‡½æ•°


å†…ç½®çš„æŸå¤±å‡½æ•°ä¸€èˆ¬æœ‰ç±»çš„å®ç°å’Œå‡½æ•°çš„å®ç°ä¸¤ç§å½¢å¼ã€‚

å¦‚ï¼šCategoricalCrossentropy å’Œ categorical_crossentropy éƒ½æ˜¯ç±»åˆ«äº¤å‰ç†µæŸå¤±å‡½æ•°ï¼Œå‰è€…æ˜¯ç±»çš„å®ç°å½¢å¼ï¼Œåè€…æ˜¯å‡½æ•°çš„å®ç°å½¢å¼ã€‚

å¸¸ç”¨çš„ä¸€äº›å†…ç½®æŸå¤±å‡½æ•°è¯´æ˜å¦‚ä¸‹ã€‚

* mean_squared_errorï¼ˆå¹³æ–¹å·®è¯¯å·®æŸå¤±ï¼Œç”¨äºå›å½’ï¼Œç®€å†™ä¸º mse, ç±»å®ç°å½¢å¼ä¸º MeanSquaredError å’Œ MSEï¼‰

* mean_absolute_error (ç»å¯¹å€¼è¯¯å·®æŸå¤±ï¼Œç”¨äºå›å½’ï¼Œç®€å†™ä¸º mae, ç±»å®ç°å½¢å¼ä¸º MeanAbsoluteError å’Œ MAE)

* mean_absolute_percentage_error (å¹³å‡ç™¾åˆ†æ¯”è¯¯å·®æŸå¤±ï¼Œç”¨äºå›å½’ï¼Œç®€å†™ä¸º mape, ç±»å®ç°å½¢å¼ä¸º MeanAbsolutePercentageError å’Œ MAPE)

* Huber(HuberæŸå¤±ï¼Œåªæœ‰ç±»å®ç°å½¢å¼ï¼Œç”¨äºå›å½’ï¼Œä»‹äºmseå’Œmaeä¹‹é—´ï¼Œå¯¹å¼‚å¸¸å€¼æ¯”è¾ƒé²æ£’ï¼Œç›¸å¯¹mseæœ‰ä¸€å®šçš„ä¼˜åŠ¿)

* binary_crossentropy(äºŒå…ƒäº¤å‰ç†µï¼Œç”¨äºäºŒåˆ†ç±»ï¼Œç±»å®ç°å½¢å¼ä¸º BinaryCrossentropy)

* categorical_crossentropy(ç±»åˆ«äº¤å‰ç†µï¼Œç”¨äºå¤šåˆ†ç±»ï¼Œè¦æ±‚labelä¸ºonehotç¼–ç ï¼Œç±»å®ç°å½¢å¼ä¸º CategoricalCrossentropy)

* sparse_categorical_crossentropy(ç¨€ç–ç±»åˆ«äº¤å‰ç†µï¼Œç”¨äºå¤šåˆ†ç±»ï¼Œè¦æ±‚labelä¸ºåºå·ç¼–ç å½¢å¼ï¼Œç±»å®ç°å½¢å¼ä¸º SparseCategoricalCrossentropy)

* hinge(åˆé¡µæŸå¤±å‡½æ•°ï¼Œç”¨äºäºŒåˆ†ç±»ï¼Œæœ€è‘—åçš„åº”ç”¨æ˜¯ä½œä¸ºæ”¯æŒå‘é‡æœºSVMçš„æŸå¤±å‡½æ•°ï¼Œç±»å®ç°å½¢å¼ä¸º Hinge)

* kld(ç›¸å¯¹ç†µæŸå¤±ï¼Œä¹Ÿå«KLæ•£åº¦ï¼Œå¸¸ç”¨äºæœ€å¤§æœŸæœ›ç®—æ³•EMçš„æŸå¤±å‡½æ•°ï¼Œä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒå·®å¼‚çš„ä¸€ç§ä¿¡æ¯åº¦é‡ã€‚ç±»å®ç°å½¢å¼ä¸º KLDivergence æˆ– KLD)

* cosine_similarity(ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œå¯ç”¨äºå¤šåˆ†ç±»ï¼Œç±»å®ç°å½¢å¼ä¸º CosineSimilarity)

```python

```

### ä¸‰ï¼Œè‡ªå®šä¹‰æŸå¤±å‡½æ•°


è‡ªå®šä¹‰æŸå¤±å‡½æ•°æ¥æ”¶ä¸¤ä¸ªå¼ é‡y_true,y_predä½œä¸ºè¾“å…¥å‚æ•°ï¼Œå¹¶è¾“å‡ºä¸€ä¸ªæ ‡é‡ä½œä¸ºæŸå¤±å‡½æ•°å€¼ã€‚

ä¹Ÿå¯ä»¥å¯¹tf.keras.losses.Lossè¿›è¡Œå­ç±»åŒ–ï¼Œé‡å†™callæ–¹æ³•å®ç°æŸå¤±çš„è®¡ç®—é€»è¾‘ï¼Œä»è€Œå¾—åˆ°æŸå¤±å‡½æ•°çš„ç±»çš„å®ç°ã€‚

ä¸‹é¢æ˜¯ä¸€ä¸ªFocal Lossçš„è‡ªå®šä¹‰å®ç°ç¤ºèŒƒã€‚Focal Lossæ˜¯ä¸€ç§å¯¹binary_crossentropyçš„æ”¹è¿›æŸå¤±å‡½æ•°å½¢å¼ã€‚

åœ¨ç±»åˆ«ä¸å¹³è¡¡å’Œå­˜åœ¨éš¾ä»¥è®­ç»ƒæ ·æœ¬çš„æƒ…å½¢ä¸‹ç›¸å¯¹äºäºŒå…ƒäº¤å‰ç†µèƒ½å¤Ÿå–å¾—æ›´å¥½çš„æ•ˆæœã€‚

è¯¦è§ã€Šå¦‚ä½•è¯„ä»·Kaimingçš„Focal Loss for Dense Object Detectionï¼Ÿã€‹

https://www.zhihu.com/question/63581984

```python
def focal_loss(gamma=2., alpha=.25):
    
    def focal_loss_fixed(y_true, y_pred):
        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))
        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))
        loss = -tf.sum(alpha * tf.pow(1. - pt_1, gamma) * tf.log(1e-07+pt_1)) \
           -tf.sum((1-alpha) * tf.pow( pt_0, gamma) * tf.log(1. - pt_0 + 1e-07))
        return loss
    return focal_loss_fixed

```

```python
class FocalLoss(losses.Loss):
    
    def __init__(self,gamma=2.0,alpha=0.25):
        self.gamma = gamma
        self.alpha = alpha

    def call(self,y_true,y_pred):
        
        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))
        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))
        loss = -tf.sum(self.alpha * tf.pow(1. - pt_1, self.gamma) * tf.log(1e-07+pt_1)) \
           -tf.sum((1-self.alpha) * tf.pow( pt_0, self.gamma) * tf.log(1. - pt_0 + 1e-07))
        return loss
```

```python

```

å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·"Pythonä¸ç®—æ³•ä¹‹ç¾"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚

![image.png](./data/Pythonä¸ç®—æ³•ä¹‹ç¾logo.jpg)

```python

```
# 5-6,è¯„ä¼°æŒ‡æ ‡metrics

æŸå¤±å‡½æ•°é™¤äº†ä½œä¸ºæ¨¡å‹è®­ç»ƒæ—¶å€™çš„ä¼˜åŒ–ç›®æ ‡ï¼Œä¹Ÿèƒ½å¤Ÿä½œä¸ºæ¨¡å‹å¥½åçš„ä¸€ç§è¯„ä»·æŒ‡æ ‡ã€‚ä½†é€šå¸¸äººä»¬è¿˜ä¼šä»å…¶å®ƒè§’åº¦è¯„ä¼°æ¨¡å‹çš„å¥½åã€‚

è¿™å°±æ˜¯è¯„ä¼°æŒ‡æ ‡ã€‚é€šå¸¸æŸå¤±å‡½æ•°éƒ½å¯ä»¥ä½œä¸ºè¯„ä¼°æŒ‡æ ‡ï¼Œå¦‚MAE,MSE,CategoricalCrossentropyç­‰ä¹Ÿæ˜¯å¸¸ç”¨çš„è¯„ä¼°æŒ‡æ ‡ã€‚

ä½†è¯„ä¼°æŒ‡æ ‡ä¸ä¸€å®šå¯ä»¥ä½œä¸ºæŸå¤±å‡½æ•°ï¼Œä¾‹å¦‚AUC,Accuracy,Precisionã€‚å› ä¸ºè¯„ä¼°æŒ‡æ ‡ä¸è¦æ±‚è¿ç»­å¯å¯¼ï¼Œè€ŒæŸå¤±å‡½æ•°é€šå¸¸è¦æ±‚è¿ç»­å¯å¯¼ã€‚

ç¼–è¯‘æ¨¡å‹æ—¶ï¼Œå¯ä»¥é€šè¿‡åˆ—è¡¨å½¢å¼æŒ‡å®šå¤šä¸ªè¯„ä¼°æŒ‡æ ‡ã€‚

å¦‚æœæœ‰éœ€è¦ï¼Œä¹Ÿå¯ä»¥è‡ªå®šä¹‰è¯„ä¼°æŒ‡æ ‡ã€‚

è‡ªå®šä¹‰è¯„ä¼°æŒ‡æ ‡éœ€è¦æ¥æ”¶ä¸¤ä¸ªå¼ é‡y_true,y_predä½œä¸ºè¾“å…¥å‚æ•°ï¼Œå¹¶è¾“å‡ºä¸€ä¸ªæ ‡é‡ä½œä¸ºè¯„ä¼°å€¼ã€‚

ä¹Ÿå¯ä»¥å¯¹tf.keras.metrics.Metricè¿›è¡Œå­ç±»åŒ–ï¼Œé‡å†™åˆå§‹åŒ–æ–¹æ³•, update_stateæ–¹æ³•, resultæ–¹æ³•å®ç°è¯„ä¼°æŒ‡æ ‡çš„è®¡ç®—é€»è¾‘ï¼Œä»è€Œå¾—åˆ°è¯„ä¼°æŒ‡æ ‡çš„ç±»çš„å®ç°å½¢å¼ã€‚

ç”±äºè®­ç»ƒçš„è¿‡ç¨‹é€šå¸¸æ˜¯åˆ†æ‰¹æ¬¡è®­ç»ƒçš„ï¼Œè€Œè¯„ä¼°æŒ‡æ ‡è¦è·‘å®Œä¸€ä¸ªepochæ‰èƒ½å¤Ÿå¾—åˆ°æ•´ä½“çš„æŒ‡æ ‡ç»“æœã€‚å› æ­¤ï¼Œç±»å½¢å¼çš„è¯„ä¼°æŒ‡æ ‡æ›´ä¸ºå¸¸è§ã€‚å³éœ€è¦ç¼–å†™åˆå§‹åŒ–æ–¹æ³•ä»¥åˆ›å»ºä¸è®¡ç®—æŒ‡æ ‡ç»“æœç›¸å…³çš„ä¸€äº›ä¸­é—´å˜é‡ï¼Œç¼–å†™update_stateæ–¹æ³•åœ¨æ¯ä¸ªbatchåæ›´æ–°ç›¸å…³ä¸­é—´å˜é‡çš„çŠ¶æ€ï¼Œç¼–å†™resultæ–¹æ³•è¾“å‡ºæœ€ç»ˆæŒ‡æ ‡ç»“æœã€‚

å¦‚æœç¼–å†™å‡½æ•°å½¢å¼çš„è¯„ä¼°æŒ‡æ ‡ï¼Œåˆ™åªèƒ½å–epochä¸­å„ä¸ªbatchè®¡ç®—çš„è¯„ä¼°æŒ‡æ ‡ç»“æœçš„å¹³å‡å€¼ä½œä¸ºæ•´ä¸ªepochä¸Šçš„è¯„ä¼°æŒ‡æ ‡ç»“æœï¼Œè¿™ä¸ªç»“æœé€šå¸¸ä¼šåç¦»æ‹¿æ•´ä¸ªepochæ•°æ®ä¸€æ¬¡è®¡ç®—çš„ç»“æœã€‚



### ä¸€ï¼Œå¸¸ç”¨çš„å†…ç½®è¯„ä¼°æŒ‡æ ‡


* MeanSquaredErrorï¼ˆå¹³æ–¹å·®è¯¯å·®ï¼Œç”¨äºå›å½’ï¼Œå¯ä»¥ç®€å†™ä¸ºMSEï¼Œå‡½æ•°å½¢å¼ä¸ºmseï¼‰

* MeanAbsoluteError (ç»å¯¹å€¼è¯¯å·®ï¼Œç”¨äºå›å½’ï¼Œå¯ä»¥ç®€å†™ä¸ºMAEï¼Œå‡½æ•°å½¢å¼ä¸ºmae)

* MeanAbsolutePercentageError (å¹³å‡ç™¾åˆ†æ¯”è¯¯å·®ï¼Œç”¨äºå›å½’ï¼Œå¯ä»¥ç®€å†™ä¸ºMAPEï¼Œå‡½æ•°å½¢å¼ä¸ºmape)

* RootMeanSquaredError (å‡æ–¹æ ¹è¯¯å·®ï¼Œç”¨äºå›å½’)

* Accuracy (å‡†ç¡®ç‡ï¼Œç”¨äºåˆ†ç±»ï¼Œå¯ä»¥ç”¨å­—ç¬¦ä¸²"Accuracy"è¡¨ç¤ºï¼ŒAccuracy=(TP+TN)/(TP+TN+FP+FN)ï¼Œè¦æ±‚y_trueå’Œy_predéƒ½ä¸ºç±»åˆ«åºå·ç¼–ç )

* Precision (ç²¾ç¡®ç‡ï¼Œç”¨äºäºŒåˆ†ç±»ï¼ŒPrecision = TP/(TP+FP))

* Recall (å¬å›ç‡ï¼Œç”¨äºäºŒåˆ†ç±»ï¼ŒRecall = TP/(TP+FN))

* TruePositives (çœŸæ­£ä¾‹ï¼Œç”¨äºäºŒåˆ†ç±»)

* TrueNegatives (çœŸè´Ÿä¾‹ï¼Œç”¨äºäºŒåˆ†ç±»)

* FalsePositives (å‡æ­£ä¾‹ï¼Œç”¨äºäºŒåˆ†ç±»)

* FalseNegatives (å‡è´Ÿä¾‹ï¼Œç”¨äºäºŒåˆ†ç±»)

* AUC(ROCæ›²çº¿(TPR vs FPR)ä¸‹çš„é¢ç§¯ï¼Œç”¨äºäºŒåˆ†ç±»ï¼Œç›´è§‚è§£é‡Šä¸ºéšæœºæŠ½å–ä¸€ä¸ªæ­£æ ·æœ¬å’Œä¸€ä¸ªè´Ÿæ ·æœ¬ï¼Œæ­£æ ·æœ¬çš„é¢„æµ‹å€¼å¤§äºè´Ÿæ ·æœ¬çš„æ¦‚ç‡)

* CategoricalAccuracyï¼ˆåˆ†ç±»å‡†ç¡®ç‡ï¼Œä¸Accuracyå«ä¹‰ç›¸åŒï¼Œè¦æ±‚y_true(label)ä¸ºonehotç¼–ç å½¢å¼ï¼‰

* SparseCategoricalAccuracy (ç¨€ç–åˆ†ç±»å‡†ç¡®ç‡ï¼Œä¸Accuracyå«ä¹‰ç›¸åŒï¼Œè¦æ±‚y_true(label)ä¸ºåºå·ç¼–ç å½¢å¼)

* MeanIoU (Intersection-Over-Unionï¼Œå¸¸ç”¨äºå›¾åƒåˆ†å‰²)

* TopKCategoricalAccuracy (å¤šåˆ†ç±»TopKå‡†ç¡®ç‡ï¼Œè¦æ±‚y_true(label)ä¸ºonehotç¼–ç å½¢å¼)

* SparseTopKCategoricalAccuracy (ç¨€ç–å¤šåˆ†ç±»TopKå‡†ç¡®ç‡ï¼Œè¦æ±‚y_true(label)ä¸ºåºå·ç¼–ç å½¢å¼)

* Mean (å¹³å‡å€¼)

* Sum (æ±‚å’Œ)

```python

```

```python

```

### äºŒï¼Œ è‡ªå®šä¹‰è¯„ä¼°æŒ‡æ ‡


æˆ‘ä»¬ä»¥é‡‘èé£æ§é¢†åŸŸå¸¸ç”¨çš„KSæŒ‡æ ‡ä¸ºä¾‹ï¼Œç¤ºèŒƒè‡ªå®šä¹‰è¯„ä¼°æŒ‡æ ‡ã€‚

KSæŒ‡æ ‡é€‚åˆäºŒåˆ†ç±»é—®é¢˜ï¼Œå…¶è®¡ç®—æ–¹å¼ä¸º KS=max(TPR-FPR).

å…¶ä¸­TPR=TP/(TP+FN) , FPR = FP/(FP+TN) 

TPRæ›²çº¿å®é™…ä¸Šå°±æ˜¯æ­£æ ·æœ¬çš„ç´¯ç§¯åˆ†å¸ƒæ›²çº¿(CDF)ï¼ŒFPRæ›²çº¿å®é™…ä¸Šå°±æ˜¯è´Ÿæ ·æœ¬çš„ç´¯ç§¯åˆ†å¸ƒæ›²çº¿(CDF)ã€‚

KSæŒ‡æ ‡å°±æ˜¯æ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬ç´¯ç§¯åˆ†å¸ƒæ›²çº¿å·®å€¼çš„æœ€å¤§å€¼ã€‚

![](./data/KS_curve.png)

```python
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras import layers,models,losses,metrics

#å‡½æ•°å½¢å¼çš„è‡ªå®šä¹‰è¯„ä¼°æŒ‡æ ‡
@tf.function
def ks(y_true,y_pred):
    y_true = tf.reshape(y_true,(-1,))
    y_pred = tf.reshape(y_pred,(-1,))
    length = tf.shape(y_true)[0]
    t = tf.math.top_k(y_pred,k = length,sorted = False)
    y_pred_sorted = tf.gather(y_pred,t.indices)
    y_true_sorted = tf.gather(y_true,t.indices)
    cum_positive_ratio = tf.truediv(
        tf.cumsum(y_true_sorted),tf.reduce_sum(y_true_sorted))
    cum_negative_ratio = tf.truediv(
        tf.cumsum(1 - y_true_sorted),tf.reduce_sum(1 - y_true_sorted))
    ks_value = tf.reduce_max(tf.abs(cum_positive_ratio - cum_negative_ratio)) 
    return ks_value
```

```python
y_true = tf.constant([[1],[1],[1],[0],[1],[1],[1],[0],[0],[0],[1],[0],[1],[0]])
y_pred = tf.constant([[0.6],[0.1],[0.4],[0.5],[0.7],[0.7],[0.7],
                      [0.4],[0.4],[0.5],[0.8],[0.3],[0.5],[0.3]])
tf.print(ks(y_true,y_pred))
```

```
0.625
```

```python
#ç±»å½¢å¼çš„è‡ªå®šä¹‰è¯„ä¼°æŒ‡æ ‡
class KS(metrics.Metric):
    
    def __init__(self, name = "ks", **kwargs):
        super(KS,self).__init__(name=name,**kwargs)
        self.true_positives = self.add_weight(
            name = "tp",shape = (101,), initializer = "zeros")
        self.false_positives = self.add_weight(
            name = "fp",shape = (101,), initializer = "zeros")
   
    @tf.function
    def update_state(self,y_true,y_pred):
        y_true = tf.cast(tf.reshape(y_true,(-1,)),tf.bool)
        y_pred = tf.cast(100*tf.reshape(y_pred,(-1,)),tf.int32)
        
        for i in tf.range(0,tf.shape(y_true)[0]):
            if y_true[i]:
                self.true_positives[y_pred[i]].assign(
                    self.true_positives[y_pred[i]]+1.0)
            else:
                self.false_positives[y_pred[i]].assign(
                    self.false_positives[y_pred[i]]+1.0)
        return (self.true_positives,self.false_positives)
    
    @tf.function
    def result(self):
        cum_positive_ratio = tf.truediv(
            tf.cumsum(self.true_positives),tf.reduce_sum(self.true_positives))
        cum_negative_ratio = tf.truediv(
            tf.cumsum(self.false_positives),tf.reduce_sum(self.false_positives))
        ks_value = tf.reduce_max(tf.abs(cum_positive_ratio - cum_negative_ratio)) 
        return ks_value

```

```python
y_true = tf.constant([[1],[1],[1],[0],[1],[1],[1],[0],[0],[0],[1],[0],[1],[0]])
y_pred = tf.constant([[0.6],[0.1],[0.4],[0.5],[0.7],[0.7],
                      [0.7],[0.4],[0.4],[0.5],[0.8],[0.3],[0.5],[0.3]])

myks = KS()
myks.update_state(y_true,y_pred)
tf.print(myks.result())

```

```
0.625
```

```python

```

å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·"Pythonä¸ç®—æ³•ä¹‹ç¾"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚

![image.png](./data/Pythonä¸ç®—æ³•ä¹‹ç¾logo.jpg)
# 5-7,ä¼˜åŒ–å™¨optimizers

æœºå™¨å­¦ä¹ ç•Œæœ‰ä¸€ç¾¤ç‚¼ä¸¹å¸ˆï¼Œä»–ä»¬æ¯å¤©çš„æ—¥å¸¸æ˜¯ï¼š

æ‹¿æ¥è¯æï¼ˆæ•°æ®ï¼‰ï¼Œæ¶èµ·å…«å¦ç‚‰ï¼ˆæ¨¡å‹ï¼‰ï¼Œç‚¹ç€å…­å‘³çœŸç«ï¼ˆä¼˜åŒ–ç®—æ³•ï¼‰ï¼Œå°±æ‘‡ç€è’²æ‰‡ç­‰ç€ä¸¹è¯å‡ºç‚‰äº†ã€‚

ä¸è¿‡ï¼Œå½“è¿‡å¨å­çš„éƒ½çŸ¥é“ï¼ŒåŒæ ·çš„é£Ÿæï¼ŒåŒæ ·çš„èœè°±ï¼Œä½†ç«å€™ä¸ä¸€æ ·äº†ï¼Œè¿™å‡ºæ¥çš„å£å‘³å¯æ˜¯åƒå·®ä¸‡åˆ«ã€‚ç«å°äº†å¤¹ç”Ÿï¼Œç«å¤§äº†æ˜“ç³Šï¼Œç«ä¸åŒ€åˆ™åŠç”ŸåŠç³Šã€‚

æœºå™¨å­¦ä¹ ä¹Ÿæ˜¯ä¸€æ ·ï¼Œæ¨¡å‹ä¼˜åŒ–ç®—æ³•çš„é€‰æ‹©ç›´æ¥å…³ç³»åˆ°æœ€ç»ˆæ¨¡å‹çš„æ€§èƒ½ã€‚æœ‰æ—¶å€™æ•ˆæœä¸å¥½ï¼Œæœªå¿…æ˜¯ç‰¹å¾çš„é—®é¢˜æˆ–è€…æ¨¡å‹è®¾è®¡çš„é—®é¢˜ï¼Œå¾ˆå¯èƒ½å°±æ˜¯ä¼˜åŒ–ç®—æ³•çš„é—®é¢˜ã€‚

æ·±åº¦å­¦ä¹ ä¼˜åŒ–ç®—æ³•å¤§æ¦‚ç»å†äº† SGD -> SGDM -> NAG ->Adagrad -> Adadelta(RMSprop) -> Adam -> Nadam è¿™æ ·çš„å‘å±•å†ç¨‹ã€‚

è¯¦è§ã€Šä¸€ä¸ªæ¡†æ¶çœ‹æ‡‚ä¼˜åŒ–ç®—æ³•ä¹‹å¼‚åŒ SGD/AdaGrad/Adamã€‹

https://zhuanlan.zhihu.com/p/32230623

å¯¹äºä¸€èˆ¬æ–°æ‰‹ç‚¼ä¸¹å¸ˆï¼Œä¼˜åŒ–å™¨ç›´æ¥ä½¿ç”¨Adamï¼Œå¹¶ä½¿ç”¨å…¶é»˜è®¤å‚æ•°å°±OKäº†ã€‚

ä¸€äº›çˆ±å†™è®ºæ–‡çš„ç‚¼ä¸¹å¸ˆç”±äºè¿½æ±‚è¯„ä¼°æŒ‡æ ‡æ•ˆæœï¼Œå¯èƒ½ä¼šåçˆ±å‰æœŸä½¿ç”¨Adamä¼˜åŒ–å™¨å¿«é€Ÿä¸‹é™ï¼ŒåæœŸä½¿ç”¨SGDå¹¶ç²¾è°ƒä¼˜åŒ–å™¨å‚æ•°å¾—åˆ°æ›´å¥½çš„ç»“æœã€‚

æ­¤å¤–ç›®å‰ä¹Ÿæœ‰ä¸€äº›å‰æ²¿çš„ä¼˜åŒ–ç®—æ³•ï¼Œæ®ç§°æ•ˆæœæ¯”Adamæ›´å¥½ï¼Œä¾‹å¦‚LazyAdam, Look-ahead, RAdam, Rangerç­‰.


```python

```

### ä¸€ï¼Œä¼˜åŒ–å™¨çš„ä½¿ç”¨


ä¼˜åŒ–å™¨ä¸»è¦ä½¿ç”¨apply_gradientsæ–¹æ³•ä¼ å…¥å˜é‡å’Œå¯¹åº”æ¢¯åº¦ä»è€Œæ¥å¯¹ç»™å®šå˜é‡è¿›è¡Œè¿­ä»£ï¼Œæˆ–è€…ç›´æ¥ä½¿ç”¨minimizeæ–¹æ³•å¯¹ç›®æ ‡å‡½æ•°è¿›è¡Œè¿­ä»£ä¼˜åŒ–ã€‚

å½“ç„¶ï¼Œæ›´å¸¸è§çš„ä½¿ç”¨æ˜¯åœ¨ç¼–è¯‘æ—¶å°†ä¼˜åŒ–å™¨ä¼ å…¥kerasçš„Model,é€šè¿‡è°ƒç”¨model.fitå®ç°å¯¹Lossçš„çš„è¿­ä»£ä¼˜åŒ–ã€‚

åˆå§‹åŒ–ä¼˜åŒ–å™¨æ—¶ä¼šåˆ›å»ºä¸€ä¸ªå˜é‡optimier.iterationsç”¨äºè®°å½•è¿­ä»£çš„æ¬¡æ•°ã€‚å› æ­¤ä¼˜åŒ–å™¨å’Œtf.Variableä¸€æ ·ï¼Œä¸€èˆ¬éœ€è¦åœ¨@tf.functionå¤–åˆ›å»ºã€‚

```python
import tensorflow as tf
import numpy as np 

#æ‰“å°æ—¶é—´åˆ†å‰²çº¿
@tf.function
def printbar():
    ts = tf.timestamp()
    today_ts = ts%(24*60*60)

    hour = tf.cast(today_ts//3600+8,tf.int32)%tf.constant(24)
    minite = tf.cast((today_ts%3600)//60,tf.int32)
    second = tf.cast(tf.floor(today_ts%60),tf.int32)
    
    def timeformat(m):
        if tf.strings.length(tf.strings.format("{}",m))==1:
            return(tf.strings.format("0{}",m))
        else:
            return(tf.strings.format("{}",m))
    
    timestring = tf.strings.join([timeformat(hour),timeformat(minite),
                timeformat(second)],separator = ":")
    tf.print("=========="*8,end = "")
    tf.print(timestring)
    
```

```python
# æ±‚f(x) = a*x**2 + b*x + cçš„æœ€å°å€¼

# ä½¿ç”¨optimizer.apply_gradients

x = tf.Variable(0.0,name = "x",dtype = tf.float32)
optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)

@tf.function
def minimizef():
    a = tf.constant(1.0)
    b = tf.constant(-2.0)
    c = tf.constant(1.0)
    
    while tf.constant(True): 
        with tf.GradientTape() as tape:
            y = a*tf.pow(x,2) + b*x + c
        dy_dx = tape.gradient(y,x)
        optimizer.apply_gradients(grads_and_vars=[(dy_dx,x)])
        
        #è¿­ä»£ç»ˆæ­¢æ¡ä»¶
        if tf.abs(dy_dx)<tf.constant(0.00001):
            break
            
        if tf.math.mod(optimizer.iterations,100)==0:
            printbar()
            tf.print("step = ",optimizer.iterations)
            tf.print("x = ", x)
            tf.print("")
                
    y = a*tf.pow(x,2) + b*x + c
    return y

tf.print("y =",minimizef())
tf.print("x =",x)
```

```python

```

```python
# æ±‚f(x) = a*x**2 + b*x + cçš„æœ€å°å€¼

# ä½¿ç”¨optimizer.minimize

x = tf.Variable(0.0,name = "x",dtype = tf.float32)
optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)   

def f():   
    a = tf.constant(1.0)
    b = tf.constant(-2.0)
    c = tf.constant(1.0)
    y = a*tf.pow(x,2)+b*x+c
    return(y)

@tf.function
def train(epoch = 1000):  
    for _ in tf.range(epoch):  
        optimizer.minimize(f,[x])
    tf.print("epoch = ",optimizer.iterations)
    return(f())

train(1000)
tf.print("y = ",f())
tf.print("x = ",x)

```

```python

```

```python
# æ±‚f(x) = a*x**2 + b*x + cçš„æœ€å°å€¼
# ä½¿ç”¨model.fit

tf.keras.backend.clear_session()

class FakeModel(tf.keras.models.Model):
    def __init__(self,a,b,c):
        super(FakeModel,self).__init__()
        self.a = a
        self.b = b
        self.c = c
    
    def build(self):
        self.x = tf.Variable(0.0,name = "x")
        self.built = True
    
    def call(self,features):
        loss  = self.a*(self.x)**2+self.b*(self.x)+self.c
        return(tf.ones_like(features)*loss)
    
def myloss(y_true,y_pred):
    return tf.reduce_mean(y_pred)

model = FakeModel(tf.constant(1.0),tf.constant(-2.0),tf.constant(1.0))

model.build()
model.summary()

model.compile(optimizer = 
              tf.keras.optimizers.SGD(learning_rate=0.01),loss = myloss)
history = model.fit(tf.zeros((100,2)),
                    tf.ones(100),batch_size = 1,epochs = 10)  #è¿­ä»£1000æ¬¡

```

```python
tf.print("x=",model.x)
tf.print("loss=",model(tf.constant(0.0)))
```

```python

```

### äºŒï¼Œå†…ç½®ä¼˜åŒ–å™¨


æ·±åº¦å­¦ä¹ ä¼˜åŒ–ç®—æ³•å¤§æ¦‚ç»å†äº† SGD -> SGDM -> NAG ->Adagrad -> Adadelta(RMSprop) -> Adam -> Nadam è¿™æ ·çš„å‘å±•å†ç¨‹ã€‚

åœ¨keras.optimizerså­æ¨¡å—ä¸­ï¼Œå®ƒä»¬åŸºæœ¬ä¸Šéƒ½æœ‰å¯¹åº”çš„ç±»çš„å®ç°ã€‚

* SGD, é»˜è®¤å‚æ•°ä¸ºçº¯SGD, è®¾ç½®momentumå‚æ•°ä¸ä¸º0å®é™…ä¸Šå˜æˆSGDM, è€ƒè™‘äº†ä¸€é˜¶åŠ¨é‡, è®¾ç½® nesterovä¸ºTrueåå˜æˆNAGï¼Œå³ Nesterov Acceleration Gradientï¼Œåœ¨è®¡ç®—æ¢¯åº¦æ—¶è®¡ç®—çš„æ˜¯å‘å‰èµ°ä¸€æ­¥æ‰€åœ¨ä½ç½®çš„æ¢¯åº¦ã€‚

* Adagrad, è€ƒè™‘äº†äºŒé˜¶åŠ¨é‡ï¼Œå¯¹äºä¸åŒçš„å‚æ•°æœ‰ä¸åŒçš„å­¦ä¹ ç‡ï¼Œå³è‡ªé€‚åº”å­¦ä¹ ç‡ã€‚ç¼ºç‚¹æ˜¯å­¦ä¹ ç‡å•è°ƒä¸‹é™ï¼Œå¯èƒ½åæœŸå­¦ä¹ é€Ÿç‡è¿‡æ…¢ä¹ƒè‡³æå‰åœæ­¢å­¦ä¹ ã€‚

* RMSprop, è€ƒè™‘äº†äºŒé˜¶åŠ¨é‡ï¼Œå¯¹äºä¸åŒçš„å‚æ•°æœ‰ä¸åŒçš„å­¦ä¹ ç‡ï¼Œå³è‡ªé€‚åº”å­¦ä¹ ç‡ï¼Œå¯¹Adagradè¿›è¡Œäº†ä¼˜åŒ–ï¼Œé€šè¿‡æŒ‡æ•°å¹³æ»‘åªè€ƒè™‘ä¸€å®šçª—å£å†…çš„äºŒé˜¶åŠ¨é‡ã€‚

* Adadelta, è€ƒè™‘äº†äºŒé˜¶åŠ¨é‡ï¼Œä¸RMSpropç±»ä¼¼ï¼Œä½†æ˜¯æ›´åŠ å¤æ‚ä¸€äº›ï¼Œè‡ªé€‚åº”æ€§æ›´å¼ºã€‚

* Adam, åŒæ—¶è€ƒè™‘äº†ä¸€é˜¶åŠ¨é‡å’ŒäºŒé˜¶åŠ¨é‡ï¼Œå¯ä»¥çœ‹æˆRMSpropä¸Šè¿›ä¸€æ­¥è€ƒè™‘äº†Momentumã€‚

* Nadam, åœ¨AdamåŸºç¡€ä¸Šè¿›ä¸€æ­¥è€ƒè™‘äº† Nesterov Accelerationã€‚

```python

```

å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·"Pythonä¸ç®—æ³•ä¹‹ç¾"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚

![image.png](./data/Pythonä¸ç®—æ³•ä¹‹ç¾logo.jpg)

```python

```

```python

```
# 5-8,å›è°ƒå‡½æ•°callbacks

tf.kerasçš„å›è°ƒå‡½æ•°å®é™…ä¸Šæ˜¯ä¸€ä¸ªç±»ï¼Œä¸€èˆ¬æ˜¯åœ¨model.fitæ—¶ä½œä¸ºå‚æ•°æŒ‡å®šï¼Œç”¨äºæ§åˆ¶åœ¨è®­ç»ƒè¿‡ç¨‹å¼€å§‹æˆ–è€…åœ¨è®­ç»ƒè¿‡ç¨‹ç»“æŸï¼Œåœ¨æ¯ä¸ªepochè®­ç»ƒå¼€å§‹æˆ–è€…è®­ç»ƒç»“æŸï¼Œåœ¨æ¯ä¸ªbatchè®­ç»ƒå¼€å§‹æˆ–è€…è®­ç»ƒç»“æŸæ—¶æ‰§è¡Œä¸€äº›æ“ä½œï¼Œä¾‹å¦‚æ”¶é›†ä¸€äº›æ—¥å¿—ä¿¡æ¯ï¼Œæ”¹å˜å­¦ä¹ ç‡ç­‰è¶…å‚æ•°ï¼Œæå‰ç»ˆæ­¢è®­ç»ƒè¿‡ç¨‹ç­‰ç­‰ã€‚

åŒæ ·åœ°ï¼Œé’ˆå¯¹model.evaluateæˆ–è€…model.predictä¹Ÿå¯ä»¥æŒ‡å®šcallbackså‚æ•°ï¼Œç”¨äºæ§åˆ¶åœ¨è¯„ä¼°æˆ–é¢„æµ‹å¼€å§‹æˆ–è€…ç»“æŸæ—¶ï¼Œåœ¨æ¯ä¸ªbatchå¼€å§‹æˆ–è€…ç»“æŸæ—¶æ‰§è¡Œä¸€äº›æ“ä½œï¼Œä½†è¿™ç§ç”¨æ³•ç›¸å¯¹å°‘è§ã€‚

å¤§éƒ¨åˆ†æ—¶å€™ï¼Œkeras.callbackså­æ¨¡å—ä¸­å®šä¹‰çš„å›è°ƒå‡½æ•°ç±»å·²ç»è¶³å¤Ÿä½¿ç”¨äº†ï¼Œå¦‚æœæœ‰ç‰¹å®šçš„éœ€è¦ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥é€šè¿‡å¯¹keras.callbacks.Callbackså®æ–½å­ç±»åŒ–æ„é€ è‡ªå®šä¹‰çš„å›è°ƒå‡½æ•°ã€‚

æ‰€æœ‰å›è°ƒå‡½æ•°éƒ½ç»§æ‰¿è‡³ keras.callbacks.CallbacksåŸºç±»ï¼Œæ‹¥æœ‰paramså’Œmodelè¿™ä¸¤ä¸ªå±æ€§ã€‚

å…¶ä¸­params æ˜¯ä¸€ä¸ªdictï¼Œè®°å½•äº† training parameters (eg. verbosity, batch size, number of epochs...).

modelå³å½“å‰å…³è”çš„æ¨¡å‹çš„å¼•ç”¨ã€‚

æ­¤å¤–ï¼Œå¯¹äºå›è°ƒç±»ä¸­çš„ä¸€äº›æ–¹æ³•å¦‚on_epoch_begin,on_batch_endï¼Œè¿˜ä¼šæœ‰ä¸€ä¸ªè¾“å…¥å‚æ•°logs, æä¾›æœ‰å…³å½“å‰epochæˆ–è€…batchçš„ä¸€äº›ä¿¡æ¯ï¼Œå¹¶èƒ½å¤Ÿè®°å½•è®¡ç®—ç»“æœï¼Œå¦‚æœmodel.fitæŒ‡å®šäº†å¤šä¸ªå›è°ƒå‡½æ•°ç±»ï¼Œè¿™äº›logså˜é‡å°†åœ¨è¿™äº›å›è°ƒå‡½æ•°ç±»çš„åŒåå‡½æ•°é—´ä¾é¡ºåºä¼ é€’ã€‚



### ä¸€ï¼Œå†…ç½®å›è°ƒå‡½æ•°


* BaseLoggerï¼š æ”¶é›†æ¯ä¸ªepochä¸Šmetricsåœ¨å„ä¸ªbatchä¸Šçš„å¹³å‡å€¼ï¼Œå¯¹stateful_metricså‚æ•°ä¸­çš„å¸¦ä¸­é—´çŠ¶æ€çš„æŒ‡æ ‡ç›´æ¥æ‹¿æœ€ç»ˆå€¼æ— éœ€å¯¹å„ä¸ªbatchå¹³å‡ï¼ŒæŒ‡æ ‡å‡å€¼ç»“æœå°†æ·»åŠ åˆ°logså˜é‡ä¸­ã€‚è¯¥å›è°ƒå‡½æ•°è¢«æ‰€æœ‰æ¨¡å‹é»˜è®¤æ·»åŠ ï¼Œä¸”æ˜¯ç¬¬ä¸€ä¸ªè¢«æ·»åŠ çš„ã€‚

* Historyï¼š å°†BaseLoggerè®¡ç®—çš„å„ä¸ªepochçš„metricsç»“æœè®°å½•åˆ°historyè¿™ä¸ªdictå˜é‡ä¸­ï¼Œå¹¶ä½œä¸ºmodel.fitçš„è¿”å›å€¼ã€‚è¯¥å›è°ƒå‡½æ•°è¢«æ‰€æœ‰æ¨¡å‹é»˜è®¤æ·»åŠ ï¼Œåœ¨BaseLoggerä¹‹åè¢«æ·»åŠ ã€‚

* EarlyStoppingï¼š å½“è¢«ç›‘æ§æŒ‡æ ‡åœ¨è®¾å®šçš„è‹¥å¹²ä¸ªepochåæ²¡æœ‰æå‡ï¼Œåˆ™æå‰ç»ˆæ­¢è®­ç»ƒã€‚

* TensorBoardï¼š ä¸ºTensorboardå¯è§†åŒ–ä¿å­˜æ—¥å¿—ä¿¡æ¯ã€‚æ”¯æŒè¯„ä¼°æŒ‡æ ‡ï¼Œè®¡ç®—å›¾ï¼Œæ¨¡å‹å‚æ•°ç­‰çš„å¯è§†åŒ–ã€‚

* ModelCheckpointï¼š åœ¨æ¯ä¸ªepochåä¿å­˜æ¨¡å‹ã€‚

* ReduceLROnPlateauï¼šå¦‚æœç›‘æ§æŒ‡æ ‡åœ¨è®¾å®šçš„è‹¥å¹²ä¸ªepochåæ²¡æœ‰æå‡ï¼Œåˆ™ä»¥ä¸€å®šçš„å› å­å‡å°‘å­¦ä¹ ç‡ã€‚

* TerminateOnNaNï¼šå¦‚æœé‡åˆ°lossä¸ºNaNï¼Œæå‰ç»ˆæ­¢è®­ç»ƒã€‚

* LearningRateSchedulerï¼šå­¦ä¹ ç‡æ§åˆ¶å™¨ã€‚ç»™å®šå­¦ä¹ ç‡lrå’Œepochçš„å‡½æ•°å…³ç³»ï¼Œæ ¹æ®è¯¥å‡½æ•°å…³ç³»åœ¨æ¯ä¸ªepochå‰è°ƒæ•´å­¦ä¹ ç‡ã€‚

* CSVLoggerï¼šå°†æ¯ä¸ªepochåçš„logsç»“æœè®°å½•åˆ°CSVæ–‡ä»¶ä¸­ã€‚

* ProgbarLoggerï¼šå°†æ¯ä¸ªepochåçš„logsç»“æœæ‰“å°åˆ°æ ‡å‡†è¾“å‡ºæµä¸­ã€‚



```python

```

### äºŒï¼Œè‡ªå®šä¹‰å›è°ƒå‡½æ•°


å¯ä»¥ä½¿ç”¨callbacks.LambdaCallbackç¼–å†™è¾ƒä¸ºç®€å•çš„å›è°ƒå‡½æ•°ï¼Œä¹Ÿå¯ä»¥é€šè¿‡å¯¹callbacks.Callbackå­ç±»åŒ–ç¼–å†™æ›´åŠ å¤æ‚çš„å›è°ƒå‡½æ•°é€»è¾‘ã€‚

å¦‚æœéœ€è¦æ·±å…¥å­¦ä¹ tf.Kerasä¸­çš„å›è°ƒå‡½æ•°ï¼Œä¸è¦çŠ¹è±«é˜…è¯»å†…ç½®å›è°ƒå‡½æ•°çš„æºä»£ç ã€‚

```python
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras import layers,models,losses,metrics,callbacks
import tensorflow.keras.backend as K 

```

```python
# ç¤ºèŒƒä½¿ç”¨LambdaCallbackç¼–å†™è¾ƒä¸ºç®€å•çš„å›è°ƒå‡½æ•°

import json
json_log = open('./data/keras_log.json', mode='wt', buffering=1)
json_logging_callback = callbacks.LambdaCallback(
    on_epoch_end=lambda epoch, logs: json_log.write(
        json.dumps(dict(epoch = epoch,**logs)) + '\n'),
    on_train_end=lambda logs: json_log.close()
)

```

```python
# ç¤ºèŒƒé€šè¿‡Callbackå­ç±»åŒ–ç¼–å†™å›è°ƒå‡½æ•°ï¼ˆLearningRateSchedulerçš„æºä»£ç ï¼‰

class LearningRateScheduler(callbacks.Callback):
    
    def __init__(self, schedule, verbose=0):
        super(LearningRateScheduler, self).__init__()
        self.schedule = schedule
        self.verbose = verbose

    def on_epoch_begin(self, epoch, logs=None):
        if not hasattr(self.model.optimizer, 'lr'):
            raise ValueError('Optimizer must have a "lr" attribute.')
        try:  
            lr = float(K.get_value(self.model.optimizer.lr))
            lr = self.schedule(epoch, lr)
        except TypeError:  # Support for old API for backward compatibility
            lr = self.schedule(epoch)
        if not isinstance(lr, (tf.Tensor, float, np.float32, np.float64)):
            raise ValueError('The output of the "schedule" function '
                             'should be float.')
        if isinstance(lr, ops.Tensor) and not lr.dtype.is_floating:
            raise ValueError('The dtype of Tensor should be float')
        K.set_value(self.model.optimizer.lr, K.get_value(lr))
        if self.verbose > 0:
            print('\nEpoch %05d: LearningRateScheduler reducing learning '
                 'rate to %s.' % (epoch + 1, lr))

    def on_epoch_end(self, epoch, logs=None):
        logs = logs or {}
        logs['lr'] = K.get_value(self.model.optimizer.lr)

```

```python

```

å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·"Pythonä¸ç®—æ³•ä¹‹ç¾"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚

![image.png](./data/Pythonä¸ç®—æ³•ä¹‹ç¾logo.jpg)
# å…­ã€TensorFlowçš„é«˜é˜¶API

TensorFlowçš„é«˜é˜¶APIä¸»è¦æ˜¯tensorflow.keras.models.

æœ¬ç« æˆ‘ä»¬ä¸»è¦è¯¦ç»†ä»‹ç»tensorflow.keras.modelsç›¸å…³çš„ä»¥ä¸‹å†…å®¹ã€‚

* æ¨¡å‹çš„æ„å»ºï¼ˆSequentialã€functional APIã€Modelå­ç±»åŒ–ï¼‰

* æ¨¡å‹çš„è®­ç»ƒï¼ˆå†…ç½®fitæ–¹æ³•ã€å†…ç½®train_on_batchæ–¹æ³•ã€è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ã€å•GPUè®­ç»ƒæ¨¡å‹ã€å¤šGPUè®­ç»ƒæ¨¡å‹ã€TPUè®­ç»ƒæ¨¡å‹ï¼‰

* æ¨¡å‹çš„éƒ¨ç½²ï¼ˆtensorflow servingéƒ¨ç½²æ¨¡å‹ã€ä½¿ç”¨spark(scala)è°ƒç”¨tensorflowæ¨¡å‹ï¼‰


å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·"Pythonä¸ç®—æ³•ä¹‹ç¾"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚

![image.png](./data/Pythonä¸ç®—æ³•ä¹‹ç¾logo.jpg)
# 6-1,æ„å»ºæ¨¡å‹çš„3ç§æ–¹æ³•

å¯ä»¥ä½¿ç”¨ä»¥ä¸‹3ç§æ–¹å¼æ„å»ºæ¨¡å‹ï¼šä½¿ç”¨SequentialæŒ‰å±‚é¡ºåºæ„å»ºæ¨¡å‹ï¼Œä½¿ç”¨å‡½æ•°å¼APIæ„å»ºä»»æ„ç»“æ„æ¨¡å‹ï¼Œç»§æ‰¿ModelåŸºç±»æ„å»ºè‡ªå®šä¹‰æ¨¡å‹ã€‚

å¯¹äºé¡ºåºç»“æ„çš„æ¨¡å‹ï¼Œä¼˜å…ˆä½¿ç”¨Sequentialæ–¹æ³•æ„å»ºã€‚

å¦‚æœæ¨¡å‹æœ‰å¤šè¾“å…¥æˆ–è€…å¤šè¾“å‡ºï¼Œæˆ–è€…æ¨¡å‹éœ€è¦å…±äº«æƒé‡ï¼Œæˆ–è€…æ¨¡å‹å…·æœ‰æ®‹å·®è¿æ¥ç­‰éé¡ºåºç»“æ„ï¼Œæ¨èä½¿ç”¨å‡½æ•°å¼APIè¿›è¡Œåˆ›å»ºã€‚

å¦‚æœæ— ç‰¹å®šå¿…è¦ï¼Œå°½å¯èƒ½é¿å…ä½¿ç”¨Modelå­ç±»åŒ–çš„æ–¹å¼æ„å»ºæ¨¡å‹ï¼Œè¿™ç§æ–¹å¼æä¾›äº†æå¤§çš„çµæ´»æ€§ï¼Œä½†ä¹Ÿæœ‰æ›´å¤§çš„æ¦‚ç‡å‡ºé”™ã€‚

ä¸‹é¢ä»¥IMDBç”µå½±è¯„è®ºçš„åˆ†ç±»é—®é¢˜ä¸ºä¾‹ï¼Œæ¼”ç¤º3ç§åˆ›å»ºæ¨¡å‹çš„æ–¹æ³•ã€‚

```python
import numpy as np 
import pandas as pd 
import tensorflow as tf
from tqdm import tqdm 
from tensorflow.keras import *


train_token_path = "./data/imdb/train_token.csv"
test_token_path = "./data/imdb/test_token.csv"

MAX_WORDS = 10000  # We will only consider the top 10,000 words in the dataset
MAX_LEN = 200  # We will cut reviews after 200 words
BATCH_SIZE = 20 

# æ„å»ºç®¡é“
def parse_line(line):
    t = tf.strings.split(line,"\t")
    label = tf.reshape(tf.cast(tf.strings.to_number(t[0]),tf.int32),(-1,))
    features = tf.cast(tf.strings.to_number(tf.strings.split(t[1]," ")),tf.int32)
    return (features,label)

ds_train=  tf.data.TextLineDataset(filenames = [train_token_path]) \
   .map(parse_line,num_parallel_calls = tf.data.experimental.AUTOTUNE) \
   .shuffle(buffer_size = 1000).batch(BATCH_SIZE) \
   .prefetch(tf.data.experimental.AUTOTUNE)

ds_test=  tf.data.TextLineDataset(filenames = [test_token_path]) \
   .map(parse_line,num_parallel_calls = tf.data.experimental.AUTOTUNE) \
   .shuffle(buffer_size = 1000).batch(BATCH_SIZE) \
   .prefetch(tf.data.experimental.AUTOTUNE)

```

```python

```

### ä¸€ï¼ŒSequentialæŒ‰å±‚é¡ºåºåˆ›å»ºæ¨¡å‹

```python
tf.keras.backend.clear_session()

model = models.Sequential()

model.add(layers.Embedding(MAX_WORDS,7,input_length=MAX_LEN))
model.add(layers.Conv1D(filters = 64,kernel_size = 5,activation = "relu"))
model.add(layers.MaxPool1D(2))
model.add(layers.Conv1D(filters = 32,kernel_size = 3,activation = "relu"))
model.add(layers.MaxPool1D(2))
model.add(layers.Flatten())
model.add(layers.Dense(1,activation = "sigmoid"))

model.compile(optimizer='Nadam',
            loss='binary_crossentropy',
            metrics=['accuracy',"AUC"])

model.summary()
```

![](./data/Sequentialæ¨¡å‹ç»“æ„.png)

```python
import datetime
baselogger = callbacks.BaseLogger(stateful_metrics=["AUC"])
logdir = "./data/keras_model/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)
history = model.fit(ds_train,validation_data = ds_test,
        epochs = 6,callbacks=[baselogger,tensorboard_callback])

```

```python
%matplotlib inline
%config InlineBackend.figure_format = 'svg'

import matplotlib.pyplot as plt

def plot_metric(history, metric):
    train_metrics = history.history[metric]
    val_metrics = history.history['val_'+metric]
    epochs = range(1, len(train_metrics) + 1)
    plt.plot(epochs, train_metrics, 'bo--')
    plt.plot(epochs, val_metrics, 'ro-')
    plt.title('Training and validation '+ metric)
    plt.xlabel("Epochs")
    plt.ylabel(metric)
    plt.legend(["train_"+metric, 'val_'+metric])
    plt.show()
```

```python
plot_metric(history,"AUC")
```

```python

```

![](./data/6-1-fitæ¨¡å‹.jpg)

```python

```

```python

```

### äºŒï¼Œå‡½æ•°å¼APIåˆ›å»ºä»»æ„ç»“æ„æ¨¡å‹

```python
tf.keras.backend.clear_session()

inputs = layers.Input(shape=[MAX_LEN])
x  = layers.Embedding(MAX_WORDS,7)(inputs)

branch1 = layers.SeparableConv1D(64,3,activation="relu")(x)
branch1 = layers.MaxPool1D(3)(branch1)
branch1 = layers.SeparableConv1D(32,3,activation="relu")(branch1)
branch1 = layers.GlobalMaxPool1D()(branch1)

branch2 = layers.SeparableConv1D(64,5,activation="relu")(x)
branch2 = layers.MaxPool1D(5)(branch2)
branch2 = layers.SeparableConv1D(32,5,activation="relu")(branch2)
branch2 = layers.GlobalMaxPool1D()(branch2)

branch3 = layers.SeparableConv1D(64,7,activation="relu")(x)
branch3 = layers.MaxPool1D(7)(branch3)
branch3 = layers.SeparableConv1D(32,7,activation="relu")(branch3)
branch3 = layers.GlobalMaxPool1D()(branch3)

concat = layers.Concatenate()([branch1,branch2,branch3])
outputs = layers.Dense(1,activation = "sigmoid")(concat)

model = models.Model(inputs = inputs,outputs = outputs)

model.compile(optimizer='Nadam',
            loss='binary_crossentropy',
            metrics=['accuracy',"AUC"])

model.summary()

```

```
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 200)]        0                                            
__________________________________________________________________________________________________
embedding (Embedding)           (None, 200, 7)       70000       input_1[0][0]                    
__________________________________________________________________________________________________
separable_conv1d (SeparableConv (None, 198, 64)      533         embedding[0][0]                  
__________________________________________________________________________________________________
separable_conv1d_2 (SeparableCo (None, 196, 64)      547         embedding[0][0]                  
__________________________________________________________________________________________________
separable_conv1d_4 (SeparableCo (None, 194, 64)      561         embedding[0][0]                  
__________________________________________________________________________________________________
max_pooling1d (MaxPooling1D)    (None, 66, 64)       0           separable_conv1d[0][0]           
__________________________________________________________________________________________________
max_pooling1d_1 (MaxPooling1D)  (None, 39, 64)       0           separable_conv1d_2[0][0]         
__________________________________________________________________________________________________
max_pooling1d_2 (MaxPooling1D)  (None, 27, 64)       0           separable_conv1d_4[0][0]         
__________________________________________________________________________________________________
separable_conv1d_1 (SeparableCo (None, 64, 32)       2272        max_pooling1d[0][0]              
__________________________________________________________________________________________________
separable_conv1d_3 (SeparableCo (None, 35, 32)       2400        max_pooling1d_1[0][0]            
__________________________________________________________________________________________________
separable_conv1d_5 (SeparableCo (None, 21, 32)       2528        max_pooling1d_2[0][0]            
__________________________________________________________________________________________________
global_max_pooling1d (GlobalMax (None, 32)           0           separable_conv1d_1[0][0]         
__________________________________________________________________________________________________
global_max_pooling1d_1 (GlobalM (None, 32)           0           separable_conv1d_3[0][0]         
__________________________________________________________________________________________________
global_max_pooling1d_2 (GlobalM (None, 32)           0           separable_conv1d_5[0][0]         
__________________________________________________________________________________________________
concatenate (Concatenate)       (None, 96)           0           global_max_pooling1d[0][0]       
                                                                 global_max_pooling1d_1[0][0]     
                                                                 global_max_pooling1d_2[0][0]     
__________________________________________________________________________________________________
dense (Dense)                   (None, 1)            97          concatenate[0][0]                
==================================================================================================
Total params: 78,938
Trainable params: 78,938
Non-trainable params: 0
__________________________________________________________________________________________________
```


![](./data/FunctionalAPIæ¨¡å‹ç»“æ„.png)

```python
import datetime
logdir = "./data/keras_model/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)
history = model.fit(ds_train,validation_data = ds_test,epochs = 6,callbacks=[tensorboard_callback])

```

```
Epoch 1/6
1000/1000 [==============================] - 32s 32ms/step - loss: 0.5527 - accuracy: 0.6758 - AUC: 0.7731 - val_loss: 0.3646 - val_accuracy: 0.8426 - val_AUC: 0.9192
Epoch 2/6
1000/1000 [==============================] - 24s 24ms/step - loss: 0.3024 - accuracy: 0.8737 - AUC: 0.9444 - val_loss: 0.3281 - val_accuracy: 0.8644 - val_AUC: 0.9350
Epoch 3/6
1000/1000 [==============================] - 24s 24ms/step - loss: 0.2158 - accuracy: 0.9159 - AUC: 0.9715 - val_loss: 0.3461 - val_accuracy: 0.8666 - val_AUC: 0.9363
Epoch 4/6
1000/1000 [==============================] - 24s 24ms/step - loss: 0.1492 - accuracy: 0.9464 - AUC: 0.9859 - val_loss: 0.4017 - val_accuracy: 0.8568 - val_AUC: 0.9311
Epoch 5/6
1000/1000 [==============================] - 24s 24ms/step - loss: 0.0944 - accuracy: 0.9696 - AUC: 0.9939 - val_loss: 0.4998 - val_accuracy: 0.8550 - val_AUC: 0.9233
Epoch 6/6
1000/1000 [==============================] - 26s 26ms/step - loss: 0.0526 - accuracy: 0.9865 - AUC: 0.9977 - val_loss: 0.6463 - val_accuracy: 0.8462 - val_AUC: 0.9138
```

```python
plot_metric(history,"AUC")
```

![](./data/6-1-2-train.jpg)

```python

```

### ä¸‰ï¼ŒModelå­ç±»åŒ–åˆ›å»ºè‡ªå®šä¹‰æ¨¡å‹

```python
# å…ˆè‡ªå®šä¹‰ä¸€ä¸ªæ®‹å·®æ¨¡å—ï¼Œä¸ºè‡ªå®šä¹‰Layer

class ResBlock(layers.Layer):
    def __init__(self, kernel_size, **kwargs):
        super(ResBlock, self).__init__(**kwargs)
        self.kernel_size = kernel_size
    
    def build(self,input_shape):
        self.conv1 = layers.Conv1D(filters=64,kernel_size=self.kernel_size,
                                   activation = "relu",padding="same")
        self.conv2 = layers.Conv1D(filters=32,kernel_size=self.kernel_size,
                                   activation = "relu",padding="same")
        self.conv3 = layers.Conv1D(filters=input_shape[-1],
                                   kernel_size=self.kernel_size,activation = "relu",padding="same")
        self.maxpool = layers.MaxPool1D(2)
        super(ResBlock,self).build(input_shape) # ç›¸å½“äºè®¾ç½®self.built = True
    
    def call(self, inputs):
        x = self.conv1(inputs)
        x = self.conv2(x)
        x = self.conv3(x)
        x = layers.Add()([inputs,x])
        x = self.maxpool(x)
        return x
    
    #å¦‚æœè¦è®©è‡ªå®šä¹‰çš„Layeré€šè¿‡Functional API ç»„åˆæˆæ¨¡å‹æ—¶å¯ä»¥åºåˆ—åŒ–ï¼Œéœ€è¦è‡ªå®šä¹‰get_configæ–¹æ³•ã€‚
    def get_config(self):  
        config = super(ResBlock, self).get_config()
        config.update({'kernel_size': self.kernel_size})
        return config
```

```python
# æµ‹è¯•ResBlock
resblock = ResBlock(kernel_size = 3)
resblock.build(input_shape = (None,200,7))
resblock.compute_output_shape(input_shape=(None,200,7))

```

```
TensorShape([None, 100, 7])
```

```python
# è‡ªå®šä¹‰æ¨¡å‹ï¼Œå®é™…ä¸Šä¹Ÿå¯ä»¥ä½¿ç”¨Sequentialæˆ–è€…FunctionalAPI

class ImdbModel(models.Model):
    def __init__(self):
        super(ImdbModel, self).__init__()
        
    def build(self,input_shape):
        self.embedding = layers.Embedding(MAX_WORDS,7)
        self.block1 = ResBlock(7)
        self.block2 = ResBlock(5)
        self.dense = layers.Dense(1,activation = "sigmoid")
        super(ImdbModel,self).build(input_shape)
    
    def call(self, x):
        x = self.embedding(x)
        x = self.block1(x)
        x = self.block2(x)
        x = layers.Flatten()(x)
        x = self.dense(x)
        return(x)

```

```python
tf.keras.backend.clear_session()

model = ImdbModel()
model.build(input_shape =(None,200))
model.summary()

model.compile(optimizer='Nadam',
            loss='binary_crossentropy',
            metrics=['accuracy',"AUC"])

```

```
Model: "imdb_model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        multiple                  70000     
_________________________________________________________________
res_block (ResBlock)         multiple                  19143     
_________________________________________________________________
res_block_1 (ResBlock)       multiple                  13703     
_________________________________________________________________
dense (Dense)                multiple                  351       
=================================================================
Total params: 103,197
Trainable params: 103,197
Non-trainable params: 0
_________________________________________________________________
```

```python

```

![](./data/Modelå­ç±»åŒ–æ¨¡å‹ç»“æ„.png)

```python

```

```python
import datetime

logdir = "./tflogs/keras_model/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)
history = model.fit(ds_train,validation_data = ds_test,
                    epochs = 6,callbacks=[tensorboard_callback])

```

```
Epoch 1/6
1000/1000 [==============================] - 47s 47ms/step - loss: 0.5629 - accuracy: 0.6618 - AUC: 0.7548 - val_loss: 0.3422 - val_accuracy: 0.8510 - val_AUC: 0.9286
Epoch 2/6
1000/1000 [==============================] - 43s 43ms/step - loss: 0.2648 - accuracy: 0.8903 - AUC: 0.9576 - val_loss: 0.3276 - val_accuracy: 0.8650 - val_AUC: 0.9410
Epoch 3/6
1000/1000 [==============================] - 42s 42ms/step - loss: 0.1573 - accuracy: 0.9439 - AUC: 0.9846 - val_loss: 0.3861 - val_accuracy: 0.8682 - val_AUC: 0.9390
Epoch 4/6
1000/1000 [==============================] - 42s 42ms/step - loss: 0.0849 - accuracy: 0.9706 - AUC: 0.9950 - val_loss: 0.5324 - val_accuracy: 0.8616 - val_AUC: 0.9292
Epoch 5/6
1000/1000 [==============================] - 43s 43ms/step - loss: 0.0393 - accuracy: 0.9876 - AUC: 0.9986 - val_loss: 0.7693 - val_accuracy: 0.8566 - val_AUC: 0.9132
Epoch 6/6
1000/1000 [==============================] - 44s 44ms/step - loss: 0.0222 - accuracy: 0.9926 - AUC: 0.9994 - val_loss: 0.9328 - val_accuracy: 0.8584 - val_AUC: 0.9052
```

```python
plot_metric(history,"AUC")
```

```python

```

![](./data/6-1-3-fitæ¨¡å‹.jpg)

```python

```

å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·"Pythonä¸ç®—æ³•ä¹‹ç¾"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚

![image.png](./data/Pythonä¸ç®—æ³•ä¹‹ç¾logo.jpg)

```python

```
# 6-2,è®­ç»ƒæ¨¡å‹çš„3ç§æ–¹æ³•

æ¨¡å‹çš„è®­ç»ƒä¸»è¦æœ‰å†…ç½®fitæ–¹æ³•ã€å†…ç½®tran_on_batchæ–¹æ³•ã€è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ã€‚

æ³¨ï¼šfit_generatoræ–¹æ³•åœ¨tf.kerasä¸­ä¸æ¨èä½¿ç”¨ï¼Œå…¶åŠŸèƒ½å·²ç»è¢«fitåŒ…å«ã€‚


```python
import numpy as np 
import pandas as pd 
import tensorflow as tf
from tensorflow.keras import * 

#æ‰“å°æ—¶é—´åˆ†å‰²çº¿
@tf.function
def printbar():
    ts = tf.timestamp()
    today_ts = ts%(24*60*60)

    hour = tf.cast(today_ts//3600+8,tf.int32)%tf.constant(24)
    minite = tf.cast((today_ts%3600)//60,tf.int32)
    second = tf.cast(tf.floor(today_ts%60),tf.int32)
    
    def timeformat(m):
        if tf.strings.length(tf.strings.format("{}",m))==1:
            return(tf.strings.format("0{}",m))
        else:
            return(tf.strings.format("{}",m))
    
    timestring = tf.strings.join([timeformat(hour),timeformat(minite),
                timeformat(second)],separator = ":")
    tf.print("=========="*8,end = "")
    tf.print(timestring)
    
```

```python
MAX_LEN = 300
BATCH_SIZE = 32
(x_train,y_train),(x_test,y_test) = datasets.reuters.load_data()
x_train = preprocessing.sequence.pad_sequences(x_train,maxlen=MAX_LEN)
x_test = preprocessing.sequence.pad_sequences(x_test,maxlen=MAX_LEN)

MAX_WORDS = x_train.max()+1
CAT_NUM = y_train.max()+1

ds_train = tf.data.Dataset.from_tensor_slices((x_train,y_train)) \
          .shuffle(buffer_size = 1000).batch(BATCH_SIZE) \
          .prefetch(tf.data.experimental.AUTOTUNE).cache()
   
ds_test = tf.data.Dataset.from_tensor_slices((x_test,y_test)) \
          .shuffle(buffer_size = 1000).batch(BATCH_SIZE) \
          .prefetch(tf.data.experimental.AUTOTUNE).cache()

```

```python

```

### ä¸€ï¼Œå†…ç½®fitæ–¹æ³•


è¯¥æ–¹æ³•åŠŸèƒ½éå¸¸å¼ºå¤§, æ”¯æŒå¯¹numpy array, tf.data.Datasetä»¥åŠ Python generatoræ•°æ®è¿›è¡Œè®­ç»ƒã€‚

å¹¶ä¸”å¯ä»¥é€šè¿‡è®¾ç½®å›è°ƒå‡½æ•°å®ç°å¯¹è®­ç»ƒè¿‡ç¨‹çš„å¤æ‚æ§åˆ¶é€»è¾‘ã€‚

```python
tf.keras.backend.clear_session()
def create_model():
    
    model = models.Sequential()
    model.add(layers.Embedding(MAX_WORDS,7,input_length=MAX_LEN))
    model.add(layers.Conv1D(filters = 64,kernel_size = 5,activation = "relu"))
    model.add(layers.MaxPool1D(2))
    model.add(layers.Conv1D(filters = 32,kernel_size = 3,activation = "relu"))
    model.add(layers.MaxPool1D(2))
    model.add(layers.Flatten())
    model.add(layers.Dense(CAT_NUM,activation = "softmax"))
    return(model)

def compile_model(model):
    model.compile(optimizer=optimizers.Nadam(),
                loss=losses.SparseCategoricalCrossentropy(),
                metrics=[metrics.SparseCategoricalAccuracy(),metrics.SparseTopKCategoricalAccuracy(5)]) 
    return(model)
 
model = create_model()
model.summary()
model = compile_model(model)

```

```
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        (None, 300, 7)            216874    
_________________________________________________________________
conv1d (Conv1D)              (None, 296, 64)           2304      
_________________________________________________________________
max_pooling1d (MaxPooling1D) (None, 148, 64)           0         
_________________________________________________________________
conv1d_1 (Conv1D)            (None, 146, 32)           6176      
_________________________________________________________________
max_pooling1d_1 (MaxPooling1 (None, 73, 32)            0         
_________________________________________________________________
flatten (Flatten)            (None, 2336)              0         
_________________________________________________________________
dense (Dense)                (None, 46)                107502    
=================================================================
Total params: 332,856
Trainable params: 332,856
Non-trainable params: 0
_________________________________________________________________
```

```python
history = model.fit(ds_train,validation_data = ds_test,epochs = 10)
```

```python

```

```
Train for 281 steps, validate for 71 steps
Epoch 1/10
281/281 [==============================] - 11s 37ms/step - loss: 2.0231 - sparse_categorical_accuracy: 0.4636 - sparse_top_k_categorical_accuracy: 0.7450 - val_loss: 1.7346 - val_sparse_categorical_accuracy: 0.5534 - val_sparse_top_k_categorical_accuracy: 0.7560
Epoch 2/10
281/281 [==============================] - 9s 31ms/step - loss: 1.5079 - sparse_categorical_accuracy: 0.6091 - sparse_top_k_categorical_accuracy: 0.7901 - val_loss: 1.5475 - val_sparse_categorical_accuracy: 0.6109 - val_sparse_top_k_categorical_accuracy: 0.7792
Epoch 3/10
281/281 [==============================] - 9s 33ms/step - loss: 1.2204 - sparse_categorical_accuracy: 0.6823 - sparse_top_k_categorical_accuracy: 0.8448 - val_loss: 1.5455 - val_sparse_categorical_accuracy: 0.6367 - val_sparse_top_k_categorical_accuracy: 0.8001
Epoch 4/10
281/281 [==============================] - 9s 33ms/step - loss: 0.9382 - sparse_categorical_accuracy: 0.7543 - sparse_top_k_categorical_accuracy: 0.9075 - val_loss: 1.6780 - val_sparse_categorical_accuracy: 0.6398 - val_sparse_top_k_categorical_accuracy: 0.8032
Epoch 5/10
281/281 [==============================] - 10s 34ms/step - loss: 0.6791 - sparse_categorical_accuracy: 0.8255 - sparse_top_k_categorical_accuracy: 0.9513 - val_loss: 1.9426 - val_sparse_categorical_accuracy: 0.6376 - val_sparse_top_k_categorical_accuracy: 0.7956
Epoch 6/10
281/281 [==============================] - 9s 33ms/step - loss: 0.5063 - sparse_categorical_accuracy: 0.8762 - sparse_top_k_categorical_accuracy: 0.9716 - val_loss: 2.2141 - val_sparse_categorical_accuracy: 0.6291 - val_sparse_top_k_categorical_accuracy: 0.7947
Epoch 7/10
281/281 [==============================] - 10s 37ms/step - loss: 0.4031 - sparse_categorical_accuracy: 0.9050 - sparse_top_k_categorical_accuracy: 0.9817 - val_loss: 2.4126 - val_sparse_categorical_accuracy: 0.6264 - val_sparse_top_k_categorical_accuracy: 0.7947
Epoch 8/10
281/281 [==============================] - 10s 35ms/step - loss: 0.3380 - sparse_categorical_accuracy: 0.9205 - sparse_top_k_categorical_accuracy: 0.9881 - val_loss: 2.5366 - val_sparse_categorical_accuracy: 0.6242 - val_sparse_top_k_categorical_accuracy: 0.7974
Epoch 9/10
281/281 [==============================] - 10s 36ms/step - loss: 0.2921 - sparse_categorical_accuracy: 0.9299 - sparse_top_k_categorical_accuracy: 0.9909 - val_loss: 2.6564 - val_sparse_categorical_accuracy: 0.6242 - val_sparse_top_k_categorical_accuracy: 0.7983
Epoch 10/10
281/281 [==============================] - 9s 30ms/step - loss: 0.2613 - sparse_categorical_accuracy: 0.9334 - sparse_top_k_categorical_accuracy: 0.9947 - val_loss: 2.7365 - val_sparse_categorical_accuracy: 0.6220 - val_sparse_top_k_categorical_accuracy: 0.8005
```

```python

```

### äºŒï¼Œå†…ç½®train_on_batchæ–¹æ³•


è¯¥å†…ç½®æ–¹æ³•ç›¸æ¯”è¾ƒfitæ–¹æ³•æ›´åŠ çµæ´»ï¼Œå¯ä»¥ä¸é€šè¿‡å›è°ƒå‡½æ•°è€Œç›´æ¥åœ¨æ‰¹æ¬¡å±‚æ¬¡ä¸Šæ›´åŠ ç²¾ç»†åœ°æ§åˆ¶è®­ç»ƒçš„è¿‡ç¨‹ã€‚

```python
tf.keras.backend.clear_session()

def create_model():
    model = models.Sequential()

    model.add(layers.Embedding(MAX_WORDS,7,input_length=MAX_LEN))
    model.add(layers.Conv1D(filters = 64,kernel_size = 5,activation = "relu"))
    model.add(layers.MaxPool1D(2))
    model.add(layers.Conv1D(filters = 32,kernel_size = 3,activation = "relu"))
    model.add(layers.MaxPool1D(2))
    model.add(layers.Flatten())
    model.add(layers.Dense(CAT_NUM,activation = "softmax"))
    return(model)

def compile_model(model):
    model.compile(optimizer=optimizers.Nadam(),
                loss=losses.SparseCategoricalCrossentropy(),
                metrics=[metrics.SparseCategoricalAccuracy(),metrics.SparseTopKCategoricalAccuracy(5)]) 
    return(model)
 
model = create_model()
model.summary()
model = compile_model(model)

```

```
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        (None, 300, 7)            216874    
_________________________________________________________________
conv1d (Conv1D)              (None, 296, 64)           2304      
_________________________________________________________________
max_pooling1d (MaxPooling1D) (None, 148, 64)           0         
_________________________________________________________________
conv1d_1 (Conv1D)            (None, 146, 32)           6176      
_________________________________________________________________
max_pooling1d_1 (MaxPooling1 (None, 73, 32)            0         
_________________________________________________________________
flatten (Flatten)            (None, 2336)              0         
_________________________________________________________________
dense (Dense)                (None, 46)                107502    
=================================================================
Total params: 332,856
Trainable params: 332,856
Non-trainable params: 0
_________________________________________________________________
```

```python
def train_model(model,ds_train,ds_valid,epoches):

    for epoch in tf.range(1,epoches+1):
        model.reset_metrics()
        
        # åœ¨åæœŸé™ä½å­¦ä¹ ç‡
        if epoch == 5:
            model.optimizer.lr.assign(model.optimizer.lr/2.0)
            tf.print("Lowering optimizer Learning Rate...\n\n")
        
        for x, y in ds_train:
            train_result = model.train_on_batch(x, y)

        for x, y in ds_valid:
            valid_result = model.test_on_batch(x, y,reset_metrics=False)
            
        if epoch%1 ==0:
            printbar()
            tf.print("epoch = ",epoch)
            print("train:",dict(zip(model.metrics_names,train_result)))
            print("valid:",dict(zip(model.metrics_names,valid_result)))
            print("")
```

```python
train_model(model,ds_train,ds_test,10)
```

```
================================================================================13:09:19
epoch =  1
train: {'loss': 0.82411176, 'sparse_categorical_accuracy': 0.77272725, 'sparse_top_k_categorical_accuracy': 0.8636364}
valid: {'loss': 1.9265995, 'sparse_categorical_accuracy': 0.5743544, 'sparse_top_k_categorical_accuracy': 0.75779164}

================================================================================13:09:27
epoch =  2
train: {'loss': 0.6006621, 'sparse_categorical_accuracy': 0.90909094, 'sparse_top_k_categorical_accuracy': 0.95454544}
valid: {'loss': 1.844159, 'sparse_categorical_accuracy': 0.6126447, 'sparse_top_k_categorical_accuracy': 0.7920748}

================================================================================13:09:35
epoch =  3
train: {'loss': 0.36935613, 'sparse_categorical_accuracy': 0.90909094, 'sparse_top_k_categorical_accuracy': 0.95454544}
valid: {'loss': 2.163433, 'sparse_categorical_accuracy': 0.63312554, 'sparse_top_k_categorical_accuracy': 0.8045414}

================================================================================13:09:42
epoch =  4
train: {'loss': 0.2304088, 'sparse_categorical_accuracy': 0.90909094, 'sparse_top_k_categorical_accuracy': 1.0}
valid: {'loss': 2.8911984, 'sparse_categorical_accuracy': 0.6344613, 'sparse_top_k_categorical_accuracy': 0.7978629}

Lowering optimizer Learning Rate...


================================================================================13:09:51
epoch =  5
train: {'loss': 0.111194365, 'sparse_categorical_accuracy': 0.95454544, 'sparse_top_k_categorical_accuracy': 1.0}
valid: {'loss': 3.6431572, 'sparse_categorical_accuracy': 0.6295637, 'sparse_top_k_categorical_accuracy': 0.7978629}

================================================================================13:09:59
epoch =  6
train: {'loss': 0.07741702, 'sparse_categorical_accuracy': 0.95454544, 'sparse_top_k_categorical_accuracy': 1.0}
valid: {'loss': 4.074161, 'sparse_categorical_accuracy': 0.6255565, 'sparse_top_k_categorical_accuracy': 0.794301}

================================================================================13:10:07
epoch =  7
train: {'loss': 0.056113098, 'sparse_categorical_accuracy': 1.0, 'sparse_top_k_categorical_accuracy': 1.0}
valid: {'loss': 4.4461513, 'sparse_categorical_accuracy': 0.6273375, 'sparse_top_k_categorical_accuracy': 0.79652715}

================================================================================13:10:17
epoch =  8
train: {'loss': 0.043448802, 'sparse_categorical_accuracy': 1.0, 'sparse_top_k_categorical_accuracy': 1.0}
valid: {'loss': 4.7687583, 'sparse_categorical_accuracy': 0.6224399, 'sparse_top_k_categorical_accuracy': 0.79741764}

================================================================================13:10:26
epoch =  9
train: {'loss': 0.035002146, 'sparse_categorical_accuracy': 1.0, 'sparse_top_k_categorical_accuracy': 1.0}
valid: {'loss': 5.130505, 'sparse_categorical_accuracy': 0.6175423, 'sparse_top_k_categorical_accuracy': 0.794301}

================================================================================13:10:34
epoch =  10
train: {'loss': 0.028303564, 'sparse_categorical_accuracy': 1.0, 'sparse_top_k_categorical_accuracy': 1.0}
valid: {'loss': 5.4559293, 'sparse_categorical_accuracy': 0.6148709, 'sparse_top_k_categorical_accuracy': 0.7947462}
```

```python

```

### ä¸‰ï¼Œè‡ªå®šä¹‰è®­ç»ƒå¾ªç¯


è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯æ— éœ€ç¼–è¯‘æ¨¡å‹ï¼Œç›´æ¥åˆ©ç”¨ä¼˜åŒ–å™¨æ ¹æ®æŸå¤±å‡½æ•°åå‘ä¼ æ’­è¿­ä»£å‚æ•°ï¼Œæ‹¥æœ‰æœ€é«˜çš„çµæ´»æ€§ã€‚

```python
tf.keras.backend.clear_session()

def create_model():
    
    model = models.Sequential()

    model.add(layers.Embedding(MAX_WORDS,7,input_length=MAX_LEN))
    model.add(layers.Conv1D(filters = 64,kernel_size = 5,activation = "relu"))
    model.add(layers.MaxPool1D(2))
    model.add(layers.Conv1D(filters = 32,kernel_size = 3,activation = "relu"))
    model.add(layers.MaxPool1D(2))
    model.add(layers.Flatten())
    model.add(layers.Dense(CAT_NUM,activation = "softmax"))
    return(model)

model = create_model()
model.summary()
```

```python
optimizer = optimizers.Nadam()
loss_func = losses.SparseCategoricalCrossentropy()

train_loss = metrics.Mean(name='train_loss')
train_metric = metrics.SparseCategoricalAccuracy(name='train_accuracy')

valid_loss = metrics.Mean(name='valid_loss')
valid_metric = metrics.SparseCategoricalAccuracy(name='valid_accuracy')

@tf.function
def train_step(model, features, labels):
    with tf.GradientTape() as tape:
        predictions = model(features,training = True)
        loss = loss_func(labels, predictions)
    gradients = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))

    train_loss.update_state(loss)
    train_metric.update_state(labels, predictions)
    

@tf.function
def valid_step(model, features, labels):
    predictions = model(features)
    batch_loss = loss_func(labels, predictions)
    valid_loss.update_state(batch_loss)
    valid_metric.update_state(labels, predictions)
    

def train_model(model,ds_train,ds_valid,epochs):
    for epoch in tf.range(1,epochs+1):
        
        for features, labels in ds_train:
            train_step(model,features,labels)

        for features, labels in ds_valid:
            valid_step(model,features,labels)

        logs = 'Epoch={},Loss:{},Accuracy:{},Valid Loss:{},Valid Accuracy:{}'
        
        if epoch%1 ==0:
            printbar()
            tf.print(tf.strings.format(logs,
            (epoch,train_loss.result(),train_metric.result(),valid_loss.result(),valid_metric.result())))
            tf.print("")
            
        train_loss.reset_states()
        valid_loss.reset_states()
        train_metric.reset_states()
        valid_metric.reset_states()

train_model(model,ds_train,ds_test,10)

```

```python

```

```
================================================================================13:12:03
Epoch=1,Loss:2.02051544,Accuracy:0.460253835,Valid Loss:1.75700927,Valid Accuracy:0.536954582

================================================================================13:12:09
Epoch=2,Loss:1.510795,Accuracy:0.610665798,Valid Loss:1.55349839,Valid Accuracy:0.616206586

================================================================================13:12:17
Epoch=3,Loss:1.19221532,Accuracy:0.696170092,Valid Loss:1.52315605,Valid Accuracy:0.651380241

================================================================================13:12:23
Epoch=4,Loss:0.90101546,Accuracy:0.766310394,Valid Loss:1.68327653,Valid Accuracy:0.648263574

================================================================================13:12:30
Epoch=5,Loss:0.655430496,Accuracy:0.831329346,Valid Loss:1.90872383,Valid Accuracy:0.641139805

================================================================================13:12:37
Epoch=6,Loss:0.492730737,Accuracy:0.877866864,Valid Loss:2.09966016,Valid Accuracy:0.63223511

================================================================================13:12:44
Epoch=7,Loss:0.391238362,Accuracy:0.904030263,Valid Loss:2.27431226,Valid Accuracy:0.625111282

================================================================================13:12:51
Epoch=8,Loss:0.327761739,Accuracy:0.922066331,Valid Loss:2.42568827,Valid Accuracy:0.617542326

================================================================================13:12:58
Epoch=9,Loss:0.285573095,Accuracy:0.930527747,Valid Loss:2.55942106,Valid Accuracy:0.612644672

================================================================================13:13:05
Epoch=10,Loss:0.255482465,Accuracy:0.936094403,Valid Loss:2.67789412,Valid Accuracy:0.612199485
```

```python

```

å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·"Pythonä¸ç®—æ³•ä¹‹ç¾"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚

![image.png](./data/Pythonä¸ç®—æ³•ä¹‹ç¾logo.jpg)

```python

```

```python

```
# 6-3,ä½¿ç”¨å•GPUè®­ç»ƒæ¨¡å‹

æ·±åº¦å­¦ä¹ çš„è®­ç»ƒè¿‡ç¨‹å¸¸å¸¸éå¸¸è€—æ—¶ï¼Œä¸€ä¸ªæ¨¡å‹è®­ç»ƒå‡ ä¸ªå°æ—¶æ˜¯å®¶å¸¸ä¾¿é¥­ï¼Œè®­ç»ƒå‡ å¤©ä¹Ÿæ˜¯å¸¸æœ‰çš„äº‹æƒ…ï¼Œæœ‰æ—¶å€™ç”šè‡³è¦è®­ç»ƒå‡ åå¤©ã€‚

è®­ç»ƒè¿‡ç¨‹çš„è€—æ—¶ä¸»è¦æ¥è‡ªäºä¸¤ä¸ªéƒ¨åˆ†ï¼Œä¸€éƒ¨åˆ†æ¥è‡ªæ•°æ®å‡†å¤‡ï¼Œå¦ä¸€éƒ¨åˆ†æ¥è‡ªå‚æ•°è¿­ä»£ã€‚

å½“æ•°æ®å‡†å¤‡è¿‡ç¨‹è¿˜æ˜¯æ¨¡å‹è®­ç»ƒæ—¶é—´çš„ä¸»è¦ç“¶é¢ˆæ—¶ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ›´å¤šè¿›ç¨‹æ¥å‡†å¤‡æ•°æ®ã€‚

å½“å‚æ•°è¿­ä»£è¿‡ç¨‹æˆä¸ºè®­ç»ƒæ—¶é—´çš„ä¸»è¦ç“¶é¢ˆæ—¶ï¼Œæˆ‘ä»¬é€šå¸¸çš„æ–¹æ³•æ˜¯åº”ç”¨GPUæˆ–è€…Googleçš„TPUæ¥è¿›è¡ŒåŠ é€Ÿã€‚

è¯¦è§ã€Šç”¨GPUåŠ é€ŸKerasæ¨¡å‹â€”â€”Colabå…è´¹GPUä½¿ç”¨æ”»ç•¥ã€‹

https://zhuanlan.zhihu.com/p/68509398


æ— è®ºæ˜¯å†…ç½®fitæ–¹æ³•ï¼Œè¿˜æ˜¯è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ï¼Œä»CPUåˆ‡æ¢æˆå•GPUè®­ç»ƒæ¨¡å‹éƒ½æ˜¯éå¸¸æ–¹ä¾¿çš„ï¼Œæ— éœ€æ›´æ”¹ä»»ä½•ä»£ç ã€‚å½“å­˜åœ¨å¯ç”¨çš„GPUæ—¶ï¼Œå¦‚æœä¸ç‰¹æ„æŒ‡å®šdeviceï¼Œtensorflowä¼šè‡ªåŠ¨ä¼˜å…ˆé€‰æ‹©ä½¿ç”¨GPUæ¥åˆ›å»ºå¼ é‡å’Œæ‰§è¡Œå¼ é‡è®¡ç®—ã€‚

ä½†å¦‚æœæ˜¯åœ¨å…¬å¸æˆ–è€…å­¦æ ¡å®éªŒå®¤çš„æœåŠ¡å™¨ç¯å¢ƒï¼Œå­˜åœ¨å¤šä¸ªGPUå’Œå¤šä¸ªä½¿ç”¨è€…æ—¶ï¼Œä¸ºäº†ä¸è®©å•ä¸ªåŒå­¦çš„ä»»åŠ¡å ç”¨å…¨éƒ¨GPUèµ„æºå¯¼è‡´å…¶ä»–åŒå­¦æ— æ³•ä½¿ç”¨ï¼ˆtensorflowé»˜è®¤è·å–å…¨éƒ¨GPUçš„å…¨éƒ¨å†…å­˜èµ„æºæƒé™ï¼Œä½†å®é™…ä¸Šåªä½¿ç”¨ä¸€ä¸ªGPUçš„éƒ¨åˆ†èµ„æºï¼‰ï¼Œæˆ‘ä»¬é€šå¸¸ä¼šåœ¨å¼€å¤´å¢åŠ ä»¥ä¸‹å‡ è¡Œä»£ç ä»¥æ§åˆ¶æ¯ä¸ªä»»åŠ¡ä½¿ç”¨çš„GPUç¼–å·å’Œæ˜¾å­˜å¤§å°ï¼Œä»¥ä¾¿å…¶ä»–åŒå­¦ä¹Ÿèƒ½å¤ŸåŒæ—¶è®­ç»ƒæ¨¡å‹ã€‚


åœ¨Colabç¬”è®°æœ¬ä¸­ï¼šä¿®æ”¹->ç¬”è®°æœ¬è®¾ç½®->ç¡¬ä»¶åŠ é€Ÿå™¨ ä¸­é€‰æ‹© GPU

æ³¨ï¼šä»¥ä¸‹ä»£ç åªèƒ½åœ¨Colab ä¸Šæ‰èƒ½æ­£ç¡®æ‰§è¡Œã€‚

å¯é€šè¿‡ä»¥ä¸‹colabé“¾æ¥æµ‹è¯•æ•ˆæœã€Štf_å•GPUã€‹ï¼š

https://colab.research.google.com/drive/1r5dLoeJq5z01sU72BX2M5UiNSkuxsEFe

```python
%tensorflow_version 2.x
import tensorflow as tf
print(tf.__version__)
```

```python
from tensorflow.keras import * 

#æ‰“å°æ—¶é—´åˆ†å‰²çº¿
@tf.function
def printbar():
    ts = tf.timestamp()
    today_ts = ts%(24*60*60)

    hour = tf.cast(today_ts//3600+8,tf.int32)%tf.constant(24)
    minite = tf.cast((today_ts%3600)//60,tf.int32)
    second = tf.cast(tf.floor(today_ts%60),tf.int32)
    
    def timeformat(m):
        if tf.strings.length(tf.strings.format("{}",m))==1:
            return(tf.strings.format("0{}",m))
        else:
            return(tf.strings.format("{}",m))
    
    timestring = tf.strings.join([timeformat(hour),timeformat(minite),
                timeformat(second)],separator = ":")
    tf.print("=========="*8,end = "")
    tf.print(timestring)
    
```

### ä¸€ï¼ŒGPUè®¾ç½®

```python
gpus = tf.config.list_physical_devices("GPU")

if gpus:
    gpu0 = gpus[0] #å¦‚æœæœ‰å¤šä¸ªGPUï¼Œä»…ä½¿ç”¨ç¬¬0ä¸ªGPU
    tf.config.experimental.set_memory_growth(gpu0, True) #è®¾ç½®GPUæ˜¾å­˜ç”¨é‡æŒ‰éœ€ä½¿ç”¨
    # æˆ–è€…ä¹Ÿå¯ä»¥è®¾ç½®GPUæ˜¾å­˜ä¸ºå›ºå®šä½¿ç”¨é‡(ä¾‹å¦‚ï¼š4G)
    #tf.config.experimental.set_virtual_device_configuration(gpu0,
    #    [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)]) 
    tf.config.set_visible_devices([gpu0],"GPU") 
```

æ¯”è¾ƒGPUå’ŒCPUçš„è®¡ç®—é€Ÿåº¦

```python
printbar()
with tf.device("/gpu:0"):
    tf.random.set_seed(0)
    a = tf.random.uniform((10000,100),minval = 0,maxval = 3.0)
    b = tf.random.uniform((100,100000),minval = 0,maxval = 3.0)
    c = a@b
    tf.print(tf.reduce_sum(tf.reduce_sum(c,axis = 0),axis=0))
printbar()
```

```
================================================================================17:37:01
2.24953778e+11
================================================================================17:37:01
```

```python
printbar()
with tf.device("/cpu:0"):
    tf.random.set_seed(0)
    a = tf.random.uniform((10000,100),minval = 0,maxval = 3.0)
    b = tf.random.uniform((100,100000),minval = 0,maxval = 3.0)
    c = a@b
    tf.print(tf.reduce_sum(tf.reduce_sum(c,axis = 0),axis=0))
printbar()
```

```
================================================================================17:37:34
2.24953795e+11
================================================================================17:37:40
```

```python

```

### äºŒï¼Œå‡†å¤‡æ•°æ®

```python
MAX_LEN = 300
BATCH_SIZE = 32
(x_train,y_train),(x_test,y_test) = datasets.reuters.load_data()
x_train = preprocessing.sequence.pad_sequences(x_train,maxlen=MAX_LEN)
x_test = preprocessing.sequence.pad_sequences(x_test,maxlen=MAX_LEN)

MAX_WORDS = x_train.max()+1
CAT_NUM = y_train.max()+1

ds_train = tf.data.Dataset.from_tensor_slices((x_train,y_train)) \
          .shuffle(buffer_size = 1000).batch(BATCH_SIZE) \
          .prefetch(tf.data.experimental.AUTOTUNE).cache()
   
ds_test = tf.data.Dataset.from_tensor_slices((x_test,y_test)) \
          .shuffle(buffer_size = 1000).batch(BATCH_SIZE) \
          .prefetch(tf.data.experimental.AUTOTUNE).cache()
          
```

```python

```

### ä¸‰ï¼Œå®šä¹‰æ¨¡å‹

```python
tf.keras.backend.clear_session()

def create_model():
    
    model = models.Sequential()

    model.add(layers.Embedding(MAX_WORDS,7,input_length=MAX_LEN))
    model.add(layers.Conv1D(filters = 64,kernel_size = 5,activation = "relu"))
    model.add(layers.MaxPool1D(2))
    model.add(layers.Conv1D(filters = 32,kernel_size = 3,activation = "relu"))
    model.add(layers.MaxPool1D(2))
    model.add(layers.Flatten())
    model.add(layers.Dense(CAT_NUM,activation = "softmax"))
    return(model)

model = create_model()
model.summary()

```

```
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        (None, 300, 7)            216874    
_________________________________________________________________
conv1d (Conv1D)              (None, 296, 64)           2304      
_________________________________________________________________
max_pooling1d (MaxPooling1D) (None, 148, 64)           0         
_________________________________________________________________
conv1d_1 (Conv1D)            (None, 146, 32)           6176      
_________________________________________________________________
max_pooling1d_1 (MaxPooling1 (None, 73, 32)            0         
_________________________________________________________________
flatten (Flatten)            (None, 2336)              0         
_________________________________________________________________
dense (Dense)                (None, 46)                107502    
=================================================================
Total params: 332,856
Trainable params: 332,856
Non-trainable params: 0
_________________________________________________________________
```

```python

```

### å››ï¼Œè®­ç»ƒæ¨¡å‹

```python
optimizer = optimizers.Nadam()
loss_func = losses.SparseCategoricalCrossentropy()

train_loss = metrics.Mean(name='train_loss')
train_metric = metrics.SparseCategoricalAccuracy(name='train_accuracy')

valid_loss = metrics.Mean(name='valid_loss')
valid_metric = metrics.SparseCategoricalAccuracy(name='valid_accuracy')

@tf.function
def train_step(model, features, labels):
    with tf.GradientTape() as tape:
        predictions = model(features,training = True)
        loss = loss_func(labels, predictions)
    gradients = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))

    train_loss.update_state(loss)
    train_metric.update_state(labels, predictions)
    
@tf.function
def valid_step(model, features, labels):
    predictions = model(features)
    batch_loss = loss_func(labels, predictions)
    valid_loss.update_state(batch_loss)
    valid_metric.update_state(labels, predictions)
    

def train_model(model,ds_train,ds_valid,epochs):
    for epoch in tf.range(1,epochs+1):
        
        for features, labels in ds_train:
            train_step(model,features,labels)

        for features, labels in ds_valid:
            valid_step(model,features,labels)

        logs = 'Epoch={},Loss:{},Accuracy:{},Valid Loss:{},Valid Accuracy:{}'
        
        if epoch%1 ==0:
            printbar()
            tf.print(tf.strings.format(logs,
            (epoch,train_loss.result(),train_metric.result(),valid_loss.result(),valid_metric.result())))
            tf.print("")
            
        train_loss.reset_states()
        valid_loss.reset_states()
        train_metric.reset_states()
        valid_metric.reset_states()

train_model(model,ds_train,ds_test,10)
```

```python

```

```
================================================================================17:13:26
Epoch=1,Loss:1.96735072,Accuracy:0.489200622,Valid Loss:1.64124215,Valid Accuracy:0.582813919

================================================================================17:13:28
Epoch=2,Loss:1.4640888,Accuracy:0.624805152,Valid Loss:1.5559175,Valid Accuracy:0.607747078

================================================================================17:13:30
Epoch=3,Loss:1.20681274,Accuracy:0.68581605,Valid Loss:1.58494771,Valid Accuracy:0.622439921

================================================================================17:13:31
Epoch=4,Loss:0.937500894,Accuracy:0.75361836,Valid Loss:1.77466083,Valid Accuracy:0.621994674

================================================================================17:13:33
Epoch=5,Loss:0.693960547,Accuracy:0.822199941,Valid Loss:2.00267363,Valid Accuracy:0.6197685

================================================================================17:13:35
Epoch=6,Loss:0.519614,Accuracy:0.870296121,Valid Loss:2.23463202,Valid Accuracy:0.613980412

================================================================================17:13:37
Epoch=7,Loss:0.408562034,Accuracy:0.901246965,Valid Loss:2.46969271,Valid Accuracy:0.612199485

================================================================================17:13:39
Epoch=8,Loss:0.339028627,Accuracy:0.920062363,Valid Loss:2.68585229,Valid Accuracy:0.615316093

================================================================================17:13:41
Epoch=9,Loss:0.293798745,Accuracy:0.92930305,Valid Loss:2.88995624,Valid Accuracy:0.613535166

================================================================================17:13:43
Epoch=10,Loss:0.263130337,Accuracy:0.936651051,Valid Loss:3.09705234,Valid Accuracy:0.612644672
```

```python

```

å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·"Pythonä¸ç®—æ³•ä¹‹ç¾"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚

![image.png](./data/Pythonä¸ç®—æ³•ä¹‹ç¾logo.jpg)



# 6-4,ä½¿ç”¨å¤šGPUè®­ç»ƒæ¨¡å‹

å¦‚æœä½¿ç”¨å¤šGPUè®­ç»ƒæ¨¡å‹ï¼Œæ¨èä½¿ç”¨å†…ç½®fitæ–¹æ³•ï¼Œè¾ƒä¸ºæ–¹ä¾¿ï¼Œä»…éœ€æ·»åŠ 2è¡Œä»£ç ã€‚

åœ¨Colabç¬”è®°æœ¬ä¸­ï¼šä¿®æ”¹->ç¬”è®°æœ¬è®¾ç½®->ç¡¬ä»¶åŠ é€Ÿå™¨ ä¸­é€‰æ‹© GPU

æ³¨ï¼šä»¥ä¸‹ä»£ç åªèƒ½åœ¨Colab ä¸Šæ‰èƒ½æ­£ç¡®æ‰§è¡Œã€‚

å¯é€šè¿‡ä»¥ä¸‹colabé“¾æ¥æµ‹è¯•æ•ˆæœã€Štf_å¤šGPUã€‹ï¼š

https://colab.research.google.com/drive/1j2kp_t0S_cofExSN7IyJ4QtMscbVlXU-



MirroredStrategyè¿‡ç¨‹ç®€ä»‹ï¼š

* è®­ç»ƒå¼€å§‹å‰ï¼Œè¯¥ç­–ç•¥åœ¨æ‰€æœ‰ N ä¸ªè®¡ç®—è®¾å¤‡ä¸Šå‡å„å¤åˆ¶ä¸€ä»½å®Œæ•´çš„æ¨¡å‹ï¼›
* æ¯æ¬¡è®­ç»ƒä¼ å…¥ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®æ—¶ï¼Œå°†æ•°æ®åˆ†æˆ N ä»½ï¼Œåˆ†åˆ«ä¼ å…¥ N ä¸ªè®¡ç®—è®¾å¤‡ï¼ˆå³æ•°æ®å¹¶è¡Œï¼‰ï¼›
* N ä¸ªè®¡ç®—è®¾å¤‡ä½¿ç”¨æœ¬åœ°å˜é‡ï¼ˆé•œåƒå˜é‡ï¼‰åˆ†åˆ«è®¡ç®—è‡ªå·±æ‰€è·å¾—çš„éƒ¨åˆ†æ•°æ®çš„æ¢¯åº¦ï¼›
* ä½¿ç”¨åˆ†å¸ƒå¼è®¡ç®—çš„ All-reduce æ“ä½œï¼Œåœ¨è®¡ç®—è®¾å¤‡é—´é«˜æ•ˆäº¤æ¢æ¢¯åº¦æ•°æ®å¹¶è¿›è¡Œæ±‚å’Œï¼Œä½¿å¾—æœ€ç»ˆæ¯ä¸ªè®¾å¤‡éƒ½æœ‰äº†æ‰€æœ‰è®¾å¤‡çš„æ¢¯åº¦ä¹‹å’Œï¼›
* ä½¿ç”¨æ¢¯åº¦æ±‚å’Œçš„ç»“æœæ›´æ–°æœ¬åœ°å˜é‡ï¼ˆé•œåƒå˜é‡ï¼‰ï¼›
* å½“æ‰€æœ‰è®¾å¤‡å‡æ›´æ–°æœ¬åœ°å˜é‡åï¼Œè¿›è¡Œä¸‹ä¸€è½®è®­ç»ƒï¼ˆå³è¯¥å¹¶è¡Œç­–ç•¥æ˜¯åŒæ­¥çš„ï¼‰ã€‚

```python
%tensorflow_version 2.x
import tensorflow as tf
print(tf.__version__)
from tensorflow.keras import * 
```

```python
#æ­¤å¤„åœ¨colabä¸Šä½¿ç”¨1ä¸ªGPUæ¨¡æ‹Ÿå‡ºä¸¤ä¸ªé€»è¾‘GPUè¿›è¡Œå¤šGPUè®­ç»ƒ
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    # è®¾ç½®ä¸¤ä¸ªé€»è¾‘GPUæ¨¡æ‹Ÿå¤šGPUè®­ç»ƒ
    try:
        tf.config.experimental.set_virtual_device_configuration(gpus[0],
            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024),
             tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)])
        logical_gpus = tf.config.experimental.list_logical_devices('GPU')
        print(len(gpus), "Physical GPU,", len(logical_gpus), "Logical GPUs")
    except RuntimeError as e:
        print(e)
```

### ä¸€ï¼Œå‡†å¤‡æ•°æ®

```python
MAX_LEN = 300
BATCH_SIZE = 32
(x_train,y_train),(x_test,y_test) = datasets.reuters.load_data()
x_train = preprocessing.sequence.pad_sequences(x_train,maxlen=MAX_LEN)
x_test = preprocessing.sequence.pad_sequences(x_test,maxlen=MAX_LEN)

MAX_WORDS = x_train.max()+1
CAT_NUM = y_train.max()+1

ds_train = tf.data.Dataset.from_tensor_slices((x_train,y_train)) \
          .shuffle(buffer_size = 1000).batch(BATCH_SIZE) \
          .prefetch(tf.data.experimental.AUTOTUNE).cache()
   
ds_test = tf.data.Dataset.from_tensor_slices((x_test,y_test)) \
          .shuffle(buffer_size = 1000).batch(BATCH_SIZE) \
          .prefetch(tf.data.experimental.AUTOTUNE).cache()

```

### äºŒï¼Œå®šä¹‰æ¨¡å‹

```python
tf.keras.backend.clear_session()
def create_model():
    
    model = models.Sequential()

    model.add(layers.Embedding(MAX_WORDS,7,input_length=MAX_LEN))
    model.add(layers.Conv1D(filters = 64,kernel_size = 5,activation = "relu"))
    model.add(layers.MaxPool1D(2))
    model.add(layers.Conv1D(filters = 32,kernel_size = 3,activation = "relu"))
    model.add(layers.MaxPool1D(2))
    model.add(layers.Flatten())
    model.add(layers.Dense(CAT_NUM,activation = "softmax"))
    return(model)

def compile_model(model):
    model.compile(optimizer=optimizers.Nadam(),
                loss=losses.SparseCategoricalCrossentropy(from_logits=True),
                metrics=[metrics.SparseCategoricalAccuracy(),metrics.SparseTopKCategoricalAccuracy(5)]) 
    return(model)
```

### ä¸‰ï¼Œè®­ç»ƒæ¨¡å‹

```python
#å¢åŠ ä»¥ä¸‹ä¸¤è¡Œä»£ç 
strategy = tf.distribute.MirroredStrategy()  
with strategy.scope(): 
    model = create_model()
    model.summary()
    model = compile_model(model)
    
history = model.fit(ds_train,validation_data = ds_test,epochs = 10)  
```

```
WARNING:tensorflow:NCCL is not supported when using virtual GPUs, fallingback to reduction to one device
INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        (None, 300, 7)            216874    
_________________________________________________________________
conv1d (Conv1D)              (None, 296, 64)           2304      
_________________________________________________________________
max_pooling1d (MaxPooling1D) (None, 148, 64)           0         
_________________________________________________________________
conv1d_1 (Conv1D)            (None, 146, 32)           6176      
_________________________________________________________________
max_pooling1d_1 (MaxPooling1 (None, 73, 32)            0         
_________________________________________________________________
flatten (Flatten)            (None, 2336)              0         
_________________________________________________________________
dense (Dense)                (None, 46)                107502    
=================================================================
Total params: 332,856
Trainable params: 332,856
Non-trainable params: 0
_________________________________________________________________
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
Train for 281 steps, validate for 71 steps
Epoch 1/10
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').
INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').
281/281 [==============================] - 15s 53ms/step - loss: 2.0270 - sparse_categorical_accuracy: 0.4653 - sparse_top_k_categorical_accuracy: 0.7481 - val_loss: 1.7517 - val_sparse_categorical_accuracy: 0.5481 - val_sparse_top_k_categorical_accuracy: 0.7578
Epoch 2/10
281/281 [==============================] - 4s 14ms/step - loss: 1.5206 - sparse_categorical_accuracy: 0.6045 - sparse_top_k_categorical_accuracy: 0.7938 - val_loss: 1.5715 - val_sparse_categorical_accuracy: 0.5993 - val_sparse_top_k_categorical_accuracy: 0.7983
Epoch 3/10
281/281 [==============================] - 4s 14ms/step - loss: 1.2178 - sparse_categorical_accuracy: 0.6843 - sparse_top_k_categorical_accuracy: 0.8547 - val_loss: 1.5232 - val_sparse_categorical_accuracy: 0.6327 - val_sparse_top_k_categorical_accuracy: 0.8112
Epoch 4/10
281/281 [==============================] - 4s 13ms/step - loss: 0.9127 - sparse_categorical_accuracy: 0.7648 - sparse_top_k_categorical_accuracy: 0.9113 - val_loss: 1.6527 - val_sparse_categorical_accuracy: 0.6296 - val_sparse_top_k_categorical_accuracy: 0.8201
Epoch 5/10
281/281 [==============================] - 4s 14ms/step - loss: 0.6606 - sparse_categorical_accuracy: 0.8321 - sparse_top_k_categorical_accuracy: 0.9525 - val_loss: 1.8791 - val_sparse_categorical_accuracy: 0.6158 - val_sparse_top_k_categorical_accuracy: 0.8219
Epoch 6/10
281/281 [==============================] - 4s 14ms/step - loss: 0.4919 - sparse_categorical_accuracy: 0.8799 - sparse_top_k_categorical_accuracy: 0.9725 - val_loss: 2.1282 - val_sparse_categorical_accuracy: 0.6037 - val_sparse_top_k_categorical_accuracy: 0.8112
Epoch 7/10
281/281 [==============================] - 4s 14ms/step - loss: 0.3947 - sparse_categorical_accuracy: 0.9051 - sparse_top_k_categorical_accuracy: 0.9814 - val_loss: 2.3033 - val_sparse_categorical_accuracy: 0.6046 - val_sparse_top_k_categorical_accuracy: 0.8094
Epoch 8/10
281/281 [==============================] - 4s 14ms/step - loss: 0.3335 - sparse_categorical_accuracy: 0.9207 - sparse_top_k_categorical_accuracy: 0.9863 - val_loss: 2.4255 - val_sparse_categorical_accuracy: 0.5993 - val_sparse_top_k_categorical_accuracy: 0.8099
Epoch 9/10
281/281 [==============================] - 4s 14ms/step - loss: 0.2919 - sparse_categorical_accuracy: 0.9304 - sparse_top_k_categorical_accuracy: 0.9911 - val_loss: 2.5571 - val_sparse_categorical_accuracy: 0.6020 - val_sparse_top_k_categorical_accuracy: 0.8126
Epoch 10/10
281/281 [==============================] - 4s 14ms/step - loss: 0.2617 - sparse_categorical_accuracy: 0.9342 - sparse_top_k_categorical_accuracy: 0.9937 - val_loss: 2.6700 - val_sparse_categorical_accuracy: 0.6077 - val_sparse_top_k_categorical_accuracy: 0.8148
CPU times: user 1min 2s, sys: 8.59 s, total: 1min 10s
Wall time: 58.5 s
```


```python

```

å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·"Pythonä¸ç®—æ³•ä¹‹ç¾"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚

![image.png](./data/Pythonä¸ç®—æ³•ä¹‹ç¾logo.jpg)

# 6-5,ä½¿ç”¨TPUè®­ç»ƒæ¨¡å‹

å¦‚æœæƒ³å°è¯•ä½¿ç”¨Google Colabä¸Šçš„TPUæ¥è®­ç»ƒæ¨¡å‹ï¼Œä¹Ÿæ˜¯éå¸¸æ–¹ä¾¿ï¼Œä»…éœ€æ·»åŠ 6è¡Œä»£ç ã€‚

åœ¨Colabç¬”è®°æœ¬ä¸­ï¼šä¿®æ”¹->ç¬”è®°æœ¬è®¾ç½®->ç¡¬ä»¶åŠ é€Ÿå™¨ ä¸­é€‰æ‹© TPU

æ³¨ï¼šä»¥ä¸‹ä»£ç åªèƒ½åœ¨Colab ä¸Šæ‰èƒ½æ­£ç¡®æ‰§è¡Œã€‚

å¯é€šè¿‡ä»¥ä¸‹colabé“¾æ¥æµ‹è¯•æ•ˆæœã€Štf_TPUã€‹ï¼š

https://colab.research.google.com/drive/1XCIhATyE1R7lq6uwFlYlRsUr5d9_-r1s


```python
%tensorflow_version 2.x
import tensorflow as tf
print(tf.__version__)
from tensorflow.keras import * 
```

### ä¸€ï¼Œå‡†å¤‡æ•°æ®

```python
MAX_LEN = 300
BATCH_SIZE = 32
(x_train,y_train),(x_test,y_test) = datasets.reuters.load_data()
x_train = preprocessing.sequence.pad_sequences(x_train,maxlen=MAX_LEN)
x_test = preprocessing.sequence.pad_sequences(x_test,maxlen=MAX_LEN)

MAX_WORDS = x_train.max()+1
CAT_NUM = y_train.max()+1

ds_train = tf.data.Dataset.from_tensor_slices((x_train,y_train)) \
          .shuffle(buffer_size = 1000).batch(BATCH_SIZE) \
          .prefetch(tf.data.experimental.AUTOTUNE).cache()
   
ds_test = tf.data.Dataset.from_tensor_slices((x_test,y_test)) \
          .shuffle(buffer_size = 1000).batch(BATCH_SIZE) \
          .prefetch(tf.data.experimental.AUTOTUNE).cache()
```

### äºŒï¼Œå®šä¹‰æ¨¡å‹

```python
tf.keras.backend.clear_session()
def create_model():
    
    model = models.Sequential()

    model.add(layers.Embedding(MAX_WORDS,7,input_length=MAX_LEN))
    model.add(layers.Conv1D(filters = 64,kernel_size = 5,activation = "relu"))
    model.add(layers.MaxPool1D(2))
    model.add(layers.Conv1D(filters = 32,kernel_size = 3,activation = "relu"))
    model.add(layers.MaxPool1D(2))
    model.add(layers.Flatten())
    model.add(layers.Dense(CAT_NUM,activation = "softmax"))
    return(model)

def compile_model(model):
    model.compile(optimizer=optimizers.Nadam(),
                loss=losses.SparseCategoricalCrossentropy(from_logits=True),
                metrics=[metrics.SparseCategoricalAccuracy(),metrics.SparseTopKCategoricalAccuracy(5)]) 
    return(model)
```

```python

```

### ä¸‰ï¼Œè®­ç»ƒæ¨¡å‹

```python
#å¢åŠ ä»¥ä¸‹6è¡Œä»£ç 
import os
resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])
tf.config.experimental_connect_to_cluster(resolver)
tf.tpu.experimental.initialize_tpu_system(resolver)
strategy = tf.distribute.experimental.TPUStrategy(resolver)
with strategy.scope():
    model = create_model()
    model.summary()
    model = compile_model(model)
    
```

```
WARNING:tensorflow:TPU system 10.26.134.242:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.
WARNING:tensorflow:TPU system 10.26.134.242:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.
INFO:tensorflow:Initializing the TPU system: 10.26.134.242:8470
INFO:tensorflow:Initializing the TPU system: 10.26.134.242:8470
INFO:tensorflow:Clearing out eager caches
INFO:tensorflow:Clearing out eager caches
INFO:tensorflow:Finished initializing TPU system.
INFO:tensorflow:Finished initializing TPU system.
INFO:tensorflow:Found TPU system:
INFO:tensorflow:Found TPU system:
INFO:tensorflow:*** Num TPU Cores: 8
INFO:tensorflow:*** Num TPU Cores: 8
INFO:tensorflow:*** Num TPU Workers: 1
INFO:tensorflow:*** Num TPU Workers: 1
INFO:tensorflow:*** Num TPU Cores Per Worker: 8
INFO:tensorflow:*** Num TPU Cores Per Worker: 8
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding (Embedding)        (None, 300, 7)            216874    
_________________________________________________________________
conv1d (Conv1D)              (None, 296, 64)           2304      
_________________________________________________________________
max_pooling1d (MaxPooling1D) (None, 148, 64)           0         
_________________________________________________________________
conv1d_1 (Conv1D)            (None, 146, 32)           6176      
_________________________________________________________________
max_pooling1d_1 (MaxPooling1 (None, 73, 32)            0         
_________________________________________________________________
flatten (Flatten)            (None, 2336)              0         
_________________________________________________________________
dense (Dense)                (None, 46)                107502    
=================================================================
Total params: 332,856
Trainable params: 332,856
Non-trainable params: 0
_________________________________________________________________
```

```python
history = model.fit(ds_train,validation_data = ds_test,epochs = 10)
```

```
Train for 281 steps, validate for 71 steps
Epoch 1/10
281/281 [==============================] - 12s 43ms/step - loss: 3.4466 - sparse_categorical_accuracy: 0.4332 - sparse_top_k_categorical_accuracy: 0.7180 - val_loss: 3.3179 - val_sparse_categorical_accuracy: 0.5352 - val_sparse_top_k_categorical_accuracy: 0.7195
Epoch 2/10
281/281 [==============================] - 6s 20ms/step - loss: 3.3251 - sparse_categorical_accuracy: 0.5405 - sparse_top_k_categorical_accuracy: 0.7302 - val_loss: 3.3082 - val_sparse_categorical_accuracy: 0.5463 - val_sparse_top_k_categorical_accuracy: 0.7235
Epoch 3/10
281/281 [==============================] - 6s 20ms/step - loss: 3.2961 - sparse_categorical_accuracy: 0.5729 - sparse_top_k_categorical_accuracy: 0.7280 - val_loss: 3.3026 - val_sparse_categorical_accuracy: 0.5499 - val_sparse_top_k_categorical_accuracy: 0.7217
Epoch 4/10
281/281 [==============================] - 5s 19ms/step - loss: 3.2751 - sparse_categorical_accuracy: 0.5924 - sparse_top_k_categorical_accuracy: 0.7276 - val_loss: 3.2957 - val_sparse_categorical_accuracy: 0.5543 - val_sparse_top_k_categorical_accuracy: 0.7217
Epoch 5/10
281/281 [==============================] - 5s 19ms/step - loss: 3.2655 - sparse_categorical_accuracy: 0.6008 - sparse_top_k_categorical_accuracy: 0.7290 - val_loss: 3.3022 - val_sparse_categorical_accuracy: 0.5490 - val_sparse_top_k_categorical_accuracy: 0.7231
Epoch 6/10
281/281 [==============================] - 5s 19ms/step - loss: 3.2616 - sparse_categorical_accuracy: 0.6041 - sparse_top_k_categorical_accuracy: 0.7295 - val_loss: 3.3015 - val_sparse_categorical_accuracy: 0.5503 - val_sparse_top_k_categorical_accuracy: 0.7235
Epoch 7/10
281/281 [==============================] - 6s 21ms/step - loss: 3.2595 - sparse_categorical_accuracy: 0.6059 - sparse_top_k_categorical_accuracy: 0.7322 - val_loss: 3.3064 - val_sparse_categorical_accuracy: 0.5454 - val_sparse_top_k_categorical_accuracy: 0.7266
Epoch 8/10
281/281 [==============================] - 6s 21ms/step - loss: 3.2591 - sparse_categorical_accuracy: 0.6063 - sparse_top_k_categorical_accuracy: 0.7327 - val_loss: 3.3025 - val_sparse_categorical_accuracy: 0.5481 - val_sparse_top_k_categorical_accuracy: 0.7231
Epoch 9/10
281/281 [==============================] - 5s 19ms/step - loss: 3.2588 - sparse_categorical_accuracy: 0.6062 - sparse_top_k_categorical_accuracy: 0.7332 - val_loss: 3.2992 - val_sparse_categorical_accuracy: 0.5521 - val_sparse_top_k_categorical_accuracy: 0.7257
Epoch 10/10
281/281 [==============================] - 5s 18ms/step - loss: 3.2577 - sparse_categorical_accuracy: 0.6073 - sparse_top_k_categorical_accuracy: 0.7363 - val_loss: 3.2981 - val_sparse_categorical_accuracy: 0.5516 - val_sparse_top_k_categorical_accuracy: 0.7306
CPU times: user 18.9 s, sys: 3.86 s, total: 22.7 s
Wall time: 1min 1s
```

```python

```

å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·"Pythonä¸ç®—æ³•ä¹‹ç¾"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚

![image.png](./data/Pythonä¸ç®—æ³•ä¹‹ç¾logo.jpg)

```python

```
# 6-6,ä½¿ç”¨tensorflow-servingéƒ¨ç½²æ¨¡å‹

TensorFlowè®­ç»ƒå¥½çš„æ¨¡å‹ä»¥tensorflowåŸç”Ÿæ–¹å¼ä¿å­˜æˆprotobufæ–‡ä»¶åå¯ä»¥ç”¨è®¸å¤šæ–¹å¼éƒ¨ç½²è¿è¡Œã€‚

ä¾‹å¦‚ï¼šé€šè¿‡ tensorflow-js å¯ä»¥ç”¨javascripè„šæœ¬åŠ è½½æ¨¡å‹å¹¶åœ¨æµè§ˆå™¨ä¸­è¿è¡Œæ¨¡å‹ã€‚

é€šè¿‡ tensorflow-lite å¯ä»¥åœ¨ç§»åŠ¨å’ŒåµŒå…¥å¼è®¾å¤‡ä¸ŠåŠ è½½å¹¶è¿è¡ŒTensorFlowæ¨¡å‹ã€‚

é€šè¿‡ tensorflow-serving å¯ä»¥åŠ è½½æ¨¡å‹åæä¾›ç½‘ç»œæ¥å£APIæœåŠ¡ï¼Œé€šè¿‡ä»»æ„ç¼–ç¨‹è¯­è¨€å‘é€ç½‘ç»œè¯·æ±‚éƒ½å¯ä»¥è·å–æ¨¡å‹é¢„æµ‹ç»“æœã€‚

é€šè¿‡ tensorFlow for Javaæ¥å£ï¼Œå¯ä»¥åœ¨Javaæˆ–è€…spark(scala)ä¸­è°ƒç”¨tensorflowæ¨¡å‹è¿›è¡Œé¢„æµ‹ã€‚

æˆ‘ä»¬ä¸»è¦ä»‹ç»tensorflow servingéƒ¨ç½²æ¨¡å‹ã€ä½¿ç”¨spark(scala)è°ƒç”¨tensorflowæ¨¡å‹çš„æ–¹æ³•ã€‚

```python

```

### ã€‡ï¼Œtensorflow servingæ¨¡å‹éƒ¨ç½²æ¦‚è¿°
ä½¿ç”¨ tensorflow serving éƒ¨ç½²æ¨¡å‹è¦å®Œæˆä»¥ä¸‹æ­¥éª¤ã€‚

* (1) å‡†å¤‡protobufæ¨¡å‹æ–‡ä»¶ã€‚

* (2) å®‰è£…tensorflow servingã€‚

* (3) å¯åŠ¨tensorflow serving æœåŠ¡ã€‚

* (4) å‘APIæœåŠ¡å‘é€è¯·æ±‚ï¼Œè·å–é¢„æµ‹ç»“æœã€‚


å¯é€šè¿‡ä»¥ä¸‹colabé“¾æ¥æµ‹è¯•æ•ˆæœã€Štf_servingã€‹ï¼š
https://colab.research.google.com/drive/1vS5LAYJTEn-H0GDb1irzIuyRB8E3eWc8



```python
%tensorflow_version 2.x
import tensorflow as tf
print(tf.__version__)
from tensorflow.keras import * 

```

### ä¸€ï¼Œå‡†å¤‡protobufæ¨¡å‹æ–‡ä»¶

æˆ‘ä»¬ä½¿ç”¨tf.keras è®­ç»ƒä¸€ä¸ªç®€å•çš„çº¿æ€§å›å½’æ¨¡å‹ï¼Œå¹¶ä¿å­˜æˆprotobufæ–‡ä»¶ã€‚

```python
import tensorflow as tf
from tensorflow.keras import models,layers,optimizers

## æ ·æœ¬æ•°é‡
n = 800

## ç”Ÿæˆæµ‹è¯•ç”¨æ•°æ®é›†
X = tf.random.uniform([n,2],minval=-10,maxval=10) 
w0 = tf.constant([[2.0],[-1.0]])
b0 = tf.constant(3.0)

Y = X@w0 + b0 + tf.random.normal([n,1],
    mean = 0.0,stddev= 2.0) # @è¡¨ç¤ºçŸ©é˜µä¹˜æ³•,å¢åŠ æ­£æ€æ‰°åŠ¨

## å»ºç«‹æ¨¡å‹
tf.keras.backend.clear_session()
inputs = layers.Input(shape = (2,),name ="inputs") #è®¾ç½®è¾“å…¥åå­—ä¸ºinputs
outputs = layers.Dense(1, name = "outputs")(inputs) #è®¾ç½®è¾“å‡ºåå­—ä¸ºoutputs
linear = models.Model(inputs = inputs,outputs = outputs)
linear.summary()

## ä½¿ç”¨fitæ–¹æ³•è¿›è¡Œè®­ç»ƒ
linear.compile(optimizer="rmsprop",loss="mse",metrics=["mae"])
linear.fit(X,Y,batch_size = 8,epochs = 100)  

tf.print("w = ",linear.layers[1].kernel)
tf.print("b = ",linear.layers[1].bias)

## å°†æ¨¡å‹ä¿å­˜æˆpbæ ¼å¼æ–‡ä»¶
export_path = "./data/linear_model/"
version = "1"       #åç»­å¯ä»¥é€šè¿‡ç‰ˆæœ¬å·è¿›è¡Œæ¨¡å‹ç‰ˆæœ¬è¿­ä»£ä¸ç®¡ç†
linear.save(export_path+version, save_format="tf") 
```

```python
#æŸ¥çœ‹ä¿å­˜çš„æ¨¡å‹æ–‡ä»¶
!ls {export_path+version}
```

```
assets	saved_model.pb	variables
```

```python
# æŸ¥çœ‹æ¨¡å‹æ–‡ä»¶ç›¸å…³ä¿¡æ¯
!saved_model_cli show --dir {export_path+str(version)} --all
```

```
MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:

signature_def['__saved_model_init_op']:
  The given SavedModel SignatureDef contains the following input(s):
  The given SavedModel SignatureDef contains the following output(s):
    outputs['__saved_model_init_op'] tensor_info:
        dtype: DT_INVALID
        shape: unknown_rank
        name: NoOp
  Method name is: 

signature_def['serving_default']:
  The given SavedModel SignatureDef contains the following input(s):
    inputs['inputs'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1, 2)
        name: serving_default_inputs:0
  The given SavedModel SignatureDef contains the following output(s):
    outputs['outputs'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1, 1)
        name: StatefulPartitionedCall:0
  Method name is: tensorflow/serving/predict
WARNING:tensorflow:From /tensorflow-2.1.0/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.

Defined Functions:
  Function Name: '__call__'
    Option #1
      Callable with:
        Argument #1
          inputs: TensorSpec(shape=(None, 2), dtype=tf.float32, name='inputs')
        Argument #2
          DType: bool
          Value: False
        Argument #3
          DType: NoneType
          Value: None
    Option #2
      Callable with:
        Argument #1
          inputs: TensorSpec(shape=(None, 2), dtype=tf.float32, name='inputs')
        Argument #2
          DType: bool
          Value: True
        Argument #3
          DType: NoneType
          Value: None

  Function Name: '_default_save_signature'
    Option #1
      Callable with:
        Argument #1
          inputs: TensorSpec(shape=(None, 2), dtype=tf.float32, name='inputs')

  Function Name: 'call_and_return_all_conditional_losses'
    Option #1
      Callable with:
        Argument #1
          inputs: TensorSpec(shape=(None, 2), dtype=tf.float32, name='inputs')
        Argument #2
          DType: bool
          Value: True
        Argument #3
          DType: NoneType
          Value: None
    Option #2
      Callable with:
        Argument #1
          inputs: TensorSpec(shape=(None, 2), dtype=tf.float32, name='inputs')
        Argument #2
          DType: bool
          Value: False
        Argument #3
          DType: NoneType
          Value: None
```

```python

```

### äºŒï¼Œå®‰è£… tensorflow serving


å®‰è£… tensorflow serving æœ‰2ç§ä¸»è¦æ–¹æ³•ï¼šé€šè¿‡Dockeré•œåƒå®‰è£…ï¼Œé€šè¿‡aptå®‰è£…ã€‚

é€šè¿‡Dockeré•œåƒå®‰è£…æ˜¯æœ€ç®€å•ï¼Œæœ€ç›´æ¥çš„æ–¹æ³•ï¼Œæ¨èé‡‡ç”¨ã€‚

Dockerå¯ä»¥ç†è§£æˆä¸€ç§å®¹å™¨ï¼Œå…¶ä¸Šé¢å¯ä»¥ç»™å„ç§ä¸åŒçš„ç¨‹åºæä¾›ç‹¬ç«‹çš„è¿è¡Œç¯å¢ƒã€‚

ä¸€èˆ¬ä¸šåŠ¡ä¸­ç”¨åˆ°tensorflowçš„ä¼ä¸šéƒ½ä¼šæœ‰è¿ç»´åŒå­¦é€šè¿‡Docker æ­å»º tensorflow serving.

æ— éœ€ç®—æ³•å·¥ç¨‹å¸ˆåŒå­¦åŠ¨æ‰‹å®‰è£…ï¼Œä»¥ä¸‹å®‰è£…è¿‡ç¨‹ä»…ä¾›å‚è€ƒã€‚

ä¸åŒæ“ä½œç³»ç»Ÿæœºå™¨ä¸Šå®‰è£…Dockerçš„æ–¹æ³•å¯ä»¥å‚ç…§ä»¥ä¸‹é“¾æ¥ã€‚

Windows: https://www.runoob.com/docker/windows-docker-install.html

MacOs: https://www.runoob.com/docker/macos-docker-install.html

CentOS: https://www.runoob.com/docker/centos-docker-install.html

å®‰è£…DockeræˆåŠŸåï¼Œä½¿ç”¨å¦‚ä¸‹å‘½ä»¤åŠ è½½ tensorflow/serving é•œåƒåˆ°Dockerä¸­

docker pull tensorflow/serving


```python

```

### ä¸‰ï¼Œå¯åŠ¨ tensorflow serving æœåŠ¡

```python
!docker run -t --rm -p 8501:8501 \
    -v "/Users/.../data/linear_model/" \
    -e MODEL_NAME=linear_model \
    tensorflow/serving & >server.log 2>&1
```

```python

```

### å››ï¼Œå‘APIæœåŠ¡å‘é€è¯·æ±‚


å¯ä»¥ä½¿ç”¨ä»»ä½•ç¼–ç¨‹è¯­è¨€çš„httpåŠŸèƒ½å‘é€è¯·æ±‚ï¼Œä¸‹é¢ç¤ºèŒƒlinuxçš„ curl å‘½ä»¤å‘é€è¯·æ±‚ï¼Œä»¥åŠPythonçš„requestsåº“å‘é€è¯·æ±‚ã€‚

```python
!curl -d '{"instances": [1.0, 2.0, 5.0]}' \
    -X POST http://localhost:8501/v1/models/linear_model:predict
```

```
{
    "predictions": [[3.06546211], [5.01313448]
    ]
}
```

```python
import json,requests

data = json.dumps({"signature_name": "serving_default", "instances": [[1.0, 2.0], [5.0,7.0]]})
headers = {"content-type": "application/json"}
json_response = requests.post('http://localhost:8501/v1/models/linear_model:predict', 
        data=data, headers=headers)
predictions = json.loads(json_response.text)["predictions"]
print(predictions)
```

```
[[3.06546211], [6.02843142]]
```

```python

```

å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·"Pythonä¸ç®—æ³•ä¹‹ç¾"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚

![image.png](./data/Pythonä¸ç®—æ³•ä¹‹ç¾logo.jpg)

```python

```

```python

```
# 6-7,ä½¿ç”¨spark-scalaè°ƒç”¨tensorflow2.0è®­ç»ƒå¥½çš„æ¨¡å‹

æœ¬ç¯‡æ–‡ç« ä»‹ç»åœ¨sparkä¸­è°ƒç”¨è®­ç»ƒå¥½çš„tensorflowæ¨¡å‹è¿›è¡Œé¢„æµ‹çš„æ–¹æ³•ã€‚

æœ¬æ–‡å†…å®¹çš„å­¦ä¹ éœ€è¦ä¸€å®šçš„sparkå’ŒscalaåŸºç¡€ã€‚

å¦‚æœä½¿ç”¨pysparkçš„è¯ä¼šæ¯”è¾ƒç®€å•ï¼Œåªéœ€è¦åœ¨æ¯ä¸ªexcutorä¸Šç”¨PythonåŠ è½½æ¨¡å‹åˆ†åˆ«é¢„æµ‹å°±å¯ä»¥äº†ã€‚

ä½†å·¥ç¨‹ä¸Šä¸ºäº†æ€§èƒ½è€ƒè™‘ï¼Œé€šå¸¸ä½¿ç”¨çš„æ˜¯scalaç‰ˆæœ¬çš„sparkã€‚

æœ¬ç¯‡æ–‡ç« æˆ‘ä»¬é€šè¿‡TensorFlow for Java åœ¨sparkä¸­è°ƒç”¨è®­ç»ƒå¥½çš„tensorflowæ¨¡å‹ã€‚

åˆ©ç”¨sparkçš„åˆ†å¸ƒå¼è®¡ç®—èƒ½åŠ›ï¼Œä»è€Œå¯ä»¥è®©è®­ç»ƒå¥½çš„tensorflowæ¨¡å‹åœ¨æˆç™¾ä¸Šåƒçš„æœºå™¨ä¸Šåˆ†å¸ƒå¼å¹¶è¡Œæ‰§è¡Œæ¨¡å‹æ¨æ–­ã€‚




```python

```

### ã€‡ï¼Œspark-scalaè°ƒç”¨tensorflowæ¨¡å‹æ¦‚è¿°


åœ¨spark(scala)ä¸­è°ƒç”¨tensorflowæ¨¡å‹è¿›è¡Œé¢„æµ‹éœ€è¦å®Œæˆä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ã€‚

ï¼ˆ1ï¼‰å‡†å¤‡protobufæ¨¡å‹æ–‡ä»¶

ï¼ˆ2ï¼‰åˆ›å»ºspark(scala)é¡¹ç›®ï¼Œåœ¨é¡¹ç›®ä¸­æ·»åŠ javaç‰ˆæœ¬çš„tensorflowå¯¹åº”çš„jaråŒ…ä¾èµ–

ï¼ˆ3ï¼‰åœ¨spark(scala)é¡¹ç›®ä¸­driverç«¯åŠ è½½tensorflowæ¨¡å‹è°ƒè¯•æˆåŠŸ

ï¼ˆ4ï¼‰åœ¨spark(scala)é¡¹ç›®ä¸­é€šè¿‡RDDåœ¨excutorä¸ŠåŠ è½½tensorflowæ¨¡å‹è°ƒè¯•æˆåŠŸ

ï¼ˆ5ï¼‰ åœ¨spark(scala)é¡¹ç›®ä¸­é€šè¿‡DataFrameåœ¨excutorä¸ŠåŠ è½½tensorflowæ¨¡å‹è°ƒè¯•æˆåŠŸ


```python

```

### ä¸€ï¼Œå‡†å¤‡protobufæ¨¡å‹æ–‡ä»¶


æˆ‘ä»¬ä½¿ç”¨tf.keras è®­ç»ƒä¸€ä¸ªç®€å•çš„çº¿æ€§å›å½’æ¨¡å‹ï¼Œå¹¶ä¿å­˜æˆprotobufæ–‡ä»¶ã€‚

```python

```

```python
import tensorflow as tf
from tensorflow.keras import models,layers,optimizers

## æ ·æœ¬æ•°é‡
n = 800

## ç”Ÿæˆæµ‹è¯•ç”¨æ•°æ®é›†
X = tf.random.uniform([n,2],minval=-10,maxval=10) 
w0 = tf.constant([[2.0],[-1.0]])
b0 = tf.constant(3.0)

Y = X@w0 + b0 + tf.random.normal([n,1],mean = 0.0,stddev= 2.0)  # @è¡¨ç¤ºçŸ©é˜µä¹˜æ³•,å¢åŠ æ­£æ€æ‰°åŠ¨

## å»ºç«‹æ¨¡å‹
tf.keras.backend.clear_session()
inputs = layers.Input(shape = (2,),name ="inputs") #è®¾ç½®è¾“å…¥åå­—ä¸ºinputs
outputs = layers.Dense(1, name = "outputs")(inputs) #è®¾ç½®è¾“å‡ºåå­—ä¸ºoutputs
linear = models.Model(inputs = inputs,outputs = outputs)
linear.summary()

## ä½¿ç”¨fitæ–¹æ³•è¿›è¡Œè®­ç»ƒ
linear.compile(optimizer="rmsprop",loss="mse",metrics=["mae"])
linear.fit(X,Y,batch_size = 8,epochs = 100)  

tf.print("w = ",linear.layers[1].kernel)
tf.print("b = ",linear.layers[1].bias)

## å°†æ¨¡å‹ä¿å­˜æˆpbæ ¼å¼æ–‡ä»¶
export_path = "./data/linear_model/"
version = "1"       #åç»­å¯ä»¥é€šè¿‡ç‰ˆæœ¬å·è¿›è¡Œæ¨¡å‹ç‰ˆæœ¬è¿­ä»£ä¸ç®¡ç†
linear.save(export_path+version, save_format="tf") 

```

```python

```

```python
!ls {export_path+version}
```

```python
# æŸ¥çœ‹æ¨¡å‹æ–‡ä»¶ç›¸å…³ä¿¡æ¯
!saved_model_cli show --dir {export_path+str(version)} --all
```

```python

```

æ¨¡å‹æ–‡ä»¶ä¿¡æ¯ä¸­è¿™äº›æ ‡çº¢çš„éƒ¨åˆ†éƒ½æ˜¯åé¢æœ‰å¯èƒ½ä¼šç”¨åˆ°çš„ã€‚

![](./data/æ¨¡å‹æ–‡ä»¶ä¿¡æ¯.png)

```python

```

### äºŒï¼Œåˆ›å»ºspark(scala)é¡¹ç›®ï¼Œåœ¨é¡¹ç›®ä¸­æ·»åŠ javaç‰ˆæœ¬çš„tensorflowå¯¹åº”çš„jaråŒ…ä¾èµ–

```python

```

å¦‚æœä½¿ç”¨mavenç®¡ç†é¡¹ç›®ï¼Œéœ€è¦æ·»åŠ å¦‚ä¸‹ jaråŒ…ä¾èµ–

```
<!-- https://mvnrepository.com/artifact/org.tensorflow/tensorflow -->
<dependency>
    <groupId>org.tensorflow</groupId>
    <artifactId>tensorflow</artifactId>
    <version>1.15.0</version>
</dependency>
```

ä¹Ÿå¯ä»¥ä»ä¸‹é¢ç½‘å€ä¸­ç›´æ¥ä¸‹è½½ org.tensorflow.tensorflowçš„jaråŒ…

ä»¥åŠå…¶ä¾èµ–çš„org.tensorflow.libtensorflow å’Œ org.tensorflowlibtensorflow_jniçš„jaråŒ… æ”¾åˆ°é¡¹ç›®ä¸­ã€‚

https://mvnrepository.com/artifact/org.tensorflow/tensorflow/1.15.0


```python

```

```python

```

### ä¸‰ï¼Œ åœ¨spark(scala)é¡¹ç›®ä¸­driverç«¯åŠ è½½tensorflowæ¨¡å‹è°ƒè¯•æˆåŠŸ


æˆ‘ä»¬çš„ç¤ºèŒƒä»£ç åœ¨jupyter notebookä¸­è¿›è¡Œæ¼”ç¤ºï¼Œéœ€è¦å®‰è£…toreeä»¥æ”¯æŒspark(scala)ã€‚


```scala
import scala.collection.mutable.WrappedArray
import org.{tensorflow=>tf}

//æ³¨ï¼šloadå‡½æ•°çš„ç¬¬äºŒä¸ªå‚æ•°ä¸€èˆ¬éƒ½æ˜¯â€œserveâ€ï¼Œå¯ä»¥ä»æ¨¡å‹æ–‡ä»¶ç›¸å…³ä¿¡æ¯ä¸­æ‰¾åˆ°

val bundle = tf.SavedModelBundle 
   .load("/Users/liangyun/CodeFiles/eat_tensorflow2_in_30_days/data/linear_model/1","serve")

//æ³¨ï¼šåœ¨javaç‰ˆæœ¬çš„tensorflowä¸­è¿˜æ˜¯ç±»ä¼¼tensorflow1.0ä¸­é™æ€è®¡ç®—å›¾çš„æ¨¡å¼ï¼Œéœ€è¦å»ºç«‹Session, æŒ‡å®šfeedçš„æ•°æ®å’Œfetchçš„ç»“æœ, ç„¶å run.
//æ³¨ï¼šå¦‚æœæœ‰å¤šä¸ªæ•°æ®éœ€è¦å–‚å…¥ï¼Œå¯ä»¥è¿ç»­ç”¨ç”¨å¤šä¸ªfeedæ–¹æ³•
//æ³¨ï¼šè¾“å…¥å¿…é¡»æ˜¯floatç±»å‹

val sess = bundle.session()
val x = tf.Tensor.create(Array(Array(1.0f,2.0f),Array(2.0f,3.0f)))
val y =  sess.runner().feed("serving_default_inputs:0", x)
         .fetch("StatefulPartitionedCall:0").run().get(0)

val result = Array.ofDim[Float](y.shape()(0).toInt,y.shape()(1).toInt)
y.copyTo(result)

if(x != null) x.close()
if(y != null) y.close()
if(sess != null) sess.close()
if(bundle != null) bundle.close()  

result

```


è¾“å‡ºå¦‚ä¸‹ï¼š

```
Array(Array(3.019596), Array(3.9878292))
```


![](./data/TfDriver.png)

```python

```

### å››ï¼Œåœ¨spark(scala)é¡¹ç›®ä¸­é€šè¿‡RDDåœ¨excutorä¸ŠåŠ è½½tensorflowæ¨¡å‹è°ƒè¯•æˆåŠŸ


ä¸‹é¢æˆ‘ä»¬é€šè¿‡å¹¿æ’­æœºåˆ¶å°†Driverç«¯åŠ è½½çš„TensorFlowæ¨¡å‹ä¼ é€’åˆ°å„ä¸ªexcutorä¸Šï¼Œå¹¶åœ¨excutorä¸Šåˆ†å¸ƒå¼åœ°è°ƒç”¨æ¨¡å‹è¿›è¡Œæ¨æ–­ã€‚



```scala
import org.apache.spark.sql.SparkSession
import scala.collection.mutable.WrappedArray
import org.{tensorflow=>tf}

val spark = SparkSession
    .builder()
    .appName("TfRDD")
    .enableHiveSupport()
    .getOrCreate()

val sc = spark.sparkContext

//åœ¨Driverç«¯åŠ è½½æ¨¡å‹
val bundle = tf.SavedModelBundle 
   .load("/Users/liangyun/CodeFiles/master_tensorflow2_in_20_hours/data/linear_model/1","serve")

//åˆ©ç”¨å¹¿æ’­å°†æ¨¡å‹å‘é€åˆ°excutorä¸Š
val broads = sc.broadcast(bundle)

//æ„é€ æ•°æ®é›†
val rdd_data = sc.makeRDD(List(Array(1.0f,2.0f),Array(3.0f,5.0f),Array(6.0f,7.0f),Array(8.0f,3.0f)))

//é€šè¿‡mapPartitionsè°ƒç”¨æ¨¡å‹è¿›è¡Œæ‰¹é‡æ¨æ–­
val rdd_result = rdd_data.mapPartitions(iter => {
    
    val arr = iter.toArray
    val model = broads.value
    val sess = model.session()
    val x = tf.Tensor.create(arr)
    val y =  sess.runner().feed("serving_default_inputs:0", x)
             .fetch("StatefulPartitionedCall:0").run().get(0)

    //å°†é¢„æµ‹ç»“æœæ‹·è´åˆ°ç›¸åŒshapeçš„Floatç±»å‹çš„Arrayä¸­
    val result = Array.ofDim[Float](y.shape()(0).toInt,y.shape()(1).toInt)
    y.copyTo(result)
    result.iterator
    
})


rdd_result.take(5)
bundle.close
```


```python

```

è¾“å‡ºå¦‚ä¸‹ï¼š

```
Array(Array(3.019596), Array(3.9264367), Array(7.8607616), Array(15.974984))
```


![](./data/TfRDD.png)

```python

```

### äº”ï¼Œ åœ¨spark(scala)é¡¹ç›®ä¸­é€šè¿‡DataFrameåœ¨excutorä¸ŠåŠ è½½tensorflowæ¨¡å‹è°ƒè¯•æˆåŠŸ


é™¤äº†å¯ä»¥åœ¨Sparkçš„RDDæ•°æ®ä¸Šè°ƒç”¨tensorflowæ¨¡å‹è¿›è¡Œåˆ†å¸ƒå¼æ¨æ–­ï¼Œ

æˆ‘ä»¬ä¹Ÿå¯ä»¥åœ¨DataFrameæ•°æ®ä¸Šè°ƒç”¨tensorflowæ¨¡å‹è¿›è¡Œåˆ†å¸ƒå¼æ¨æ–­ã€‚

ä¸»è¦æ€è·¯æ˜¯å°†æ¨æ–­æ–¹æ³•æ³¨å†Œæˆä¸ºä¸€ä¸ªsparkSQLå‡½æ•°ã€‚


```scala
import org.apache.spark.sql.SparkSession
import scala.collection.mutable.WrappedArray
import org.{tensorflow=>tf}

object TfDataFrame extends Serializable{
    
    
    def main(args:Array[String]):Unit = {
        
        val spark = SparkSession
        .builder()
        .appName("TfDataFrame")
        .enableHiveSupport()
        .getOrCreate()
        val sc = spark.sparkContext
        
        
        import spark.implicits._

        val bundle = tf.SavedModelBundle 
           .load("/Users/liangyun/CodeFiles/master_tensorflow2_in_20_hours/data/linear_model/1","serve")

        val broads = sc.broadcast(bundle)
        
        //æ„é€ é¢„æµ‹å‡½æ•°ï¼Œå¹¶å°†å…¶æ³¨å†ŒæˆsparkSQLçš„udf
        val tfpredict = (features:WrappedArray[Float])  => {
            val bund = broads.value
            val sess = bund.session()
            val x = tf.Tensor.create(Array(features.toArray))
            val y =  sess.runner().feed("serving_default_inputs:0", x)
                     .fetch("StatefulPartitionedCall:0").run().get(0)
            val result = Array.ofDim[Float](y.shape()(0).toInt,y.shape()(1).toInt)
            y.copyTo(result)
            val y_pred = result(0)(0)
            y_pred
        }
        spark.udf.register("tfpredict",tfpredict)
        
        //æ„é€ DataFrameæ•°æ®é›†ï¼Œå°†featuresæ”¾åˆ°ä¸€åˆ—ä¸­
        val dfdata = sc.parallelize(List(Array(1.0f,2.0f),Array(3.0f,5.0f),Array(7.0f,8.0f))).toDF("features")
        dfdata.show 
        
        //è°ƒç”¨sparkSQLé¢„æµ‹å‡½æ•°ï¼Œå¢åŠ ä¸€ä¸ªæ–°çš„åˆ—ä½œä¸ºy_preds
        val dfresult = dfdata.selectExpr("features","tfpredict(features) as y_preds")
        dfresult.show 
        bundle.close
    }
}

```



```scala
TfDataFrame.main(Array())
```


```
+----------+
|  features|
+----------+
|[1.0, 2.0]|
|[3.0, 5.0]|
|[7.0, 8.0]|
+----------+

+----------+---------+
|  features|  y_preds|
+----------+---------+
|[1.0, 2.0]| 3.019596|
|[3.0, 5.0]|3.9264367|
|[7.0, 8.0]| 8.828995|
+----------+---------+
```


ä»¥ä¸Šæˆ‘ä»¬åˆ†åˆ«åœ¨spark çš„RDDæ•°æ®ç»“æ„å’ŒDataFrameæ•°æ®ç»“æ„ä¸Šå®ç°äº†è°ƒç”¨ä¸€ä¸ªtf.keraså®ç°çš„çº¿æ€§å›å½’æ¨¡å‹è¿›è¡Œåˆ†å¸ƒå¼æ¨¡å‹æ¨æ–­ã€‚

åœ¨æœ¬ä¾‹åŸºç¡€ä¸Šç¨ä½œä¿®æ”¹åˆ™å¯ä»¥ç”¨sparkè°ƒç”¨è®­ç»ƒå¥½çš„å„ç§å¤æ‚çš„ç¥ç»ç½‘ç»œæ¨¡å‹è¿›è¡Œåˆ†å¸ƒå¼æ¨¡å‹æ¨æ–­ã€‚

ä½†å®é™…ä¸Štensorflowå¹¶ä¸ä»…ä»…é€‚åˆå®ç°ç¥ç»ç½‘ç»œï¼Œå…¶åº•å±‚çš„è®¡ç®—å›¾è¯­è¨€å¯ä»¥è¡¨è¾¾å„ç§æ•°å€¼è®¡ç®—è¿‡ç¨‹ã€‚

åˆ©ç”¨å…¶ä¸°å¯Œçš„ä½é˜¶APIï¼Œæˆ‘ä»¬å¯ä»¥åœ¨tensorflow2.0ä¸Šå®ç°ä»»æ„æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œ

ç»“åˆtf.Moduleæä¾›çš„ä¾¿æ·çš„å°è£…åŠŸèƒ½ï¼Œæˆ‘ä»¬å¯ä»¥å°†è®­ç»ƒå¥½çš„ä»»æ„æœºå™¨å­¦ä¹ æ¨¡å‹å¯¼å‡ºæˆæ¨¡å‹æ–‡ä»¶å¹¶åœ¨sparkä¸Šåˆ†å¸ƒå¼è°ƒç”¨æ‰§è¡Œã€‚

è¿™æ— ç–‘ä¸ºæˆ‘ä»¬çš„å·¥ç¨‹åº”ç”¨æä¾›äº†å·¨å¤§çš„æƒ³è±¡ç©ºé—´ã€‚


å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·"Pythonä¸ç®—æ³•ä¹‹ç¾"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚

![image.png](./data/Pythonä¸ç®—æ³•ä¹‹ç¾logo.jpg)
